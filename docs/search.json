[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a public service announcement!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "iaingallagher.github.io",
    "section": "",
    "text": "Hello Data World 2 (python)\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHello Data World 1 (R)\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nQuarto with Github Pages\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2022\n\n\nIain J Gallagher\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2022\n\n\nIain J Gallagher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html",
    "href": "posts/quarto_github_pages/blog_process.html",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Quarto is an “open-source scientific and technical publishing system built on Pandoc”. What does that mean?\nIt means that you can use a simple text file to create documents like blogs, papers, books etc. This blog will be generated using Quarto. Quarto takes simple text files written in markdown and converts those documents into a number of different formats.\nQuarto allows you to embed code written in R, python, julia and Observable JS into your documents and that code will be executable. This makes it easy to share code for analysis or teaching or reminding yourself how things work!.\nBelow I’ll document how I set up this blog on Github pages using Quarto.\n\n\nFirst you’ll need to have the version control sofware git installed on your system. If you’re not familiar with git then there’s some good teaching here. Once you have git installed you have to set your identity.\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"YourEmail@XYZ.com\"\nThe email you use should be the same one you’ll use when you sign up for Github.\nYou can also set a default branch for your code. I use main for this. This is where the final code for the blog will live.\ngit config --global init.defaultBranch main\nYou’ll also need an account on Github. Github provides a free service called Github pages that allows you to host a free website.\nYou’ll also need to install Quarto.\nOnce we have git & Quarto installed and a Github account ready the workflow we’ll follow here is:\n\nCreate a blog repository on Github\nCopy (clone) that repository to our local computer\nSetup the blog structure in the local repository using Quarto\nCreate content for the blog using Quarto\nPush the content to the online repository\n\nOnce we’ve done the first three steps the last two steps can be repeated as we add new blog posts.\nI’ll be using a Unix based operating system either Mac OSX or linux & we’ll be using the command line. If you want to follow the process on Windows you’ll probably need to change a few commands used to create files or change directories from Unix commands to Windows commands. The git and quarto commands will all remain the same.\nOnce you have installed git and set up your git identity go to Github, create a free account and set up the various security options. Install Quarto on your computer.\n\n\n\nOn Github create a new repo named your_github_username.github.io replacing your_github_username with your actual Github username. Do not add a README or license file just now.\nOn your local computer change to the directory you want to use for your blog and clone the Github repo into that directory.\ncd ~/blog_directory\ngit clone git@github.com:THE_REPO_ADDRESS\nif you’re using ssh.\ncd ~/blog_directory\ngit clone https://github.com/THE_REPO_ADDRESS\nif you’re using https.\nThis will download the files on Github into the local directory. You’ll probably get a warning like:\nwarning: You appear to have cloned an empty repository.\nThat’s ok… you have cloned an empty repository!\n\n\n\nThere are full instructions here.\nChange into the local repo you just cloned.\ncd ~/blog_directory/your_github_username.github.io\nCreate a local copy of your blog by typing:\nquarto create-project --type website:blog\nQuarto will create several files and directories required to create the blog:\n\n_quarto.yml\nindex.qmd\nabout.qmd\nposts/\nposts/_metadata.yml\nstyles.css\n\nAdd an empty file named .nojekyll at the top level:\n# change directory\ncd your_github_username.github.io\n# add a file\ntouch .nojekyll\nThis is required so that Github pages will serve our blog properly later. See the Github pages section here.\nWe also need to set the output directory in the _quarto.yml file so the top of the file reads:\nproject:\n  type: website\n  output-dir: docs\nIf you want to add social media details you can make edits to the about.qmd file (see the webpage above).\n\n\n\nMove into the new directory created by Quarto:\ncd your_github_username.github.io\nType git init. This will tell git to start tracking the files in the blog directory. You can make sure this is working by typing git status & git should list all the directories and files in the your_github_username.github.io directory. You should also see that you are on the main branch.\n\n\n\nGit allows us to create different branches for projects we are working on. At the moment we only have one branch in our blog project - the main branch. For adding content etc we do not want to work on the main branch; we want the main branch to be the destination for code/posts we know we want to publish.\nOn your local machine create a new branch in the repo called e.g. adding-content.\ngit checkout -b adding-content\nYou should see a message Switched to a new branch 'adding-content'.\nAs you create content you will create that content on the adding-content branch. Once you’re happy with that content you can merge the adding-content and main branches. That way you’re never going to ‘break’ the main branch (the stuff you will actually blog) with code/content that doesn’t work.\nBefore we go any further we’ll push everything we have done to the adding-content branch on our local machine.\n# make sure we're on the adding-content branch\ngit status\n# add all the files & changes; . means add everything\ngit add .\n# commit the changes to the git repo\ngit commit -m \"started adding-content branch\"\n# check\ngit status\nYou should see a message:\nOn branch adding-content\nnothing to commit, working tree clean\n\n\n\nThe workflow to create content is to write in markdown and then use the tools in Quarto to render the markdown to html. If you made your blog repo as above with Quarto then Quarto will take care of adding blog posts to the index page of your blog.\nCreate a new Quarto file written in markdown. As you’re doing so you can check what the page will look like using the Render button in RStudio orVS Code if you’re using either of these for your writing.\nOnce you’re satisfied with the markdown file create a new folder in the /posts directory of your blog repo. Give the folder an informative name.\n# change to posts dir\ncd your_github.username.github.io/posts\n# create new dir to hold current content\nmkdir quarto_github_pages\n# move .qmd file to posts/new_dir\ncp path/to/content.qmd posts/quarto_github_pages\nNow we can add this file to git.\n# check your on adding content\ngit status\n# add the file\ngit add posts/quarto_github_pages/content.qmd\n# commit the file & add a message\ngit commit -m \"added first post to blog\"\n# check all is well\ngit status\n\n\n\nNow we have some content we can use quarto to render the actual site. Make sure you are in the top level directory of your blog (e.g. your_github_username.github.io) and type quarto preview to preview the site.\nIf it all looks good press ctrl+c to stop the site being served.\nYou can now render the site with quarto render. This will add several new files & directories.\nAdd these to the adding-content branch following the git workflow.\ngit status # make sure you are on adding-content branch\ngit add .\ngit status\ngit commit -m \"some useful commit message\"\ngit status\nNow we’re ready to send the site to Github.\n\n\n\nWe want to push the files we have created to the adding-content branch online.\ngit push origin adding-content\nGo to Github and log in if you have to. You should see a message that the adding-content branch has had content pushed to it.\n\n\n\nPull request\n\n\nPress the green button and follow the prompts to merge your changes to the main branch on Github.\nOn the GitHub page for your repo go to Settings (top right of the page) & scroll down until you see the section for GitHub pages. Click on the Source (it probably says Deploy from a branch) & select /docs.\n\n\n\nSet serve directory\n\n\nPress save. This tells GitHub Pages to look in themain branch of the project and in the docs directory for files to serve.\n\n\n\nSo far we have made all our changes to the local adding-content branch, sent these changes to the adding-content branch online and merged those changes with the main branch online. Our local main branch needs to be synchronised to the online main branch.\nIn your local blog directory checkout your main branch.\ngit checkout main\nNow pull the content from the online main branch down to your local machine.\ngit pull origin main\nYour local main branch and your online main branch will now be the same.\n\n\n\nA lot of what we did above was once off. For subsequent posts we will:\n\nDraft content in markdown\nTrack that content on the local adding-content branch\n\ngit checkout adding-content\ngit status # make sure you're on adding-content\ngit add XYZ\ngit status\ngit commit -m \"some commit message\"\ngit status\n\nPush the adding-content branch to Github when we’re ready to publish the page\n\ngit push origin adding-content\n\nMerge the changes to the main branch online using a Github pull request\n\nGithub should automatically serve the page we created.\n\nPull the finalised main branch from Github back to the local machine\n\ngit checkout main   \ngit pull origin main"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#preliminaries",
    "href": "posts/quarto_github_pages/blog_process.html#preliminaries",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "First you’ll need to have the version control sofware git installed on your system. If you’re not familiar with git then there’s some good teaching here. Once you have git installed you have to set your identity.\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"YourEmail@XYZ.com\"\nThe email you use should be the same one you’ll use when you sign up for Github.\nYou can also set a default branch for your code. I use main for this. This is where the final code for the blog will live.\ngit config --global init.defaultBranch main\nYou’ll also need an account on Github. Github provides a free service called Github pages that allows you to host a free website.\nYou’ll also need to install Quarto.\nOnce we have git & Quarto installed and a Github account ready the workflow we’ll follow here is:\n\nCreate a blog repository on Github\nCopy (clone) that repository to our local computer\nSetup the blog structure in the local repository using Quarto\nCreate content for the blog using Quarto\nPush the content to the online repository\n\nOnce we’ve done the first three steps the last two steps can be repeated as we add new blog posts.\nI’ll be using a Unix based operating system either Mac OSX or linux & we’ll be using the command line. If you want to follow the process on Windows you’ll probably need to change a few commands used to create files or change directories from Unix commands to Windows commands. The git and quarto commands will all remain the same.\nOnce you have installed git and set up your git identity go to Github, create a free account and set up the various security options. Install Quarto on your computer."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#github-set-up",
    "href": "posts/quarto_github_pages/blog_process.html#github-set-up",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "On Github create a new repo named your_github_username.github.io replacing your_github_username with your actual Github username. Do not add a README or license file just now.\nOn your local computer change to the directory you want to use for your blog and clone the Github repo into that directory.\ncd ~/blog_directory\ngit clone git@github.com:THE_REPO_ADDRESS\nif you’re using ssh.\ncd ~/blog_directory\ngit clone https://github.com/THE_REPO_ADDRESS\nif you’re using https.\nThis will download the files on Github into the local directory. You’ll probably get a warning like:\nwarning: You appear to have cloned an empty repository.\nThat’s ok… you have cloned an empty repository!"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#create-the-blog",
    "href": "posts/quarto_github_pages/blog_process.html#create-the-blog",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "There are full instructions here.\nChange into the local repo you just cloned.\ncd ~/blog_directory/your_github_username.github.io\nCreate a local copy of your blog by typing:\nquarto create-project --type website:blog\nQuarto will create several files and directories required to create the blog:\n\n_quarto.yml\nindex.qmd\nabout.qmd\nposts/\nposts/_metadata.yml\nstyles.css\n\nAdd an empty file named .nojekyll at the top level:\n# change directory\ncd your_github_username.github.io\n# add a file\ntouch .nojekyll\nThis is required so that Github pages will serve our blog properly later. See the Github pages section here.\nWe also need to set the output directory in the _quarto.yml file so the top of the file reads:\nproject:\n  type: website\n  output-dir: docs\nIf you want to add social media details you can make edits to the about.qmd file (see the webpage above)."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#setting-git-to-track-the-blog",
    "href": "posts/quarto_github_pages/blog_process.html#setting-git-to-track-the-blog",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Move into the new directory created by Quarto:\ncd your_github_username.github.io\nType git init. This will tell git to start tracking the files in the blog directory. You can make sure this is working by typing git status & git should list all the directories and files in the your_github_username.github.io directory. You should also see that you are on the main branch."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#adding-a-content-branch",
    "href": "posts/quarto_github_pages/blog_process.html#adding-a-content-branch",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Git allows us to create different branches for projects we are working on. At the moment we only have one branch in our blog project - the main branch. For adding content etc we do not want to work on the main branch; we want the main branch to be the destination for code/posts we know we want to publish.\nOn your local machine create a new branch in the repo called e.g. adding-content.\ngit checkout -b adding-content\nYou should see a message Switched to a new branch 'adding-content'.\nAs you create content you will create that content on the adding-content branch. Once you’re happy with that content you can merge the adding-content and main branches. That way you’re never going to ‘break’ the main branch (the stuff you will actually blog) with code/content that doesn’t work.\nBefore we go any further we’ll push everything we have done to the adding-content branch on our local machine.\n# make sure we're on the adding-content branch\ngit status\n# add all the files & changes; . means add everything\ngit add .\n# commit the changes to the git repo\ngit commit -m \"started adding-content branch\"\n# check\ngit status\nYou should see a message:\nOn branch adding-content\nnothing to commit, working tree clean"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#adding-content",
    "href": "posts/quarto_github_pages/blog_process.html#adding-content",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "The workflow to create content is to write in markdown and then use the tools in Quarto to render the markdown to html. If you made your blog repo as above with Quarto then Quarto will take care of adding blog posts to the index page of your blog.\nCreate a new Quarto file written in markdown. As you’re doing so you can check what the page will look like using the Render button in RStudio orVS Code if you’re using either of these for your writing.\nOnce you’re satisfied with the markdown file create a new folder in the /posts directory of your blog repo. Give the folder an informative name.\n# change to posts dir\ncd your_github.username.github.io/posts\n# create new dir to hold current content\nmkdir quarto_github_pages\n# move .qmd file to posts/new_dir\ncp path/to/content.qmd posts/quarto_github_pages\nNow we can add this file to git.\n# check your on adding content\ngit status\n# add the file\ngit add posts/quarto_github_pages/content.qmd\n# commit the file & add a message\ngit commit -m \"added first post to blog\"\n# check all is well\ngit status"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#render-the-site",
    "href": "posts/quarto_github_pages/blog_process.html#render-the-site",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Now we have some content we can use quarto to render the actual site. Make sure you are in the top level directory of your blog (e.g. your_github_username.github.io) and type quarto preview to preview the site.\nIf it all looks good press ctrl+c to stop the site being served.\nYou can now render the site with quarto render. This will add several new files & directories.\nAdd these to the adding-content branch following the git workflow.\ngit status # make sure you are on adding-content branch\ngit add .\ngit status\ngit commit -m \"some useful commit message\"\ngit status\nNow we’re ready to send the site to Github."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#push-blog-to-github",
    "href": "posts/quarto_github_pages/blog_process.html#push-blog-to-github",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "We want to push the files we have created to the adding-content branch online.\ngit push origin adding-content\nGo to Github and log in if you have to. You should see a message that the adding-content branch has had content pushed to it.\n\n\n\nPull request\n\n\nPress the green button and follow the prompts to merge your changes to the main branch on Github.\nOn the GitHub page for your repo go to Settings (top right of the page) & scroll down until you see the section for GitHub pages. Click on the Source (it probably says Deploy from a branch) & select /docs.\n\n\n\nSet serve directory\n\n\nPress save. This tells GitHub Pages to look in themain branch of the project and in the docs directory for files to serve."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#syncing-online-and-local",
    "href": "posts/quarto_github_pages/blog_process.html#syncing-online-and-local",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "So far we have made all our changes to the local adding-content branch, sent these changes to the adding-content branch online and merged those changes with the main branch online. Our local main branch needs to be synchronised to the online main branch.\nIn your local blog directory checkout your main branch.\ngit checkout main\nNow pull the content from the online main branch down to your local machine.\ngit pull origin main\nYour local main branch and your online main branch will now be the same."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#workflow-process",
    "href": "posts/quarto_github_pages/blog_process.html#workflow-process",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "A lot of what we did above was once off. For subsequent posts we will:\n\nDraft content in markdown\nTrack that content on the local adding-content branch\n\ngit checkout adding-content\ngit status # make sure you're on adding-content\ngit add XYZ\ngit status\ngit commit -m \"some commit message\"\ngit status\n\nPush the adding-content branch to Github when we’re ready to publish the page\n\ngit push origin adding-content\n\nMerge the changes to the main branch online using a Github pull request\n\nGithub should automatically serve the page we created.\n\nPull the finalised main branch from Github back to the local machine\n\ngit checkout main   \ngit pull origin main"
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html",
    "href": "posts/hello_data_r/hello_world_data_R.html",
    "title": "Hello Data World 1 (R)",
    "section": "",
    "text": "In programming it’s traditional that the first thing you learn to do in a new language is to print ‘Hello, World!’ to the screen. This is the first of three ‘Hello World’ posts that will walk through some data handling & analysis tasks. These will be a bit more complex than printing ‘Hello, World!’ but will provide a look at how to approach data loading, exploration, filtering, plotting and statistical testing. Each post will use a different language & in this first post we will use R - because it’s the language I know best (i.e. least worst). The next two posts will carry out the same tasks using python and julia. R and python are popular in data science and julia is a promising newcomer.\nIn each post we will load a dataset from a csv file, carry out some summarisation and exploratory plotting, some data filtering and finally carry out statistical testing on two groups using frequentist and Bayesian techniques. These are not exactly beginners posts but the aim is to give a flavour of how basic data exploration & analysis can be done in each language.\nIf you want to follow along the data are here."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html#introduction",
    "href": "posts/hello_data_r/hello_world_data_R.html#introduction",
    "title": "Hello Data World 1 (R)",
    "section": "",
    "text": "In programming it’s traditional that the first thing you learn to do in a new language is to print ‘Hello, World!’ to the screen. This is the first of three ‘Hello World’ posts that will walk through some data handling & analysis tasks. These will be a bit more complex than printing ‘Hello, World!’ but will provide a look at how to approach data loading, exploration, filtering, plotting and statistical testing. Each post will use a different language & in this first post we will use R - because it’s the language I know best (i.e. least worst). The next two posts will carry out the same tasks using python and julia. R and python are popular in data science and julia is a promising newcomer.\nIn each post we will load a dataset from a csv file, carry out some summarisation and exploratory plotting, some data filtering and finally carry out statistical testing on two groups using frequentist and Bayesian techniques. These are not exactly beginners posts but the aim is to give a flavour of how basic data exploration & analysis can be done in each language.\nIf you want to follow along the data are here."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html#preliminaries",
    "href": "posts/hello_data_r/hello_world_data_R.html#preliminaries",
    "title": "Hello Data World 1 (R)",
    "section": "Preliminaries",
    "text": "Preliminaries\nR has a lot of base functionality for data handling, exploration & statistical analysis; it’s what R was designed for. However we are going to make use of the ‘tidyverse’ (Wickham et al. 2019) because it has become a very popular approach to data handling & analysis in R.\n\nThe tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming.\n\nAs well as data handling & visualisation we will also be carrying out some statistical testing. R is well served for basic frequentist statistics and there’s nothing extra we need. For Bayesian analysis we will use the Stan probabilistic programming language (Carpenter et al. 2017). We will code a model by hand and use the cmdstanr package to pass that model to Stan. We will also use the brms package (Bürkner 2017) which makes writing Stan models easier. Details on how to install the cmdstanr package and Stan are here (see the section on Installing CmdStan for how to install Stan). Note that brms also needs Stan to be installed. We load the packages we need in the code below.\n\n# data loading & plotting\nlibrary(tidyverse) # meta-package; loads several packages\n# set theme for ggplot2 plotting\ntheme_set(theme_bw())\n# bayesian modeling\nlibrary(cmdstanr)\n# easier bayesian modeling\nlibrary(brms)\n# plot bayesian models\nlibrary(bayesplot)\n\n\nLoading the data\nThese data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a number of years by the students who carried out various measures on themselves.\n\n# load the data\ndata_in &lt;- read_csv('data/BODY_COMPOSITION_DATA.csv')\n\nRows: 203 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (9): girths, bia, DW, jackson, HW, skinfolds, BMI, WHR, Waist\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nExploration & tidying\nFirst we make sure the data looks as we expect it to.\n\n# examine the data\nglimpse(data_in)\n\nRows: 203\nColumns: 10\n$ sex       &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ girths    &lt;dbl&gt; 10.85, 14.12, 12.30, 8.50, 11.66, 15.65, 13.22, 14.62, 17.21…\n$ bia       &lt;dbl&gt; 5.7, 6.2, 6.3, 6.4, 6.6, 6.8, 6.9, 7.4, 7.6, 7.7, 7.8, 7.9, …\n$ DW        &lt;dbl&gt; 9.220, 11.800, 12.000, 10.850, 15.600, 21.420, 14.400, 9.820…\n$ jackson   &lt;dbl&gt; 4.75, 5.50, 5.50, 5.00, 12.00, 3.00, 7.80, 4.50, 9.00, 6.80,…\n$ HW        &lt;dbl&gt; 17.00, 16.90, 14.80, 10.20, 11.86, 33.10, 13.40, 14.35, 21.4…\n$ skinfolds &lt;dbl&gt; 50.75, 46.30, 45.80, 43.55, 93.50, 49.75, 56.70, 39.70, 73.5…\n$ BMI       &lt;dbl&gt; 20.70, 21.90, 21.39, 19.26, 22.30, 20.23, 23.54, 21.18, 20.5…\n$ WHR       &lt;dbl&gt; 0.8000, 0.8100, 0.7300, 0.7400, 0.7800, 0.8500, 0.8700, 0.77…\n$ Waist     &lt;dbl&gt; 76.5, 75.0, 70.0, 68.5, 74.0, 73.0, 80.0, 76.0, 75.0, 76.7, …\n\nsummary(data_in) # tells us about NA values\n\n     sex                girths           bia              DW       \n Length:203         Min.   : 7.15   Min.   : 5.70   Min.   : 4.10  \n Class :character   1st Qu.:15.04   1st Qu.:11.90   1st Qu.:16.34  \n Mode  :character   Median :20.12   Median :16.20   Median :21.40  \n                    Mean   :20.70   Mean   :16.98   Mean   :21.66  \n                    3rd Qu.:24.60   3rd Qu.:21.18   3rd Qu.:28.00  \n                    Max.   :87.90   Max.   :39.30   Max.   :45.90  \n                                    NA's   :1                      \n    jackson            HW          skinfolds           BMI       \n Min.   : 3.00   Min.   : 4.10   Min.   : 27.75   Min.   : 2.90  \n 1st Qu.: 8.00   1st Qu.:15.04   1st Qu.: 59.27   1st Qu.:21.18  \n Median :12.80   Median :21.00   Median : 76.23   Median :23.00  \n Mean   :14.23   Mean   :21.42   Mean   : 82.88   Mean   :23.25  \n 3rd Qu.:19.00   3rd Qu.:27.00   3rd Qu.:100.67   3rd Qu.:24.80  \n Max.   :35.00   Max.   :43.00   Max.   :181.00   Max.   :33.03  \n                 NA's   :1                                       \n      WHR             Waist       \n Min.   :0.6700   Min.   : 61.00  \n 1st Qu.:0.7400   1st Qu.: 72.25  \n Median :0.7800   Median : 76.00  \n Mean   :0.7821   Mean   : 76.84  \n 3rd Qu.:0.8170   3rd Qu.: 81.00  \n Max.   :0.9900   Max.   :100.80  \n                                  \n\n\nWe should deal with the missing values before we do any further analysis. There are many ways to deal with missing values but here we will just drop rows with missing values from the data using the complete.cases() function.\n\n# drop rows with NA values\ndata_in &lt;- data_in[complete.cases(data_in), ]\nsummary(data_in)\n\n     sex                girths           bia              DW       \n Length:201         Min.   : 7.15   Min.   : 5.70   Min.   : 4.10  \n Class :character   1st Qu.:15.08   1st Qu.:11.90   1st Qu.:16.30  \n Mode  :character   Median :20.12   Median :15.90   Median :21.40  \n                    Mean   :20.73   Mean   :16.98   Mean   :21.61  \n                    3rd Qu.:24.40   3rd Qu.:21.20   3rd Qu.:28.00  \n                    Max.   :87.90   Max.   :39.30   Max.   :45.90  \n    jackson            HW          skinfolds           BMI       \n Min.   : 3.00   Min.   : 4.10   Min.   : 27.75   Min.   : 2.90  \n 1st Qu.: 8.00   1st Qu.:15.00   1st Qu.: 59.25   1st Qu.:21.17  \n Median :12.60   Median :21.00   Median : 76.23   Median :23.00  \n Mean   :14.21   Mean   :21.43   Mean   : 82.68   Mean   :23.22  \n 3rd Qu.:19.00   3rd Qu.:27.00   3rd Qu.:100.35   3rd Qu.:24.80  \n Max.   :35.00   Max.   :43.00   Max.   :181.00   Max.   :33.03  \n      WHR             Waist       \n Min.   :0.6700   Min.   : 61.00  \n 1st Qu.:0.7400   1st Qu.: 72.00  \n Median :0.7800   Median : 76.00  \n Mean   :0.7815   Mean   : 76.76  \n 3rd Qu.:0.8150   3rd Qu.: 81.00  \n Max.   :0.9900   Max.   :100.80  \n\n\nAccording to the ‘tidy data’ philosophy (Wickham 2014) we want our data in long format rather than wide format. This also makes it easier to carry out later data wrangling, plotting and testing.\n\n# wide to long data\ndata_inL &lt;- pivot_longer(data_in, cols = `girths`:`Waist`, names_to = 'measure', values_to = 'value')\nhead(data_inL)\n\n# A tibble: 6 × 3\n  sex   measure   value\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 M     girths    10.8 \n2 M     bia        5.7 \n3 M     DW         9.22\n4 M     jackson    4.75\n5 M     HW        17   \n6 M     skinfolds 50.8 \n\n\nNow the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations. Exploration with plots is an essential step for checking values and the distribution of data. The tidyverse provides the ggplot2 package for this.\n\n# custom colors for male & female\nplot_cols &lt;- c('firebrick', 'cornflowerblue')\n# make the plot\nggplot(data_inL, aes(sex, value, colour = sex)) + geom_jitter(width = 0.1) + \n  scale_colour_manual(values = plot_cols) + \n  theme(legend.position = \"none\") +\n  facet_wrap(~measure, scales = \"free_y\")\n\n\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. Removing outliers is a contentious subject but here a BMI of 2 is incompatible with life! So we’ll remove this unreasonably low value.\n\n# get just bmi data\nbmi_data &lt;- data_inL %&gt;% filter(measure == \"BMI\")\n# remove low value\nbmi_data &lt;- bmi_data %&gt;% filter(value &gt; 15)\n# check with a new plot\nbmi_data %&gt;% ggplot(aes(sex, value, colour = sex)) + geom_jitter(width = 0.1, size = 3) +\n  scale_colour_manual(values = plot_cols) + \n  theme(legend.position = \"none\") \n\n\n\n\nMuch better!\n\n\nFrequentist testing\nNow let’s use a t-test to examine whether male and female BMI is different. In R basic statistical tests are easy; there are no extraneous packages to load and there’s a pretty simple ‘formula’ interface using the tilde (~). Note that by default R uses Welch’s t-test which does not assume equal variances in each group (see ?t.test).\n\n# t-test\nt.test(value ~ sex, data = bmi_data)\n\n\n    Welch Two Sample t-test\n\ndata:  value by sex\nt = -2.1134, df = 192.05, p-value = 0.03586\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -1.59338708 -0.05499779\nsample estimates:\nmean in group F mean in group M \n       22.83834        23.66253 \n\n\nThe difference between male & female BMI is significant. This means that in a hypothetical long series of repeats of this study with different samples from the same population we would expect to see a difference as big or bigger between the sexes in more than 95% of those study repeats.\n\n\nBayesian testing\nThere are several packages for Bayesian statistics in R. We’ll use the cmdstanr package to write a Bayesian model in the Stan probabilistic programming language for assessing the difference between male and female BMI. Stan will do the heavy lifting for us (Markov Chain Monte Carlo (MCMC sampling)) and return a data object we can use in R.\n\n# create data list\nsex &lt;- bmi_data %&gt;% select(sex) %&gt;% pull() # labels for participant sex\n# convert to dummy coding; females are coded as 0\nsex_dummy &lt;- ifelse(sex == 'F', 0, 1)\n# bmi values\nbmi &lt;- bmi_data %&gt;% select(value) %&gt;% pull() \n# get num subjects\nN &lt;- nrow(bmi_data) # length of dataset\n# make a list of data to pass to Stan\ndata_list &lt;- list(N = N, sex = sex_dummy, bmi = bmi)\n\n# define the model in Stan as a text string; can also pass in a separate .stan file\n# stan code is written in blocks (data, parameters, model etc) defined by {}\nmodel_string &lt;- \"\n\n// data we want to model\ndata{\n  int&lt;lower=1&gt; N; // length of the data\n  vector[N] bmi; // bmi data of length N\n  vector[N] sex; // sex data of length N\n}\n\n// parameters we want to estimate\nparameters{\n  real beta0; // intercept\n  real beta1; // slope\n  real&lt;lower=0&gt; sigma; // residual sd, must be positive\n}\n\n// priors for model\nmodel{\n  // priors\n  beta0 ~ normal(25, 10); // intercept\n  beta1 ~ normal(0, 5); // slope\n  sigma ~ normal(0,100); // defined as positive only in parameters block\n  \n  //likelihood\n  bmi ~ normal(beta0 + beta1*sex, sigma);\n}\"\n\n# write file to temp dir\nstan_mod_temp &lt;- write_stan_file(model_string, dir = tempdir())\n# create Stan model\nstan_mod &lt;- cmdstan_model(stan_mod_temp)\n# fit the model using Stan\nfit &lt;- stan_mod$sample(data = data_list, seed = 123, chains = 4, parallel_chains = 2, refresh = 500 )\n\nRunning MCMC with 4 chains, at most 2 in parallel...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.3 seconds.\n\n# summary plus diagnostics\nfit$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median    sd   mad       q5     q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -306.    -306.    1.19  0.980 -308.    -305.    1.00    1899.\n2 beta0      22.8     22.8   0.313 0.322   22.3     23.4   1.00    2021.\n3 beta1       0.813    0.812 0.406 0.405    0.136    1.47  1.00    1965.\n4 sigma       2.82     2.82  0.140 0.140    2.60     3.07  1.00    2856.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n# just the params\n# fit$summary(c(\"beta0\", \"beta1\", \"sigma\"), \"mean\", \"sd\")\n\nThe output tells us that the estimated means for female BMI is 22.8 (females were dummy coded as 0). Given the priors we used we can say that there is a 90% probability that the value for female BMI lies between 22.3 and 23.4. The estimated male BMI is 0.81 (with 90% probability of being between 0.13 & 1.48) units greater than female BMI i.e. ~23.6. The mean values are the same as estimated by the frequentist \\(t\\)-test procedure.\nTo plot the posterior distributions we can extract the posterior draws and use the bayesplot package.\n\n# get the draws; uses posterior package\ndraws &lt;- fit$draws(variables = c('beta0', 'beta1', 'sigma'))\n# plot the draws; bayesplot package\nmcmc_dens(draws)\n\n\n\n\nPlotting the posterior distribution for the male BMI is as simple as adding together the draws for beta0 and beta1.\n\n# draws to dataframe\ndraws_df &lt;- as_draws_df(draws)\n# posterior for male bmi included\nbmi_posteriors &lt;- draws_df %&gt;% mutate(male_bmi_post = beta0 + beta1)\nmcmc_dens(bmi_posteriors, pars = c('beta0', 'male_bmi_post', 'sigma'))\n\n\n\n\nThere are easier ways to create basic (and more complex) Bayesian models than writing out the Stan code by hand. The brms package allows us to write Bayesian models using R modeling syntax. The model is translated to Stan and then compiled & run.\n\n# brms bayesian modelling; same priors as above\nbrms_mod &lt;- brm(value ~ sex, data = bmi_data,\n                prior = c(prior(normal(25, 10), class = \"Intercept\"), # prior on intercept\n                          prior(normal(0, 5), class = \"b\", coef = 'sexM'), # prior on slope\n                          prior(normal(0, 100), class = \"sigma\")), # prior on resid var\n                iter = 3000, warmup = 500, chains = 4, seed = 1234)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 1: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 1: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 1: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 1: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 1: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 1: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 1: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 1: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 1: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 1: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 1: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.005785 seconds (Warm-up)\nChain 1:                0.020606 seconds (Sampling)\nChain 1:                0.026391 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 3e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 2: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 2: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 2: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 2: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 2: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 2: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 2: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 2: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 2: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 2: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 2: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.005628 seconds (Warm-up)\nChain 2:                0.020249 seconds (Sampling)\nChain 2:                0.025877 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 3: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 3: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 3: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 3: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 3: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 3: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 3: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 3: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 3: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 3: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 3: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.005748 seconds (Warm-up)\nChain 3:                0.018951 seconds (Sampling)\nChain 3:                0.024699 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 2e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 4: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 4: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 4: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 4: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 4: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 4: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 4: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 4: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 4: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 4: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 4: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.005496 seconds (Warm-up)\nChain 4:                0.021491 seconds (Sampling)\nChain 4:                0.026987 seconds (Total)\nChain 4: \n\n# model summary\nsummary(brms_mod)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: value ~ sex \n   Data: bmi_data (Number of observations: 200) \n  Draws: 4 chains, each with iter = 3000; warmup = 500; thin = 1;\n         total post-warmup draws = 10000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    22.84      0.31    22.22    23.46 1.00     9382     7692\nsexM          0.82      0.41     0.01     1.63 1.00    10240     7126\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.82      0.14     2.56     3.12 1.00     9313     7484\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe values for each coefficient are the same as both the frequentist model and the handcoded Stan model (as we’d expect).\nPlotting the model can be done with the mcmc_plot() function in brms.\n\n# plot the draws using built-in brms functions (that calls bayesplot)\n# regex  = TRUE for regular expression (^b) to pull out beta coefficients\nmcmc_plot(brms_mod, variable = c('^b', 'sigma'), type = 'dens', regex = TRUE)\n\n\n\n\nAn even easier (but less flexible) package is rstanarm."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html#summary",
    "href": "posts/hello_data_r/hello_world_data_R.html#summary",
    "title": "Hello Data World 1 (R)",
    "section": "Summary",
    "text": "Summary\nThis post has been a quick skip through some data loading, exploration, filtering and both frequentist & Bayesian modelling in R."
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html",
    "href": "posts/hello_data_python/hello_world_data_python.html",
    "title": "Hello Data World 2 (python)",
    "section": "",
    "text": "This is the second of three posts that will carry out data loading, exploration, filtering and statistical testing (frequentist & Bayesian). In the first post of the series we used R. In this post we’ll use python. Like the previous post there won’t be much exposition - we’ll just move through the process.\nIf you want to follow along the data are here.\n\n\nPython, like R, has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. The first step is to load the packages we will need. I use the Anaconda python distribution and packages that are not installed by default can be installed with the conda tool. In this post we use the pymc package for Bayesian modeling. The installation notes for pymc recommend installing it into its own python conda environment so this is what I did! To run the code in VSCode I set the relevant python interpreter by using Ctrl+Shift+P to bring up the Command Palette and selecting the relevant python environment. The other packages had to be installed into the same enviroment using conda install.\n\n\n\nSetting the python environment\n\n\nOk, let’s get on with loading the packages we’ll need!\n\nimport pandas as pd # dataframes for python\nimport plotnine as pn # ggplot2 clone\npn.options.figure_size = (5, 5) # set a default figure size for plotnine plots\npn.options.current_theme = pn.theme_bw() # set simple theme\nimport seaborn as sns # statistical plotting in python land\nsns.set_theme(style=\"whitegrid\") # plot theme\n# frequentist modeling\nimport scipy.stats as stats # classic freq stats for python\nimport pingouin as pg # alt to scipy.stats\n# bayesian modeling\nimport pymc as pm # write your models explicitly\nimport bambi as bmb # formula like interface\nimport arviz as az # plots for MCMC objects\n\n\n\n\nWe can use the read_csv() function of the pandas (Reback et al. 2020) package to read in the data. These data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a numbers of years by the students who carried out various measures on themselves.\n\n# get the data\ndata_in = pd.read_csv('data/BODY_COMPOSITION_DATA.csv', sep=',', na_values = \"NA\")\n\n\n\n\nThe pandas package also provides some tools for exploring the data.\n\ndata_in.head()\n# examine summary of types etc\ndata_in.info() # there are missing values in bia & HW\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 203 entries, 0 to 202\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        203 non-null    object \n 1   girths     203 non-null    float64\n 2   bia        202 non-null    float64\n 3   DW         203 non-null    float64\n 4   jackson    203 non-null    float64\n 5   HW         202 non-null    float64\n 6   skinfolds  203 non-null    float64\n 7   BMI        203 non-null    float64\n 8   WHR        203 non-null    float64\n 9   Waist      203 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 16.0+ KB\n\n\nWe can see that there are some missing values in the BIA and HW variables (these variables have 202 non-null values). There are many ways to deal with missing values but here we will just drop rows with missing values. The dropna() method for pandas dataframes allows us to drop rows (axis 0) or columns (axis 1) with missing values. We also specify the inplace = True argument so that the data we are working on is altered.\n\n# drop the rows (index 0) with missing values; alter dataframe (inplace = True)\ndata_in.dropna(axis = 0, inplace = True)\ndata_in.info() # all non-null values\n\n# summary stats\ndata_in.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 0 to 201\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        201 non-null    object \n 1   girths     201 non-null    float64\n 2   bia        201 non-null    float64\n 3   DW         201 non-null    float64\n 4   jackson    201 non-null    float64\n 5   HW         201 non-null    float64\n 6   skinfolds  201 non-null    float64\n 7   BMI        201 non-null    float64\n 8   WHR        201 non-null    float64\n 9   Waist      201 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 17.3+ KB\n\n\n\n\n\n\n\n\n\ngirths\nbia\nDW\njackson\nHW\nskinfolds\nBMI\nWHR\nWaist\n\n\n\n\ncount\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n\n\nmean\n20.734552\n16.980100\n21.614647\n14.212189\n21.426408\n82.684751\n23.223000\n0.781529\n76.756716\n\n\nstd\n8.382091\n6.792665\n7.307617\n7.479344\n8.006785\n33.108192\n3.168286\n0.057142\n7.352106\n\n\nmin\n7.150000\n5.700000\n4.100000\n3.000000\n4.100000\n27.750000\n2.900000\n0.670000\n61.000000\n\n\n25%\n15.080000\n11.900000\n16.300000\n8.000000\n15.000000\n59.250000\n21.170000\n0.740000\n72.000000\n\n\n50%\n20.120000\n15.900000\n21.400000\n12.600000\n21.000000\n76.230000\n23.000000\n0.780000\n76.000000\n\n\n75%\n24.400000\n21.200000\n28.000000\n19.000000\n27.000000\n100.350000\n24.800000\n0.815000\n81.000000\n\n\nmax\n87.900000\n39.300000\n45.900000\n35.000000\n43.000000\n181.000000\n33.030000\n0.990000\n100.800000\n\n\n\n\n\n\n\nNext we will convert our data from wide format to long format (Wickham 2014) with the pandas.melt() function. Long data makes plotting and statistical analyses easier. In long format data the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations.\n\n# long data\ndataL = pd.melt(data_in, id_vars = \"sex\", var_name = \"method\", value_name = \"value\")\ndataL.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n0\nM\ngirths\n10.85\n\n\n1\nM\ngirths\n14.12\n\n\n2\nM\ngirths\n12.30\n\n\n3\nM\ngirths\n8.50\n\n\n4\nM\ngirths\n11.66\n\n\n\n\n\n\n\nExploration with plots is an essential step for checking values and the distribution of data. There is an extensive plotting ecosystem in python.\n\n\n\nPython visualisation landscape (source)\n\n\nThe seaborn (Waskom 2021) package provides a high level interface for plotting data & statistical summaries. If you’re used to e.g. ggplot2 in R then the plotnine package provides very similar functionality.The tabs below demonstrate the same plot using each of these packages.\n\nseabornplotnine\n\n\n\nfg = sns.FacetGrid(dataL, col = 'method', hue = 'sex', col_wrap = 3, sharey = False); # create grid\n\nfg.map(sns.stripplot, 'sex', 'value', jitter = 0.05, size = 10, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.5, order = [\"F\", \"M\"]); # map stripplot onto grid\n\n\n\n\n\n\n\npt = pn.ggplot(dataL, pn.aes('sex', 'value', colour = 'sex')) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.facet_wrap(\"method\", scales = \"free_y\") + pn.scale_colour_manual(values=['firebrick', 'cornflowerblue'])\npt\n\n/home/iain/anaconda3/envs/pymc_env/lib/python3.11/site-packages/plotnine/facets/facet.py:440: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. First we’ll filter the data to just BMI.\n\n# filter to just bmi data\nbmi_data = dataL[dataL.method == \"BMI\"]\nbmi_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 1206 to 1406\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   sex     201 non-null    object \n 1   method  201 non-null    object \n 2   value   201 non-null    float64\ndtypes: float64(1), object(2)\nmemory usage: 6.3+ KB\n\n\n\n# first few values\nbmi_data.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n1206\nM\nBMI\n20.70\n\n\n1207\nM\nBMI\n21.90\n\n\n1208\nM\nBMI\n21.39\n\n\n1209\nM\nBMI\n19.26\n\n\n1210\nM\nBMI\n22.30\n\n\n\n\n\n\n\nWe’ll re-plot these data.\n\nseabornplotnine\n\n\n\nbmi_pt1 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt1\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\nbmi_pt2 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt2\n\n\n\n\n\n\n\nWe can clearly see the outlier in the male data. Removing outliers is a contentious subject but a BMI of 2 is unrealistic so we’ll remove this value.\n\n# note very low bmi point in M; let's drop that\nbmi_data = bmi_data[bmi_data.value &gt; 15]\n# summary\nbmi_data.describe()\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\ncount\n200.000000\n\n\nmean\n23.324615\n\n\nstd\n2.828887\n\n\nmin\n18.080000\n\n\n25%\n21.177500\n\n\n50%\n23.000000\n\n\n75%\n24.802500\n\n\nmax\n33.030000\n\n\n\n\n\n\n\n\nseabornplotnine\n\n\n\n# seaborn plot\nbmi_pt3 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt3\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\n# plotnine plot\nbmi_pt4 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt4\n\n\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#introduction",
    "href": "posts/hello_data_python/hello_world_data_python.html#introduction",
    "title": "Hello Data World 2 (python)",
    "section": "",
    "text": "This is the second of three posts that will carry out data loading, exploration, filtering and statistical testing (frequentist & Bayesian). In the first post of the series we used R. In this post we’ll use python. Like the previous post there won’t be much exposition - we’ll just move through the process.\nIf you want to follow along the data are here.\n\n\nPython, like R, has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. The first step is to load the packages we will need. I use the Anaconda python distribution and packages that are not installed by default can be installed with the conda tool. In this post we use the pymc package for Bayesian modeling. The installation notes for pymc recommend installing it into its own python conda environment so this is what I did! To run the code in VSCode I set the relevant python interpreter by using Ctrl+Shift+P to bring up the Command Palette and selecting the relevant python environment. The other packages had to be installed into the same enviroment using conda install.\n\n\n\nSetting the python environment\n\n\nOk, let’s get on with loading the packages we’ll need!\n\nimport pandas as pd # dataframes for python\nimport plotnine as pn # ggplot2 clone\npn.options.figure_size = (5, 5) # set a default figure size for plotnine plots\npn.options.current_theme = pn.theme_bw() # set simple theme\nimport seaborn as sns # statistical plotting in python land\nsns.set_theme(style=\"whitegrid\") # plot theme\n# frequentist modeling\nimport scipy.stats as stats # classic freq stats for python\nimport pingouin as pg # alt to scipy.stats\n# bayesian modeling\nimport pymc as pm # write your models explicitly\nimport bambi as bmb # formula like interface\nimport arviz as az # plots for MCMC objects\n\n\n\n\nWe can use the read_csv() function of the pandas (Reback et al. 2020) package to read in the data. These data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a numbers of years by the students who carried out various measures on themselves.\n\n# get the data\ndata_in = pd.read_csv('data/BODY_COMPOSITION_DATA.csv', sep=',', na_values = \"NA\")\n\n\n\n\nThe pandas package also provides some tools for exploring the data.\n\ndata_in.head()\n# examine summary of types etc\ndata_in.info() # there are missing values in bia & HW\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 203 entries, 0 to 202\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        203 non-null    object \n 1   girths     203 non-null    float64\n 2   bia        202 non-null    float64\n 3   DW         203 non-null    float64\n 4   jackson    203 non-null    float64\n 5   HW         202 non-null    float64\n 6   skinfolds  203 non-null    float64\n 7   BMI        203 non-null    float64\n 8   WHR        203 non-null    float64\n 9   Waist      203 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 16.0+ KB\n\n\nWe can see that there are some missing values in the BIA and HW variables (these variables have 202 non-null values). There are many ways to deal with missing values but here we will just drop rows with missing values. The dropna() method for pandas dataframes allows us to drop rows (axis 0) or columns (axis 1) with missing values. We also specify the inplace = True argument so that the data we are working on is altered.\n\n# drop the rows (index 0) with missing values; alter dataframe (inplace = True)\ndata_in.dropna(axis = 0, inplace = True)\ndata_in.info() # all non-null values\n\n# summary stats\ndata_in.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 0 to 201\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        201 non-null    object \n 1   girths     201 non-null    float64\n 2   bia        201 non-null    float64\n 3   DW         201 non-null    float64\n 4   jackson    201 non-null    float64\n 5   HW         201 non-null    float64\n 6   skinfolds  201 non-null    float64\n 7   BMI        201 non-null    float64\n 8   WHR        201 non-null    float64\n 9   Waist      201 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 17.3+ KB\n\n\n\n\n\n\n\n\n\ngirths\nbia\nDW\njackson\nHW\nskinfolds\nBMI\nWHR\nWaist\n\n\n\n\ncount\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n\n\nmean\n20.734552\n16.980100\n21.614647\n14.212189\n21.426408\n82.684751\n23.223000\n0.781529\n76.756716\n\n\nstd\n8.382091\n6.792665\n7.307617\n7.479344\n8.006785\n33.108192\n3.168286\n0.057142\n7.352106\n\n\nmin\n7.150000\n5.700000\n4.100000\n3.000000\n4.100000\n27.750000\n2.900000\n0.670000\n61.000000\n\n\n25%\n15.080000\n11.900000\n16.300000\n8.000000\n15.000000\n59.250000\n21.170000\n0.740000\n72.000000\n\n\n50%\n20.120000\n15.900000\n21.400000\n12.600000\n21.000000\n76.230000\n23.000000\n0.780000\n76.000000\n\n\n75%\n24.400000\n21.200000\n28.000000\n19.000000\n27.000000\n100.350000\n24.800000\n0.815000\n81.000000\n\n\nmax\n87.900000\n39.300000\n45.900000\n35.000000\n43.000000\n181.000000\n33.030000\n0.990000\n100.800000\n\n\n\n\n\n\n\nNext we will convert our data from wide format to long format (Wickham 2014) with the pandas.melt() function. Long data makes plotting and statistical analyses easier. In long format data the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations.\n\n# long data\ndataL = pd.melt(data_in, id_vars = \"sex\", var_name = \"method\", value_name = \"value\")\ndataL.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n0\nM\ngirths\n10.85\n\n\n1\nM\ngirths\n14.12\n\n\n2\nM\ngirths\n12.30\n\n\n3\nM\ngirths\n8.50\n\n\n4\nM\ngirths\n11.66\n\n\n\n\n\n\n\nExploration with plots is an essential step for checking values and the distribution of data. There is an extensive plotting ecosystem in python.\n\n\n\nPython visualisation landscape (source)\n\n\nThe seaborn (Waskom 2021) package provides a high level interface for plotting data & statistical summaries. If you’re used to e.g. ggplot2 in R then the plotnine package provides very similar functionality.The tabs below demonstrate the same plot using each of these packages.\n\nseabornplotnine\n\n\n\nfg = sns.FacetGrid(dataL, col = 'method', hue = 'sex', col_wrap = 3, sharey = False); # create grid\n\nfg.map(sns.stripplot, 'sex', 'value', jitter = 0.05, size = 10, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.5, order = [\"F\", \"M\"]); # map stripplot onto grid\n\n\n\n\n\n\n\npt = pn.ggplot(dataL, pn.aes('sex', 'value', colour = 'sex')) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.facet_wrap(\"method\", scales = \"free_y\") + pn.scale_colour_manual(values=['firebrick', 'cornflowerblue'])\npt\n\n/home/iain/anaconda3/envs/pymc_env/lib/python3.11/site-packages/plotnine/facets/facet.py:440: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. First we’ll filter the data to just BMI.\n\n# filter to just bmi data\nbmi_data = dataL[dataL.method == \"BMI\"]\nbmi_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 1206 to 1406\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   sex     201 non-null    object \n 1   method  201 non-null    object \n 2   value   201 non-null    float64\ndtypes: float64(1), object(2)\nmemory usage: 6.3+ KB\n\n\n\n# first few values\nbmi_data.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n1206\nM\nBMI\n20.70\n\n\n1207\nM\nBMI\n21.90\n\n\n1208\nM\nBMI\n21.39\n\n\n1209\nM\nBMI\n19.26\n\n\n1210\nM\nBMI\n22.30\n\n\n\n\n\n\n\nWe’ll re-plot these data.\n\nseabornplotnine\n\n\n\nbmi_pt1 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt1\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\nbmi_pt2 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt2\n\n\n\n\n\n\n\nWe can clearly see the outlier in the male data. Removing outliers is a contentious subject but a BMI of 2 is unrealistic so we’ll remove this value.\n\n# note very low bmi point in M; let's drop that\nbmi_data = bmi_data[bmi_data.value &gt; 15]\n# summary\nbmi_data.describe()\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\ncount\n200.000000\n\n\nmean\n23.324615\n\n\nstd\n2.828887\n\n\nmin\n18.080000\n\n\n25%\n21.177500\n\n\n50%\n23.000000\n\n\n75%\n24.802500\n\n\nmax\n33.030000\n\n\n\n\n\n\n\n\nseabornplotnine\n\n\n\n# seaborn plot\nbmi_pt3 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt3\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\n# plotnine plot\nbmi_pt4 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt4\n\n\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#frequentist-testing",
    "href": "posts/hello_data_python/hello_world_data_python.html#frequentist-testing",
    "title": "Hello Data World 2 (python)",
    "section": "Frequentist testing",
    "text": "Frequentist testing\nWe’re now in a position to undertake some statistical analysis. We’ll start with a simple t-test to examine the mean difference in BMI between males and females. The scipy.stats (Virtanen et al. 2020) library provides functions for one sample, paired & independent t-tests (and other tests). We first extract the data we want to test into separate series and then pass these series to the appropriate function. The stats.ttest_ind() function returns a tuple containing the t-statistic and the p-value for the test and we can extract these and print those. The equal_var = False argument means we get Welch’s t-test which doesn’t assume equal variances in each group.\n\n# test diff between men & women; get data\nmale_data = bmi_data[bmi_data.sex == \"M\"]\nfemale_data = bmi_data[bmi_data.sex == \"F\"]\n\n# do the test\nt_res = stats.ttest_ind(male_data.value, female_data.value, equal_var = False) # tuple out, t-stat and p-value\nt_res\n# print informative result\nprint(\"The t-statistic is %.2f with a p-value of %.3f.\" % (t_res[0], t_res[1]))\n\nThe t-statistic is 2.11 with a p-value of 0.036.\n\n\nThe pingouin (Vallat 2018) package also provides functions for statistical testing.\nUsing the ttest() function with correction = 'auto' means pingouin automatically uses Welch’s T-test when the sample sizes are unequal as they are here.\n\n# pingouin example; correction =‘auto’\npg.ttest(male_data.value, female_data.value, paired = False, correction = 'auto')\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n2.11342\n192.047574\ntwo-sided\n0.035856\n[0.05, 1.59]\n0.293662\n1.242\n0.529014\n\n\n\n\n\n\n\nThe pingouin package provides us with much more information - which may or may not be useful to you. The difference between male & female BMI is significant. This means that in a hypothetical long series of repeats of this study with different samples from the same population we would expect to see a difference as big or bigger between the sexes in more than 95% of those repeats. The pingouin package also reports the power of the test here. This is post-hoc power though & post-hoc power is witchcraft e.g. (Gelman 2019)."
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#bayesian-testing",
    "href": "posts/hello_data_python/hello_world_data_python.html#bayesian-testing",
    "title": "Hello Data World 2 (python)",
    "section": "Bayesian testing",
    "text": "Bayesian testing\nIn the previous post with R we used the Stan probabilistic programming language to create a Bayesian model for the BMI data. We could also use Stan here via the pystan interface but instead we’ll use a native python library called [pymc] (Salvatier, Wiecki, and Fonnesbeck 2016). The pymc package allows us to write data generating models and then use Markov Chain Monte Carlo (MCMC) sampling with those model definitions to generate posterior distributions. pymc supports a range of MCMC algorithms. In the code below we use the same priors we defined in the post using R.\n\n# bayesian test with pymc\n# create dummy variables; F = 0, M = 1\nbmi_data_dummy = pd.get_dummies(bmi_data, columns = [\"sex\"], drop_first = True)\n\n# set up priors & likelihood\n# https://docs.pymc.io/en/latest/api/generated/pymc.sample.html\nwith pm.Model() as model:  # model specifications in PyMC3 are wrapped in a `with` statement\n\n    # Define priors\n    sigma = pm.HalfNormal(\"sigma\", sigma = 100)\n    intercept = pm.Normal(\"Intercept\", mu = 25, sigma=10)\n    x_coeff = pm.Normal(\"male_diff\", mu = 0, sigma = 5)\n\n    # Define likelihood\n    likelihood = pm.Normal(\"value\", mu = intercept + x_coeff * bmi_data_dummy.sex_M, sigma=sigma, observed=bmi_data_dummy.value)\n\nNext we run the MCMC sampling on the model we defined above; by default the NUTS algorithm is used. This is the same MCMC algorithm as the Stan probabilistic progamming language uses by default. Using return_inferencedata = True means we can easily plot the results (see below).\n\n# MCMC sampling\n# 3 MCMC chains\n# draw 3000 posterior samples using NUTS sampling; 1000 iter burn-in\nwith model:\n    bayes_bmi = pm.sample(3000, tune = 1000, return_inferencedata = True, chains = 3)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (3 chains in 4 jobs)\nNUTS: [sigma, Intercept, male_diff]\nSampling 3 chains for 1_000 tune and 3_000 draw iterations (3_000 + 9_000 draws total) took 2 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:01&lt;00:00 Sampling 3 chains, 0 divergences]\n    \n    \n\n\nThe arviz library (Kumar et al. 2019) provides tools for summarising & plotting data from MCMC chains & posterior distributions.\n\naz.plot_trace(bayes_bmi);\n\n\n\n\nWe want the traceplots (on the right) to look like ‘hairy caterpillars’ & they all look fine here. The posterior distributions for each parameter also look healthy. We can plot the posteriors using arviz as well.\n\naz.plot_posterior(bayes_bmi, grid = (2,2), hdi_prob = 0.95);\n\n\n\n\nThe posterior distributions all look good. We can extract the intercept posterior and the posterior for the effect of ‘male’ and add these together to get the posterior for male BMI.\n\n# add Intercept & male diff posteriors; keep this new posterior in existing InferenceData object\nbayes_bmi.posterior[\"male_bmi\"] = bayes_bmi.posterior[\"Intercept\"] + bayes_bmi.posterior[\"male_diff\"]\n\n# replot with only intercept (female BMI), male BMI and sigma\naz.plot_posterior(bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"sigma\"]  , grid = (2,2), hdi_prob = 0.95);\n\n\n\n\n\n# summary\naz.summary(bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"sigma\"] , kind = \"stats\", hdi_prob = 0.9)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\n\nIntercept\n22.835\n0.316\n22.292\n23.317\n\n\nmale_bmi\n23.661\n0.258\n23.247\n24.099\n\n\nsigma\n2.826\n0.143\n2.588\n3.051\n\n\n\n\n\n\n\nThe output tells us that the estimated mean for female BMI is 22.8 (females were dummy coded as 0). Given the priors we used we can say that there is a 90% probability that the value for female BMI lies between 22.3 and 23.4. The estimated male BMI is 23.6 with 90% probability of being between 23.2 & 24. Note that the actual values might vary in the decimal point because the MCMC chains are random.\nThe bambi library (Capretto et al. 2022) can be used to create Bayesian models with a more intuitive formula interface like brms or rstanarm in R.\n\n# model with bambi\n# define priors\nprior_spec = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu = 25, sigma = 10),\n    \"sex_M\": bmb.Prior(\"Normal\", mu = 0, sigma = 5),\n    \"value_sigma\": bmb.Prior(\"HalfNormal\", sigma = 100)\n}\n\n# define the model; formula syntax\nbmb_bayes_model = bmb.Model(\"value ~ sex\", priors = prior_spec, data = bmi_data)\n# MCMC sampling; returns InferenceData obj\nbmb_bayes_bmi = bmb_bayes_model.fit(draws = 3000, tune = 1000, chains = 3)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (3 chains in 4 jobs)\nNUTS: [value_sigma, Intercept, sex]\nSampling 3 chains for 1_000 tune and 3_000 draw iterations (3_000 + 9_000 draws total) took 2 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:01&lt;00:00 Sampling 3 chains, 0 divergences]\n    \n    \n\n\nThe bmb_bayes_bmi object is of type InferenceData like that returned from pymc (bambi uses pymc under the hood). We can use the bambi result in the same way we used the pymc result with arviz.\nFirst we’ll plot the posterior distributions and plots for each MCMC chain.\n\n# plots and dists\naz.plot_trace(bmb_bayes_bmi);\n\n\n\n\nNext we’ll plot the posterior distributions and get summaries of those posteriors.\n\n# plot posteriors\naz.plot_posterior(bmb_bayes_bmi, grid = (2,2), hdi_prob = 0.95);\n\n# Key summary and diagnostic info on the model parameters\naz.summary(bmb_bayes_bmi)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n22.840\n0.309\n22.255\n23.417\n0.003\n0.002\n15248.0\n7273.0\n1.0\n\n\nsex[M]\n0.828\n0.406\n0.027\n1.549\n0.003\n0.003\n15087.0\n7029.0\n1.0\n\n\nvalue_sigma\n2.816\n0.144\n2.545\n3.086\n0.001\n0.001\n12954.0\n6515.0\n1.0\n\n\n\n\n\n\n\n\n\n\nWe get some extra information using the bambi summary.\nAs we did in the pymc3 example we can add the Intercept and sex chains together to get a posterior distribution for the male BMI and add this data to our existing `` object.\n\nbmb_bayes_bmi.posterior[\"male_bmi\"] = bmb_bayes_bmi.posterior[\"Intercept\"] + bmb_bayes_bmi.posterior[\"sex\"]\n\nWe can easily summarise & plot the parameters we are interested in.\n\n# plot selected posteriors\naz.plot_posterior(bmb_bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"value_sigma\"], grid = (2,2), hdi_prob = 0.95);\n\n# posterior summary\naz.summary(bmb_bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"value_sigma\"], kind = \"stats\", hdi_prob = 0.9)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\n\nIntercept\n22.840\n0.309\n22.322\n23.335\n\n\nmale_bmi[M]\n23.668\n0.259\n23.249\n24.091\n\n\nvalue_sigma\n2.816\n0.144\n2.570\n3.043"
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#summary",
    "href": "posts/hello_data_python/hello_world_data_python.html#summary",
    "title": "Hello Data World 2 (python)",
    "section": "Summary",
    "text": "Summary\nThis post has been a quick skip through some data loading, exploration, filtering and both frequentist & Bayesian modelling with python."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]