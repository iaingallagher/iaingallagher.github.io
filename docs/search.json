[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a public service announcement!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "iaingallagher.github.io",
    "section": "",
    "text": "Bayes rule & distributions; Inference with conjugate priors\n\n\n\n\n\n\n\nR\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBayes rule & distributions; Inference with grid approximation\n\n\n\n\n\n\n\nR\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nProbability and Bayes Rule\n\n\n\n\n\n\n\nR\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHello Data World 3 (julia)\n\n\n\n\n\n\n\njulia\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHello Data World 2 (python)\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHello Data World 1 (R)\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nQuarto with Github Pages\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2022\n\n\nIain J Gallagher\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2022\n\n\nIain J Gallagher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html",
    "href": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html",
    "title": "Bayes rule & distributions; Inference with conjugate priors",
    "section": "",
    "text": "In the last post we used Bayes theorem and grid approximation to estimate the probability of 4 different hypotheses about the proportion of students achieving a high mark in an undergraduate course. Specifically we examined proportions of 5%, 10%, 15% & 20%. For each proportion (hypothesis) we set a prior probability distribution \\(P(H)\\), used the binomial distribution to calculate the likelihood of the data we had for each of the prior probabilities, multiplied the prior probabilities and the likelihoods together and finally calculated the posterior distribution by dividing these products by their combined sum. Grid approximation is useful to see how the moving parts work but it doesn’t scale well and it’s not useful if we are dealing with continuous priors & likelihoods. For the student marks data we might want to consider any proportion for first class marks from 0 to 1.\nIf you want to follow along with the analysis later the data are here."
  },
  {
    "objectID": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#the-binomial-distribution",
    "href": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#the-binomial-distribution",
    "title": "Bayes rule & distributions; Inference with conjugate priors",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\nThe Binomial distribution tells us the probability of getting \\(s\\) successes in \\(n\\) trials if the probability of success is \\(\\theta\\). Here success is defined (non-judgmentally) as a first class mark; &gt;70% in the UK university system. The formula for the Binomial probability distribution is:\n\\(P(s) = {n \\choose s}\\theta^s(1-\\theta)^{n-s}\\)\nThe \\({n\\choose s}\\) part is called the binomial coefficient and is calculated by \\(\\frac{n!}{s!(n-s)!}\\). The \\(!\\) means factorial - \\(4! = 4 \\times 3 \\times 2 \\times 1\\).\n\n\\(s\\) is the number of successes\n\\(n\\) is the total number of trials\n\\(\\theta\\) is the proportion (or probability) of success\n\nThe Binomial distribution is a discrete distribution because the number of trials (\\(n\\)) and the number of successes (\\(s\\)) can only be integers. In the current analysis the binomial distribution is useful in the likelihood function because in any set of trials the number of successes and failures will be integers.\nWe’re going to combine the Binomial likelihood with some prior distribution to estimate the proportion of successes. The prior distribution over the proportion of successes should therefore be continuous - any proportion between 0% and 100% is possible!"
  },
  {
    "objectID": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#the-beta-distribution",
    "href": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#the-beta-distribution",
    "title": "Bayes rule & distributions; Inference with conjugate priors",
    "section": "The Beta Distribution",
    "text": "The Beta Distribution\nWe want a prior distribution that spans the range [0,1]. The continuous distribution for conjugacy in this context is the Beta distribution.\nThe Beta distribution has two parameters, \\(a\\) and \\(b\\) and is denoted as \\(Beta(a,b)\\). The density of the Beta distribution over a parameter, \\(\\theta\\) has the form:\n\\[p(\\theta| a,\\ b) = \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\\]\nwhere \\(B(a,b)\\) is the Beta function (not distribution) and acts as the normalising constant to ensure the area under the curve of the distribution sums to 1. The Beta function is:\n\\[B(a,\\ b) = \\int_0^1 d\\theta\\ \\theta^{a-1}(1-\\theta)^{b-1}\\]\nIf we use a Beta prior and a Binomial likelihood the prior and the likelihood share some mathematical form and the resulting posterior is conjugate to the prior. This just means it’s easy to calculate the posterior distribution. The ‘same mathematical form’ parts of the Binomial & Beta distributions are shown in the figure below.\n\n\n\nBeta-Binomial mathematical form equivalence\n\n\nIn both distributions we have \\(\\theta\\) raised to some power and \\(1-\\theta\\) raised to some power. This mathematical equivalence makes it easy to combine the prior and likelihood to form a posterior distribution.\n\nCalculating the posterior\nBayes rule is:\n\\[P(\\theta|data) = \\frac{P(data|\\theta)P(\\theta)}{P(data)}\\]\nHere \\(\\theta\\) is a placeholder for the proportion we are trying to estimate.\nWe calculate the posterior distribution for \\(\\theta\\) by multiplying the prior (here a \\(Beta(a,b)\\) distribution) by the likelihood - \\(P(data|\\theta)\\) - here a binomial distribution.\nThe advantage of using a conjugate prior is that it simplifies the mathematics. In this case we have (ignoring the normalising constant in the denominator):\n\\[{n \\choose s}\\theta^s(1-\\theta)^{n-s} \\times \\theta^{a-1}(1-\\theta)^{b-1}\\]\nSince we’re multiplying the same quantities (\\(\\theta\\) and \\(1-\\theta\\)) raised to different powers we can use the Laws of Exponents and all we have to do is raise \\(\\theta\\) and \\(1-\\theta\\) to the sum of powers.\nSo \\(\\theta\\) gets raised to \\(a+s-1\\) and \\(1-\\theta\\) gets raised to \\(b+n-s-1\\).\nSee here for a refresher on the Laws of Exponents.\nWe end up with a posterior distribution:\n\\[\\theta^{(a+s-1)}(1-\\theta^{(b+n-s-1)})\\]\nThe important point to note here is that our posterior distribution is:\n\\[\nBeta(a, b)\n\\] where \\(a = a+s\\) and \\(b = b+n-s\\).\nThis makes it easy to calculate - we just add some numbers together! So cool!\n\n\nShape of the beta distribution\nIn the Beta distribution \\(a\\) and \\(b\\) should both be positive and together they describe the shape of a particular Beta distribution. The first parameter, \\(a\\) can be thought of as the mode of the distribution i.e. where most of our probability will pile up. In this sense \\(a\\) can be considered the ‘number of successes’ we think there should be. If we think there should be a lot of successes then the beta distribution will pile more probability closer to 1.\nHere are several Beta distributions.\n\npar(mfrow=c(2,3)) # plotting in grid; 2 rows, 3 cols\nx &lt;- seq(0, 1, 0.01) # possible probs\na=1; b=5 # Beta(a,b) params\nfor (i in seq(0, 5)){\n    a_mod = a+i+5\n    y=dbeta(x, a_mod, b) # density of Beta distribution\n    # plot the curves\n    plot(x, y, type=\"l\", ylab='Prob', \n         xlab=expression(theta), lwd=2,\n         main = paste(paste('a=',a_mod), paste('b=',b), sep = '; '))\n} \n\n\n\n\nBeta distributions\n\n\n\n\nAs \\(a\\) increases the mode of the distribution moves right indicating more belief in an increased probability of success (\\(\\theta\\) on the x-axis). Conversely if \\(a\\) gets smaller then the mode moves left.\nThe sum, \\(a+b\\) controls the spread of the distribution and can be thought of as the ‘amount of evidence’, the plausibility, or my ‘belief’ in a particular probability of success. As \\(a\\) and \\(b\\) get bigger the sum \\(a+b\\) gets bigger and the distribution gets narrower piling more probability over some value of \\(\\theta\\).\nIf \\(a\\) and \\(b\\) are the same number and &gt; 1 then the distribution is symmetrical around 0.5. One special case is if \\(a = b = 1\\). In this case we have a uniform distribution over the whole probability range - a straight horizontal line.\nHere are some more Beta distributions to illustrate some of those points.\n\npar(mfrow=c(2, 3)) # plotting in grid\nx &lt;- seq(0, 1, 0.01) # probs\na=1; b=1\nfor (i in seq(0, 30, 6)){\n    y=dbeta(x, a+i, b+i) # Beta(a,b) density\n    #the plots\n    plot(x, y, ylim = c(0, max(y+0.05)), type=\"l\", ylab='prob', \n         xlab=expression(theta), lwd=2,\n         main = paste(paste('a=',a+i), paste('b=', b+i), sep = '; '))\n}   \n\n\n\n\nMore Beta distributions\n\n\n\n\nAs we add more ‘evidence’ (\\(a+b\\)) the Beta distribution gets narrower.\n\n\nSetting a prior distribution\nGetting back to our question of the proportion of students who get first class marks we’ll begin with flat prior i.e. we initially believe that any proportion from 0 to 100% is realistic. We can represent this belief in any proportion as a Beta distribution with \\(a\\) = 1 and \\(b\\) = 1.\nThe first year available to us is 2010 and we’ll use this data to calculate a posterior distribution for the proportion of students who get a first class mark.\n\n# load packages\nlibrary(readr)\nlibrary(dplyr)\n\n# get the data\ndata_in &lt;- read_csv('data/c5_firsts.csv')\nten &lt;- data_in %&gt;% filter(year==2010) # 2010 data\n\nsucc &lt;- ten %&gt;% tally(first) %&gt;% pull() # all successes\nn &lt;- nrow(ten) # all trials\n\nWe’ve read in the data, subset it down to just the 2010 data and calculated both the number of successes and the total number of trials.\nLet’s set up and plot our prior Beta distribution.\n\n# plot prior\na &lt;- 1\nb &lt;- 1\n# possible proportions (theta)\nx &lt;- seq(0, 1, 0.001)\n# density of x under Beta(1,1) dist\ny &lt;- dbeta(x, a, b) # prior\n# plot\nplot(x, y, type='l', xlab = expression(theta), ylab = expression(paste('P(', theta, ')')))\n\n\n\n\nA Uniform prior distribution (Beta(1,1))\n\n\n\n\nAs you can see this is a uniform distribution - the probability is exactly the same for every proportion (\\(\\theta\\)) on the x-axis.\nNow let’s calculate our posterior distribution. We just have to add the number of successes to the \\(a\\) parameter of our Beta prior; then calculate n minus successes and add that to the \\(b\\) parameter of our Beta prior. We’ll then plot our prior and our posterior together.\n\n# calculate posterior\nx &lt;- seq(0, 1, 0.001)\nnew_y &lt;- dbeta(x, a + succ, b + n - succ) # posterior - so easy!\n\n# posterior\nplot(x, new_y, type='l', lwd=2, xlab=expression(theta), ylab=expression(paste('P(', theta, ')')), col='darkorange3')\n# prior\nlines(x, y, type='l', lty=2, lwd=2)\n# labels\nlegend(x=0.6, y=max(new_y)-4, legend=c('prior', 'posterior'), col=c('black', 'darkorange3'), lty=c(2,1), bty='n')\n\n\n\n\nPrior & posterior\n\n\n\n\nSweet! We’ve updated our view of the world based on some data. Where we thought every proportion was equally plausible before data actually suggests support for about 1-20%. And all we had to do was add some numbers together!\nThe posterior distribution is a Beta distribution. Let’s see what the parameters of that distribution are:\n\n# Beta posterior parameters\na_post &lt;- a + succ\nb_post &lt;- b + n - succ\na_post\n\n[1] 5\n\nb_post\n\n[1] 58\n\n\nSo \\(a\\) is 5 and \\(b\\) is 58 and the posterior is \\(Beta(5,58)\\).\nLet’s see what the most probable proportion is. This is the mode of the posterior. The mode of a Beta distribution calculated as:\n\\[\\frac{a-1}{a+b-2}\\]\n\n# posterior mode\npost_mode &lt;- (a_post - 1)/(a_post + b_post - 2)\npost_mode\n\n[1] 0.06557377\n\n\nThe best supported proportion (\\(\\theta\\)) of first class students is about 6.6%.\nWhat about a 95% probability interval for the true proportion? Well we can get that using the qbeta() function. We have to enter the probability interval we want and then the \\(a\\) and \\(b\\) parameters of the distribution.\nIf we want a 95% interval we need to have an interval such that 5% of the distribution is excluded and this means 2.5% from each end. So we need to enter 0.025 and 0.975 as the probabilities we want in qbeta()\n\n# highest density interval\nhdi &lt;- qbeta(c(0.025, 0.975), a_post, b_post)\nhdi\n\n[1] 0.02670462 0.15702808\n\n\nThis analysis would suggest there’s a 95% probability that the interval from about 3% to about 16% contains the true proportion of first class marks based on our data and our prior assumption of a uniform distribution over \\(\\theta\\). This interval is called the Highest Density Interval (HDI) or Credible Interval (CI) and (unlike a 95% confidence interval) is directly interpretable as a probability of where \\(\\theta\\) lies.\nLet’s plot this interval.\nThe strategy for this is to plot the posterior distribution and then use the polygon() function to shade the posterior within the limits we define.\nWe first make a vector of x-values that go from the first quantile we want (0.027), along a sequence (seq(start, stop, gap)) to the last quantile we want (0.157).\nWe then make a vector of y-values to match this going from zero, up the density curve of our distribution (dbeta(seq(start, stop, gap), a, b)) and then back to zero. We plot this shape and colour fill it using the polygon() function.\n\n# create data for plot\nx &lt;- seq(0, 1, 0.001)\nnew_y &lt;- dbeta(x, a_post, b_post) # posterior - so easy!\n# plot\nplot(x, new_y, type = 'l', lwd = 2, \n     xlab = expression(theta), ylab = expression(paste('P(', theta, ')')),\n     main  = '95% HDI for Proportion',\n     col = 'darkorange3')\n# define HDI\nx_coords &lt;- c(hdi[1], seq(hdi[1], hdi[2], 0.001), hdi[2])\ny_coords &lt;- c(0, dbeta(seq(hdi[1], hdi[2], 0.001), a_post, b_post), 0)\n# plot HDI\npolygon(x_coords, y_coords, col='skyblue', border = 'darkorange3')\n\n\n\n\nPosterior with 95% HDI\n\n\n\n\nThe blue part is where we believe the true proportion, \\(\\theta\\) of first class marks lies with 95% probability.\n\n\nAn informed prior\nWe’ll now examine the student data using a more informed prior. In the previous post my initial belief was that around 10% of students would achieve a first class mark and this is somewhat supported by the analysis above. To reflect my belief in 10% I’ll create a semi-informative Beta prior around 10%.\nWhat should the parameters (\\(a\\) and \\(b\\)) of this Beta distribution be? One way to approach this is to consider that the mean proportion of successes will be \\(m = a/(a+b)\\) and the sample size (\\(n\\)) will be \\(n = a+b\\). From this we can see that \\(a = m \\times n\\) and \\(b = (1-m)n\\).\nThe value of \\(m\\) is our guess for the mean value of \\(\\theta\\) before we see data or based on our domain expertise - here that’s 0.1 (i.e. 10%). The value of \\(n\\) can be thought of as the amount of data we have previously seen which informs our prior guesstimate for \\(\\theta\\). In this case although we’re guessing we’d feel somewhat happy basing our belief on 20 previous sets of trials.\nNow we have values for \\(m\\) (0.1) and \\(n\\) (20) we can calculate \\(a\\) and \\(b\\).\n\\(a = m\\times n = 0.1\\times 20 = 2\\)\n\\(b = (1-m)\\times n = (1-0.1)\\times 20 = 18\\)\nOur prior distribution over \\(\\theta\\) is \\(Beta(2, 18)\\) & looks like this:\n\n# params\nx &lt;- seq(0, 1, 0.001)\ny &lt;- dbeta(x, 2, 18)\n# plot\nplot(x, y, type='l', lwd=2, xlab=expression(theta), ylab=expression(paste('P(', theta, ')')), main = 'Informed Prior')\n\n\n\n\nAn informed prior\n\n\n\n\nYou can see this is most strongly peaked over 10%. Now we’ll simply repeat the analysis we did above with our new, informed beta prior.\n\n# set prior\na &lt;- 2\nb &lt;- 18\n# posterior\nnew_y &lt;- dbeta(x, a + succ, b + n - succ)\n\n# plot posterior\nplot(x, new_y, type='l', lwd=2, xlab=expression(theta), ylab=expression(paste('P(', theta, ')')), col='darkorange3')\n# plot prior\nlines(x, y, type='l', lty=2, lwd=2)\n# legend\nlegend(x=0.6, y=max(new_y)-4, legend=c('prior', 'posterior'), col=c('black', 'darkorange3'), lty=c(2,1), bty='n')\n\n\n\n\nInformed prior and posterior\n\n\n\n\nThe more informed prior has led to a posterior distribution that is more tightly peaked over 10%. The mode and 95% HDI for this new posterior distribution are:\n\n# posterior params\na_post &lt;- a + succ\nb_post &lt;- b + n - succ\n# posterior mode\npost_mode &lt;- (a_post-1)/(a_post + b_post-2)\npost_mode\n\n[1] 0.06329114\n\n# hdi\nhdi &lt;- qbeta(c(0.025, 0.975), a_post, b_post)\nhdi\n\n[1] 0.02802055 0.13985708\n\n\nThe posterior with the 95% HDI looks like this:\n\n# posterior & hdi\nnew_y &lt;- dbeta(x, a_post, b_post) # posterior - so easy!\nplot(x, new_y, type = 'l', lwd = 2, \n     xlab = expression(theta), ylab = expression(paste('P(', theta, ')')),\n     main  = '95% HDI for Proportion',\n     col = 'darkorange3')\n# define HDI\nx_coords &lt;- c(hdi[1], seq(hdi[1], hdi[2], 0.001), hdi[2])\ny_coords &lt;- c(0, dbeta(seq(hdi[1], hdi[2], 0.001), a_post, b_post), 0)\n# plot HDI\npolygon(x_coords, y_coords, col='skyblue', border = 'darkorange3')\n\n\n\n\nInformed prior and posterior with HDI\n\n\n\n\n\n\nPosteriors as priors\nOne of the best features of Bayesian analysis is that it’s easy to update your knowledge as you sequentially collect more data. We can often just treat the posterior from a previous analysis as the prior for a new analysis. This kind of sequential analysis is harder in the frequentist world.\nWe have data here from 2010 until 2015. Imagine we analysed this data as we went along from year to year. We might start in 2010 with the informative prior around 10% as we did in the analysis above. To examine the 2011 data we could use the posterior distribution from 2010 as the prior distribution for 2011 and so on.\nWe do that in the code below and generate a grid of plots showing how the posterior distribution evolves over time.\n\npar(mfrow=c(2,3)) # grid for plots\n# get the data\ndata_in &lt;- read_csv('data/c5_firsts.csv')\n\nRows: 402 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): module\ndbl (2): year, first\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# get the unique years\nyears &lt;- unique(data_in$year)\n\n# set chosen prior\na &lt;- 2\nb &lt;- 18\n\n# for loop\nfor(i in 1:length(years)){\n  # subset the data\n  yr_data &lt;- data_in %&gt;% filter(year == years[i])\n    \n  # calculate successes and total trials\n  succ &lt;- sum(yr_data$first)\n  n &lt;- nrow(yr_data)\n\n  # set values for initial prior plot\n  x &lt;- seq(0,1, 0.001) \n  y &lt;- dbeta(x, a, b)\n  \n  # calculate a & b for Beta posterior\n  a &lt;- a + succ\n  b &lt;- b + n - succ\n  \n  # calculate the posterior & mode\n  new_y &lt;- dbeta( x, a, b) \n  md &lt;- (a-1)/(a+b-2) \n    \n  # make plots\n  plot(x, new_y, type='l', lwd=2, xlab=expression(theta), ylab=expression(paste('P(', theta, ')')),\n       main=years[i], col='darkorange3')\n  \n  mtext(paste('Posterior Mode: ', round(md,2), sep=''), side=3, cex=0.9)\n  \n  lines(x,y, type='l', lty=2, lwd=2, col='black')\n  \n  legend(x=0.5, y=max(new_y)-1, legend=c('prior', 'posterior'), col=c('black', 'darkorange3'), lty=c(2,1), bty='n', cex=0.8)\n}\n\n\n\n\nUpdating posteriors\n\n\n\n\nThere are a few things to note here. Firstly we can see that as we collect more data over the years the proportion of first class marks does indeed approach 10% 😁 Secondly it’s also apparent that as we collect data over the years the posterior becomes progressively narrower; we become more & more confident about the actual proportion. However even by 2015 with a total of 400 datapoints collected over the years there is still some width to the posterior. We’re close to 10% but not entirely sure!"
  },
  {
    "objectID": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#impact-of-a-poor-prior",
    "href": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#impact-of-a-poor-prior",
    "title": "Bayes rule & distributions; Inference with conjugate priors",
    "section": "Impact of a poor prior",
    "text": "Impact of a poor prior\nIn Bayesian analysis the posterior distribution is a compromise between the prior and the likelihood. The prior can therefore have quite a strong effect on the posterior - especially if data is limited. In the analysis below we see the effect that an initial prior with a mode of 0.8 has on our parameter estimate. Given that my initial guess for the proportion of students getting a first class mark was ~10% this prior, which best supports 80% is not particularly sensible.\n\npar(mfrow=c(2,3)) # plot grid\n\n# set poor prior; calculation as above but with a mean of 0.8 instead of 0.1\na &lt;- 16\nb &lt;- 4\n\n# for loop\nfor(i in 1:length(years)){\n  # subset data\n  yr_data &lt;- subset(data_in, data_in$year==years[i])\n    \n  # successes & total trials\n  succ &lt;- sum(yr_data$first)\n  n &lt;- nrow(yr_data)\n\n  # set values for initial prior plot\n  x &lt;- seq(0, 1, 0.001)\n  y &lt;- dbeta( x, a, b)\n  \n  # calculate new a & b for Beta posterior\n  a &lt;- a + succ\n  b &lt;- b + n - succ\n  \n  # calculate posterior and mode\n  new_y &lt;- dbeta( x, a, b)\n  md &lt;- (a-1)/(a+b-2)\n    \n  # plots\n  plot(x, new_y, type='l', lwd=2, xlab=expression(theta), ylab=expression(paste('P(', theta, ')')),\n       main=years[i], col='darkorange3')\n  \n  mtext(paste('Posterior Mode: ', round(md,2), sep=''), side=3, cex=0.7)\n  \n  lines(x,y, type='l', lty=2, lwd=2, col='black')\n  \n  legend(x=0.5, y=max(new_y)+0.1, legend=c('prior', 'posterior'), col=c('black', 'darkorange3'), lty=c(2,1), bty='n', cex=0.8)\n}\n\n\n\n\nData will overwhelm an uninformative prior\n\n\n\n\nAlthough the estimate of \\(\\theta\\) starts off at 80% (prior in the top left plot) we can see that the data pulls the posterior distribution over to the lower end of the possible proportions. Once we have analysed all our data the prior and posterior are approaching the same estimate as before. This demonstrates that enough data will overwhelm a poor choice of prior. However you may not always have a lot of data! The width of the prior also makes a difference. If we are very confident about a value then we might have a very narrow prior over that value. It then takes more data to shift that prior to other values."
  },
  {
    "objectID": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#other-conjugate-priors",
    "href": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#other-conjugate-priors",
    "title": "Bayes rule & distributions; Inference with conjugate priors",
    "section": "Other conjugate priors",
    "text": "Other conjugate priors\nIn this post we have concentrated on the Beta-Binomial to illustrate the idea and use of conjugate priors. However there are other conjugate combinations. The illustration below from John Cook shows some of these.\n\n\n\nCommon Conjugate Distributions\n\n\nA much fuller account of conjugate priors is given in Chapter 5 of the excellent Bayes Rules online book. The bayesrules package which accompanies the book has useful functionality for conjugate prior analysis.\n\nlibrary(bayesrules)\nlibrary(ggplot2)\n# plot prior, posterior & likelihood\nplot_beta_binomial(alpha = 2, beta = 18, y = sum(ten$first), n = nrow(ten)) + theme_bw()\n\n\n\n# summarise posterior\nsummarize_beta_binomial(alpha = 2, beta = 18, y = sum(ten$first), n = nrow(ten))\n\n      model alpha beta       mean       mode          var         sd\n1     prior     2   18 0.10000000 0.05555556 0.0042857143 0.06546537\n2 posterior     6   75 0.07407407 0.06329114 0.0008364281 0.02892107\n\n\nThe summary value for the posterior mode using the 2010 data are the same as we got above with our informed prior."
  },
  {
    "objectID": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#summary",
    "href": "posts/bayes_three_conjugate/BayesIntro3_Conjugate_Priors.html#summary",
    "title": "Bayes rule & distributions; Inference with conjugate priors",
    "section": "Summary",
    "text": "Summary\nIn this post we have looked at conjugate priors. In the context of Bayesian inference conjugacy means that the posterior distribution is in the same mathematical family as the prior distribution. Conjugate priors make Bayesian inference computationally tractable because we can go from prior to posterior with simple arithmetic. However there are not many conjugate distributions and realistic data analysis often cannot be addressed with a conjugate analysis. Usually we have to use more computationally intensive methods like Monte Carlo Markov Chain (MCMC) to define the posterior distribution. We’ll examine MCMC approaches to Bayesian inference in the next post."
  },
  {
    "objectID": "posts/hello_data_julia/hello_world_data_julia.html",
    "href": "posts/hello_data_julia/hello_world_data_julia.html",
    "title": "Hello Data World 3 (julia)",
    "section": "",
    "text": "This is the third of three posts that will carry out data loading, exploration, filtering and statistical testing using different ‘data science’ programming languages. In the first post of the series we used R; in the second post we used python. In this post we’ll use julia. I’ll add some extra commentary in this post about using julia because it’s new and not so familiar (to me anyway). If you want to follow along then the data are here.\n\n\n\n\n\n\n‘Time to first plot’ problem\n\n\n\nJulia has been designed to be fast as well as having a bunch of other advantages from modern computer science. The speed comes from the use of software called LLVM for just-in-time compilation. The developers hope it helps solve the ‘two-language problem’ where machine learning applications/data science are written in a slow high-level language like R or python and then translated to a fast lower-level language like C++ or Rust for actual use. You can read more about this here.\nHowever one consequence of just-in-time compilation is increased latency the first time any function is called because new machine code has to be compiled. In the julia community this is described as the ‘time to first plot problem’ because it can take a while to generate a plot the first time you call a plotting function (as we’ll see later). The time-to-first plot problem makes julia like the F1 car in this video (start at ~5.30 if you don’t want the whole thing). It starts later but once it gets going it flies along. The julia version used to write this post was version 1.8.5. Latency improvments are expected in julia 1.9.\nIf all this is gobbledygook then the TLDR is that the first time you do anything in julia in a fresh session it can take a while (especially plotting). Once it’s going though it goes very fast.\n\n\n\n\nLike R and python, julia has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. Julia is a young language in the data science/ numerical computing space. The version 1.0 release was only in 2018. This means that the infrastructure for analysis, data wrangling, plotting etc is not quite as stable as either R or python (although the version 1.0 release helped a lot with this). Julia packages may come and go and may or may not be maintained over the coming years. Everything I’ve used here (written in 2022) has a good level of support though and these packages should still be in existence in years to come although the exact syntax for usage might change. You can read about how to install julia packages here.\nIn any case the first step is to load the packages we will need. This will also take a while because some code is compiled at load time!\n\n# loading & wrangling data\nusing CSV, DataFrames \n\n# plotting; algebraofgraphics is built on the Makie plotting package\nusing CairoMakie, AlgebraOfGraphics\nCairoMakie.activate!(type = \"svg\") # high quality plots; note use of !\n\n# frequentist stats\nusing HypothesisTests \n\n# bayesian stats\nusing Turing, TuringGLM, ArviZ \n\n\n\n\nThe read() function from the CSV package reads in the csv formatted data. The last argument to the function (DataFrame) provides a ‘sink’ for the loaded data i.e. turns the loaded data into a DataFrame object.\n\n# get data; note defintion of missing values in function call\ndf = CSV.read(\"data/BODY_COMPOSITION_DATA.csv\", header = 1, missingstring = \"NA\", DataFrame)\nfirst(df, 5) # first 5 rows\n\n5×10 DataFrame\n\n\n\nRow\nsex\ngirths\nbia\nDW\njackson\nHW\nskinfolds\nBMI\nWHR\nWaist\n\n\n\nString1\nFloat64\nFloat64?\nFloat64\nFloat64\nFloat64?\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nM\n10.85\n5.7\n9.22\n4.75\n17.0\n50.75\n20.7\n0.8\n76.5\n\n\n2\nM\n14.12\n6.2\n11.8\n5.5\n16.9\n46.3\n21.9\n0.81\n75.0\n\n\n3\nM\n12.3\n6.3\n12.0\n5.5\n14.8\n45.8\n21.39\n0.73\n70.0\n\n\n4\nM\n8.5\n6.4\n10.85\n5.0\n10.2\n43.55\n19.26\n0.74\n68.5\n\n\n5\nM\n11.66\n6.6\n15.6\n12.0\n11.86\n93.5\n22.3\n0.78\n74.0\n\n\n\n\n\n\n\n\n\nThe DataFrames package provides tools for exploring the data.\n\n#summarise data\ndescribe(df)\n\n10×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsex\n\nF\n\nM\n0\nString1\n\n\n2\ngirths\n20.705\n7.15\n20.12\n87.9\n0\nFloat64\n\n\n3\nbia\n16.9797\n5.7\n16.2\n39.3\n1\nUnion{Missing, Float64}\n\n\n4\nDW\n21.6638\n4.1\n21.4\n45.9\n0\nFloat64\n\n\n5\njackson\n14.2333\n3.0\n12.8\n35.0\n0\nFloat64\n\n\n6\nHW\n21.4243\n4.1\n21.0\n43.0\n1\nUnion{Missing, Float64}\n\n\n7\nskinfolds\n82.881\n27.75\n76.23\n181.0\n0\nFloat64\n\n\n8\nBMI\n23.2509\n2.9\n23.0\n33.03\n0\nFloat64\n\n\n9\nWHR\n0.782105\n0.67\n0.78\n0.99\n0\nFloat64\n\n\n10\nWaist\n76.8379\n61.0\n76.0\n100.8\n0\nFloat64\n\n\n\n\n\n\nWe can see from the nmissing column that there are missing data in the HW and bia columns. The last column of this output (eltype) tells us the type of data we have & where we see Union{Missing, Float64} the column of data contains both Float64 and Missing data.\nWe can drop the rows containing missing values with the dropmissing() function. The dropmissing!() variant (i.e. with !) means we change the data ‘in place’; the actual data we loaded is changed. The use of ! like this is a common motif in julia to make in-place changes to objects (data, plots, variables etc).\n\n# ! means in-place change\nDataFrames.dropmissing!(df)\ndescribe(df)\n\n10×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsex\n\nF\n\nM\n0\nString1\n\n\n2\ngirths\n20.7346\n7.15\n20.12\n87.9\n0\nFloat64\n\n\n3\nbia\n16.9801\n5.7\n15.9\n39.3\n0\nFloat64\n\n\n4\nDW\n21.6146\n4.1\n21.4\n45.9\n0\nFloat64\n\n\n5\njackson\n14.2122\n3.0\n12.6\n35.0\n0\nFloat64\n\n\n6\nHW\n21.4264\n4.1\n21.0\n43.0\n0\nFloat64\n\n\n7\nskinfolds\n82.6848\n27.75\n76.23\n181.0\n0\nFloat64\n\n\n8\nBMI\n23.223\n2.9\n23.0\n33.03\n0\nFloat64\n\n\n9\nWHR\n0.781529\n0.67\n0.78\n0.99\n0\nFloat64\n\n\n10\nWaist\n76.7567\n61.0\n76.0\n100.8\n0\nFloat64\n\n\n\n\n\n\nThe missing value rows have been removed from the data. Next we will convert our data from wide format to long format (Wickham 2014) using the stack() function. In long format the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations. The long data format will make later plotting and statistical analyses easier.\n\n# reshape data to long\ndfl = DataFrames.stack(df, 2:10)\n# DataFrames.stack() here because TuringGLM also has a stack function; we need to be explicit about the version of stack() we want to use\nfirst(dfl, 5)\n\n5×3 DataFrame\n\n\n\nRow\nsex\nvariable\nvalue\n\n\n\nString1\nString\nFloat64\n\n\n\n\n1\nM\ngirths\n10.85\n\n\n2\nM\ngirths\n14.12\n\n\n3\nM\ngirths\n12.3\n\n\n4\nM\ngirths\n8.5\n\n\n5\nM\ngirths\n11.66\n\n\n\n\n\n\n\ndescribe(dfl)\n\n3×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsex\n\nF\n\nM\n0\nString1\n\n\n2\nvariable\n\nBMI\n\nskinfolds\n0\nString\n\n\n3\nvalue\n30.9349\n0.67\n21.6\n181.0\n0\nFloat64\n\n\n\n\n\n\nAs well as data summaries, exploration with plots is an essential step for checking values and the distribution of data. There are quite a few plotting packages for julia with various general or more specific uses. In this post we’ll use the Makie (Danisch and Krumbiegel 2021) package which seems to have good mindshare in the julia community, is being actively developed and can cover many different graphical presentation styles. To use Makie for faceted plots we can call on the AlgebraOfGraphics package which is built on Makie. If you’ve used ggplot2 in R then AlgebraOfGraphics aims to provide very similar functionality. I tried several other packages (Gadfly, StatsPlots, VegaLite) here as well but I couldn’t get any of them to produce a plot I liked. It seems as though production of facet/trellis plots with jittered data points is an area for development in julia!\nUnfortunately AlgebraOfGraphics doesn’t support jittering points (or a beeswarm plot) yet (I think jittering is useful in a plot like this so all the data can be seen) so in the code below we create boxplots rather than jittered points.\n\n# faceted boxplot of all variables coloured by Sex\nbxpl = data(dfl) * visual(BoxPlot) * mapping(:sex, :value, color =:sex, layout = :variable) # faceting is defined by layout argument\n# http://juliaplots.org/AlgebraOfGraphics.jl/dev/gallery/gallery/layout/faceting/\ncols = [\"F\" =&gt; :firebrick, \"M\" =&gt; :cornflowerblue]\n# draw() to show plot\ndraw(bxpl, facet = (;linkyaxes = :none), palettes = (color = cols,), figure=(;resolution=(800,800))) # note trailing comma needed in palettes argument (defines a tuple)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOn my systems (i7 macbook pro (2015) & i7 linux machine; both 16Gb RAM) this plot takes about 55s… time to first plot problem!\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. First we’ll filter the data to just BMI and then plot that data.\n\n# filter using anonymous function x -&gt; x == \"BMI\"; x where x = BMI\n# https://juliadatascience.io/filter_subset\nbmi = subset(dfl, :variable =&gt; ByRow(x -&gt; x == \"BMI\"))\n\n# plot just BMI\nbmipl = data(bmi) * visual(BoxPlot) * mapping(:sex, :value, color = :sex)\ndraw(bmipl, palettes = (color = cols,), figure = (; resolution = (400,400)))\n\n\n\n\nThe unrealistically low value in the male BMI is obvious.\nWe’ll filter the BMI variable to sensible values for sample (i.e. students) by only including values here BMI is &gt; 18.\n\nbmi = filter(:value =&gt; x -&gt; x &gt; 18, bmi)\n# redo plot to check\nbmipl = data(bmi) * visual(BoxPlot) * mapping(:sex, :value, color = :sex)\ndraw(bmipl, palettes = (color = cols,), figure = (; resolution = (400,400)))\n\n\n\n\nThe data look much better.\n\n\n\nWe’re now in a position to undertake some statistical analysis. We’ll start with a simple t-test to examine the mean difference in BMI between males and females. The HypothesisTesting.jl package provides functions for frequentist testing including t-tests. We first extract the data we want to test into separate series and then pass these series to the appropriate function. Here we are using the unequal variance t-test (i.e. Welch’s test).\n\n# create data vectors\nmdata = filter(:sex =&gt; x -&gt; x == \"M\", bmi).value\nfdata  = filter(:sex =&gt; x -&gt; x == \"F\", bmi).value\n# carry out test\nres = UnequalVarianceTTest(mdata, fdata)\nres\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          0.824192\n    95% confidence interval: (0.055, 1.593)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0359\n\nDetails:\n    number of observations:   [118,82]\n    t-statistic:              2.113420475300814\n    degrees of freedom:       192.04757366286913\n    empirical standard error: 0.38998033970175344\n\n\nThere is quite a lot of detail here although this is not so different from the R t.test() output. The point estimate & p-value are of most interest. We can get just the p-value using the pvalue() extractor function passing in the test and tail areas (i.e. one- or two-sided) we want. We can print a rounded p-value using string interpolation with $(thing_we_want_to_print). There doesn’t seem to be a function to extract the point estimate though… that would be handy since p-values don’t contain point estimate information.\n\npv = pvalue(UnequalVarianceTTest(mdata, fdata); tail = :both)\n# print p rounded to 3 dp\nprintln(\"The p-value for the difference in male versus female BMI is $(round(pv, digits = 3)).\")\n\nThe p-value for the difference in male versus female BMI is 0.036.\n\n\nThere’s also a confint() function for confidence intervals.\n\nci = confint(UnequalVarianceTTest(mdata, fdata); tail = :both, level = 0.95)\nprintln(\"The 95% CI for the difference in male versus female BMI is from $(round(ci[1], digits = 3)) to $(round(ci[2], digits = 3)).\")\n\nThe 95% CI for the difference in male versus female BMI is from 0.055 to 1.593.\n\n\nThe 95% CI here ranges from barely different (0.055 units larger) to quite different (1.59 units larger).\n\n\n\nLIke R and python julia has a package for the Stan probabilistic programming language called Stan.jl. So one way to write Bayesian models in julia is to use Stan.jl. However we’ll use a native julia library called Turing.jl (Ge, Xu, and Ghahramani 2018). Turing.jl allows us to write data generating models and then use Markov Chain Monte Carlo (MCMC) sampling with those model definitions to generate posterior distributions. Turing.jl supports a range of MCMC algorithms. In the code below we use the same priors we defined in the posts using R & python.\nFirst we create a dummy variable for sex such that males are coded as 1 and females are coded as 0 and we also extract the BMI values into a separate variable.\n\n# create dummy vars for sex & get value data\nindep_var = Int64.(bmi.sex .== \"M\"); # vector of 1 & 0's for M & F respectively; the . after Int64 means 'broadcast' i.e. apply to every value in the vector of M/F\n# values\ndep_var = bmi.value;\n\nNext we set up the priors and define the likelihood for the data.\n\n#  bayesian model Turing\n#  same priors as R/python\n@model function bayes_bmi(x, y)\n\n    # priors\n    α ~ Normal(25, 10) # julia allows unicode characters; \\alpha + TAB\n    β ~ Normal(0, 5) # \\beta + TAB\n    # +ve only Normal dist for residual var\n    σ ~ truncated(Normal(0, 100), lower = 0) # \\sigma + TAB\n\n    # likelihood for each y\n    for i in 1:length(y)\n        y[i] ~ Normal((α + β * x[i]), σ)\n    end\nend\n\nbayes_bmi (generic function with 2 methods)\n\n\nWe sample from the model we just set up using the NUTS algorithm (the same algorithm used by Stan by default) to create the posterior distribution.\n\n# sample; 1000 is burn in; 0.65 is acceptance rate for samples; 3000 samples; 3 chains; MCMCThreads() required to get &gt; 1 chain\n# note about threads on Turing.jl guide page: \"Be aware that Turing cannot add threads for you – you must have started your Julia instance with multiple threads to experience any kind of parallelism.\"\nbayes_bmi_result = sample(bayes_bmi(indep_var, dep_var), NUTS(1000, 0.65), MCMCThreads(), 3000, 3);\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling (1 threads):  67%|███████████████████▍         |  ETA: 0:00:00Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:00\n\n\nIn the python post we used the arviz library (Kumar et al. 2019) to visualise and summarise the distributions. The same library is available for julia as ArviZ.jl and it works in much the same way. In order to examine summaries of the posterior distributions we first convert the MCMCChains object from the posterior sampling to an InferenceData object.\n\n# convert to InferenceData object using ArviZ\nidata_bayes_bmi_result = from_mcmcchains(bayes_bmi_result)\n\nInferenceData\nposterior\nDataset with dimensions: Dim{:draw}, Dim{:chain}\nand 3 layers:\n  :α Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :σ Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :β Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" =&gt; \"2023-04-09T17:26:46.465\"\n  \"inference_library_version\" =&gt; \"6.0.0\"\n  \"inference_library\" =&gt; \"MCMCChains\"\n\n\nsample_stats\nDataset with dimensions: Dim{:draw}, Dim{:chain}\nand 12 layers:\n  :energy           Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :n_steps          Int64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :diverging        Bool dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :max_energy_error Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :energy_error     Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :is_accept        Bool dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :log_density      Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :tree_depth       Int64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :step_size        Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :acceptance_rate  Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :lp               Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :step_size_nom    Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" =&gt; \"2023-04-09T17:26:45.376\"\n  \"inference_library_version\" =&gt; \"6.0.0\"\n  \"inference_library\" =&gt; \"MCMCChains\"\n\n\n\n\nFirst we examine the posterior distributions with traceplots of the MCMC sampling process to make sure the MCMC chains converged.\n\nplot_trace(idata_bayes_bmi_result, figsize = (5,6)); # bit annoying that diff plot engines use diff units for fig sizes e.g. px vs inches\n\n\n\n\nThese all look good.\nWe can then examine summary data. ArviZ.jl uses summarystats() rather than summary() which is used by arviz in python.\n\n# show summary stats\nsummarystats(idata_bayes_bmi_result, kind = \"stats\", hdi_prob = 0.9)\n# can also get variables explicitly with var_names = \n# summarystats(idata_bayes_bmi_result, var_names = [\"α\", \"β\", \"σ\"], kind = \"stats\", hdi_prob = 0.9)\n\n3×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nα\n22.834\n0.318\n22.31\n23.344\n\n\n2\nσ\n2.823\n0.143\n2.585\n3.052\n\n\n3\nβ\n0.826\n0.41\n0.177\n1.518\n\n\n\n\n\n\nFinally we can use ArviZ to examine more detailed plots of the posterior distributions.\n\nplot_posterior(idata_bayes_bmi_result, grid=(2,2), hdi_prob = 0.9, round_to = 3, figsize = (8,5));\n\n\n\n\nIn order to assess the full posterior for male BMI we can extract the MCMC chains for the intercept and coefficient for male and add these together. This returns an Array object rather than an MCMCChains object. We convert the Array to an InferenceData object using convert_to_inference_data().\n\n# posterior for male bmi\nmale_bmi = idata_bayes_bmi_result.posterior[:α] + idata_bayes_bmi_result.posterior[:β]\n# convert to InferenceData\nmale_bmi = convert_to_inference_data(male_bmi)\n# plot\nplot_posterior(male_bmi, hdi_prob = 0.9, round_to = 3, figsize=(5,5));\n\n\n\n\nWe can generate a summary table as we did above using ArviZ.jl.\n\nsummarystats(male_bmi, kind = \"stats\", hdi_prob = 0.9)\n\n1×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n\n23.661\n0.262\n23.232\n24.084\n\n\n\n\n\n\nFrom this analysis we’d conclude that the female BMI averages 22.85 and with 90% probability ranges from 22.34 to 23.37. Male BMI is greater with an average of 23.66 (notably greater than the upper limit of the female 90% HDI) and ranging from 23.23 to 24.08 with 90% probability. These values are contingent on the priors we used.\nUsing Turing.jl we have to type the model out explicitly. If you’d prefer a formula type interface then the TuringGLM.jl library can be used to create Bayesian models in a similar manner to brms or rstanarm in R or bambi in python.\n\n\n\n\n\n\nNote\n\n\n\nTuringGLM is a work in progress and at the moment has some limitations. For hierarchical models only single random-intercept hierarchical models are supported (so no random slope models).\nCurrently TuringGLM.jl supports the following likelihoods:\n\nNormal (the default if not specified): linear regression\nTDist: robust linear regression\nBernoulli: logistic regression\nPoisson: count data regression\nNegativeBinomial: robust count data regression where there is overdispersion\n\n\n\nAs before we first have to define the priors we want (although TuringGLM does provide default priors as well).\n\n# create custom priors\n# turingGLM takes predictors first, then intercept, then auxilliary (e.g. sigma)\n# https://beta.turing.ml/TuringGLM.jl/dev/tutorials/custom_priors/ & ?CustomPrior\npriors = CustomPrior(Normal(0, 5), Normal(25, 10), truncated(Normal(0, 100), lower = 0))\n\nCustomPrior(Normal{Float64}(μ=0.0, σ=5.0), Normal{Float64}(μ=25.0, σ=10.0), Truncated(Normal{Float64}(μ=0.0, σ=100.0); lower=0.0))\n\n\nNow we can define the model using a formula interface and TuringGLM will take care of the heavy lifting for us.\n\n# bayesian model TuringGLM\n# add intercept to formula\nfrm = @formula(value ~ 1 + sex)\n# create model (formula, data; priors)\nturing_bmi_bayes = turing_model(frm, bmi; priors) # formula, data; priors... note comma & semi-colon use\n# sample from model as per Turing above\nturing_bmi_bayes_samples = sample(turing_bmi_bayes, NUTS(1000, 0.65), MCMCThreads(), 3000, 3);\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling (1 threads):  67%|███████████████████▍         |  ETA: 0:00:00Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:00\n\n\nAfter converting the MCMCChains object to an InferenceData object we can use ArviZ to summarise & plot the posterior distributions.\n\n# convert to InferenceData object using ArviZ & shpw summary stats\nidata_turing_bmi_bayes = from_mcmcchains(turing_bmi_bayes_samples)\n# show summary stats; explicit variable selection\nsummarystats(idata_turing_bmi_bayes, var_names = [\"α\", \"β\", \"σ\"] , kind = \"stats\", hdi_prob = 0.9)\n\n3×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nα\n22.843\n0.309\n22.364\n23.382\n\n\n2\nβ[1]\n0.818\n0.402\n0.151\n1.465\n\n\n3\nσ\n2.814\n0.138\n2.582\n3.034\n\n\n\n\n\n\nWe can plot the posterior distributions.\n\nplot_posterior(idata_turing_bmi_bayes, grid=(2,2), hdi_prob = 0.9, round_to = 3, figsize = (8,10));\n\n\n\n\nWe calculate the posteriors for male BMI as before by extracting the intercept and beta coefficient MCMC samples and adding them together.\n\n# calculate male bmi posterior\nturing_male_bmi = idata_turing_bmi_bayes.posterior[:α] + idata_turing_bmi_bayes.posterior[:β] # returns a 3x3000x1 Array, not an MCMCChains object\n# convert to InferenceData\nidata_turing_male_bmi = convert_to_inference_data(turing_male_bmi) # function here is convert_to_inference_data\n\nInferenceData\nposterior\nDataset with dimensions: Dim{:draw}, Dim{:chain}, Dim{:β_dim_1}\nand 1 layer:\n  : Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:β_dim_1} (3000×3×1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" =&gt; \"2023-04-09T17:27:12.686\"\n\n\n\n\nWe can summarise and plot the distribtion of male BMI as we did above.\n\n# summarise the posterior\nsummarystats(idata_turing_male_bmi, kind = \"stats\", hdi_prob = 0.9)\n\n1×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n[1]\n23.66\n0.26\n23.246\n24.1\n\n\n\n\n\n\nNow we can plot the posterior distribution for male BMI.\n\n# plot the posterior\nplot_posterior(idata_turing_male_bmi, hdi_prob = 0.9, round_to = 3, figsize = (5,5));\n\n\n\n\nThese results are essetially the same as we got from Turing.jl & the results from both Bayesian analyses are essentially the same as those we got from the frequentist analysis."
  },
  {
    "objectID": "posts/hello_data_julia/hello_world_data_julia.html#introduction",
    "href": "posts/hello_data_julia/hello_world_data_julia.html#introduction",
    "title": "Hello Data World 3 (julia)",
    "section": "",
    "text": "This is the third of three posts that will carry out data loading, exploration, filtering and statistical testing using different ‘data science’ programming languages. In the first post of the series we used R; in the second post we used python. In this post we’ll use julia. I’ll add some extra commentary in this post about using julia because it’s new and not so familiar (to me anyway). If you want to follow along then the data are here.\n\n\n\n\n\n\n‘Time to first plot’ problem\n\n\n\nJulia has been designed to be fast as well as having a bunch of other advantages from modern computer science. The speed comes from the use of software called LLVM for just-in-time compilation. The developers hope it helps solve the ‘two-language problem’ where machine learning applications/data science are written in a slow high-level language like R or python and then translated to a fast lower-level language like C++ or Rust for actual use. You can read more about this here.\nHowever one consequence of just-in-time compilation is increased latency the first time any function is called because new machine code has to be compiled. In the julia community this is described as the ‘time to first plot problem’ because it can take a while to generate a plot the first time you call a plotting function (as we’ll see later). The time-to-first plot problem makes julia like the F1 car in this video (start at ~5.30 if you don’t want the whole thing). It starts later but once it gets going it flies along. The julia version used to write this post was version 1.8.5. Latency improvments are expected in julia 1.9.\nIf all this is gobbledygook then the TLDR is that the first time you do anything in julia in a fresh session it can take a while (especially plotting). Once it’s going though it goes very fast.\n\n\n\n\nLike R and python, julia has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. Julia is a young language in the data science/ numerical computing space. The version 1.0 release was only in 2018. This means that the infrastructure for analysis, data wrangling, plotting etc is not quite as stable as either R or python (although the version 1.0 release helped a lot with this). Julia packages may come and go and may or may not be maintained over the coming years. Everything I’ve used here (written in 2022) has a good level of support though and these packages should still be in existence in years to come although the exact syntax for usage might change. You can read about how to install julia packages here.\nIn any case the first step is to load the packages we will need. This will also take a while because some code is compiled at load time!\n\n# loading & wrangling data\nusing CSV, DataFrames \n\n# plotting; algebraofgraphics is built on the Makie plotting package\nusing CairoMakie, AlgebraOfGraphics\nCairoMakie.activate!(type = \"svg\") # high quality plots; note use of !\n\n# frequentist stats\nusing HypothesisTests \n\n# bayesian stats\nusing Turing, TuringGLM, ArviZ \n\n\n\n\nThe read() function from the CSV package reads in the csv formatted data. The last argument to the function (DataFrame) provides a ‘sink’ for the loaded data i.e. turns the loaded data into a DataFrame object.\n\n# get data; note defintion of missing values in function call\ndf = CSV.read(\"data/BODY_COMPOSITION_DATA.csv\", header = 1, missingstring = \"NA\", DataFrame)\nfirst(df, 5) # first 5 rows\n\n5×10 DataFrame\n\n\n\nRow\nsex\ngirths\nbia\nDW\njackson\nHW\nskinfolds\nBMI\nWHR\nWaist\n\n\n\nString1\nFloat64\nFloat64?\nFloat64\nFloat64\nFloat64?\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nM\n10.85\n5.7\n9.22\n4.75\n17.0\n50.75\n20.7\n0.8\n76.5\n\n\n2\nM\n14.12\n6.2\n11.8\n5.5\n16.9\n46.3\n21.9\n0.81\n75.0\n\n\n3\nM\n12.3\n6.3\n12.0\n5.5\n14.8\n45.8\n21.39\n0.73\n70.0\n\n\n4\nM\n8.5\n6.4\n10.85\n5.0\n10.2\n43.55\n19.26\n0.74\n68.5\n\n\n5\nM\n11.66\n6.6\n15.6\n12.0\n11.86\n93.5\n22.3\n0.78\n74.0\n\n\n\n\n\n\n\n\n\nThe DataFrames package provides tools for exploring the data.\n\n#summarise data\ndescribe(df)\n\n10×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nType\n\n\n\n\n1\nsex\n\nF\n\nM\n0\nString1\n\n\n2\ngirths\n20.705\n7.15\n20.12\n87.9\n0\nFloat64\n\n\n3\nbia\n16.9797\n5.7\n16.2\n39.3\n1\nUnion{Missing, Float64}\n\n\n4\nDW\n21.6638\n4.1\n21.4\n45.9\n0\nFloat64\n\n\n5\njackson\n14.2333\n3.0\n12.8\n35.0\n0\nFloat64\n\n\n6\nHW\n21.4243\n4.1\n21.0\n43.0\n1\nUnion{Missing, Float64}\n\n\n7\nskinfolds\n82.881\n27.75\n76.23\n181.0\n0\nFloat64\n\n\n8\nBMI\n23.2509\n2.9\n23.0\n33.03\n0\nFloat64\n\n\n9\nWHR\n0.782105\n0.67\n0.78\n0.99\n0\nFloat64\n\n\n10\nWaist\n76.8379\n61.0\n76.0\n100.8\n0\nFloat64\n\n\n\n\n\n\nWe can see from the nmissing column that there are missing data in the HW and bia columns. The last column of this output (eltype) tells us the type of data we have & where we see Union{Missing, Float64} the column of data contains both Float64 and Missing data.\nWe can drop the rows containing missing values with the dropmissing() function. The dropmissing!() variant (i.e. with !) means we change the data ‘in place’; the actual data we loaded is changed. The use of ! like this is a common motif in julia to make in-place changes to objects (data, plots, variables etc).\n\n# ! means in-place change\nDataFrames.dropmissing!(df)\ndescribe(df)\n\n10×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsex\n\nF\n\nM\n0\nString1\n\n\n2\ngirths\n20.7346\n7.15\n20.12\n87.9\n0\nFloat64\n\n\n3\nbia\n16.9801\n5.7\n15.9\n39.3\n0\nFloat64\n\n\n4\nDW\n21.6146\n4.1\n21.4\n45.9\n0\nFloat64\n\n\n5\njackson\n14.2122\n3.0\n12.6\n35.0\n0\nFloat64\n\n\n6\nHW\n21.4264\n4.1\n21.0\n43.0\n0\nFloat64\n\n\n7\nskinfolds\n82.6848\n27.75\n76.23\n181.0\n0\nFloat64\n\n\n8\nBMI\n23.223\n2.9\n23.0\n33.03\n0\nFloat64\n\n\n9\nWHR\n0.781529\n0.67\n0.78\n0.99\n0\nFloat64\n\n\n10\nWaist\n76.7567\n61.0\n76.0\n100.8\n0\nFloat64\n\n\n\n\n\n\nThe missing value rows have been removed from the data. Next we will convert our data from wide format to long format (Wickham 2014) using the stack() function. In long format the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations. The long data format will make later plotting and statistical analyses easier.\n\n# reshape data to long\ndfl = DataFrames.stack(df, 2:10)\n# DataFrames.stack() here because TuringGLM also has a stack function; we need to be explicit about the version of stack() we want to use\nfirst(dfl, 5)\n\n5×3 DataFrame\n\n\n\nRow\nsex\nvariable\nvalue\n\n\n\nString1\nString\nFloat64\n\n\n\n\n1\nM\ngirths\n10.85\n\n\n2\nM\ngirths\n14.12\n\n\n3\nM\ngirths\n12.3\n\n\n4\nM\ngirths\n8.5\n\n\n5\nM\ngirths\n11.66\n\n\n\n\n\n\n\ndescribe(dfl)\n\n3×7 DataFrame\n\n\n\nRow\nvariable\nmean\nmin\nmedian\nmax\nnmissing\neltype\n\n\n\nSymbol\nUnion…\nAny\nUnion…\nAny\nInt64\nDataType\n\n\n\n\n1\nsex\n\nF\n\nM\n0\nString1\n\n\n2\nvariable\n\nBMI\n\nskinfolds\n0\nString\n\n\n3\nvalue\n30.9349\n0.67\n21.6\n181.0\n0\nFloat64\n\n\n\n\n\n\nAs well as data summaries, exploration with plots is an essential step for checking values and the distribution of data. There are quite a few plotting packages for julia with various general or more specific uses. In this post we’ll use the Makie (Danisch and Krumbiegel 2021) package which seems to have good mindshare in the julia community, is being actively developed and can cover many different graphical presentation styles. To use Makie for faceted plots we can call on the AlgebraOfGraphics package which is built on Makie. If you’ve used ggplot2 in R then AlgebraOfGraphics aims to provide very similar functionality. I tried several other packages (Gadfly, StatsPlots, VegaLite) here as well but I couldn’t get any of them to produce a plot I liked. It seems as though production of facet/trellis plots with jittered data points is an area for development in julia!\nUnfortunately AlgebraOfGraphics doesn’t support jittering points (or a beeswarm plot) yet (I think jittering is useful in a plot like this so all the data can be seen) so in the code below we create boxplots rather than jittered points.\n\n# faceted boxplot of all variables coloured by Sex\nbxpl = data(dfl) * visual(BoxPlot) * mapping(:sex, :value, color =:sex, layout = :variable) # faceting is defined by layout argument\n# http://juliaplots.org/AlgebraOfGraphics.jl/dev/gallery/gallery/layout/faceting/\ncols = [\"F\" =&gt; :firebrick, \"M\" =&gt; :cornflowerblue]\n# draw() to show plot\ndraw(bxpl, facet = (;linkyaxes = :none), palettes = (color = cols,), figure=(;resolution=(800,800))) # note trailing comma needed in palettes argument (defines a tuple)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOn my systems (i7 macbook pro (2015) & i7 linux machine; both 16Gb RAM) this plot takes about 55s… time to first plot problem!\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. First we’ll filter the data to just BMI and then plot that data.\n\n# filter using anonymous function x -&gt; x == \"BMI\"; x where x = BMI\n# https://juliadatascience.io/filter_subset\nbmi = subset(dfl, :variable =&gt; ByRow(x -&gt; x == \"BMI\"))\n\n# plot just BMI\nbmipl = data(bmi) * visual(BoxPlot) * mapping(:sex, :value, color = :sex)\ndraw(bmipl, palettes = (color = cols,), figure = (; resolution = (400,400)))\n\n\n\n\nThe unrealistically low value in the male BMI is obvious.\nWe’ll filter the BMI variable to sensible values for sample (i.e. students) by only including values here BMI is &gt; 18.\n\nbmi = filter(:value =&gt; x -&gt; x &gt; 18, bmi)\n# redo plot to check\nbmipl = data(bmi) * visual(BoxPlot) * mapping(:sex, :value, color = :sex)\ndraw(bmipl, palettes = (color = cols,), figure = (; resolution = (400,400)))\n\n\n\n\nThe data look much better.\n\n\n\nWe’re now in a position to undertake some statistical analysis. We’ll start with a simple t-test to examine the mean difference in BMI between males and females. The HypothesisTesting.jl package provides functions for frequentist testing including t-tests. We first extract the data we want to test into separate series and then pass these series to the appropriate function. Here we are using the unequal variance t-test (i.e. Welch’s test).\n\n# create data vectors\nmdata = filter(:sex =&gt; x -&gt; x == \"M\", bmi).value\nfdata  = filter(:sex =&gt; x -&gt; x == \"F\", bmi).value\n# carry out test\nres = UnequalVarianceTTest(mdata, fdata)\nres\n\nTwo sample t-test (unequal variance)\n------------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          0.824192\n    95% confidence interval: (0.055, 1.593)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0359\n\nDetails:\n    number of observations:   [118,82]\n    t-statistic:              2.113420475300814\n    degrees of freedom:       192.04757366286913\n    empirical standard error: 0.38998033970175344\n\n\nThere is quite a lot of detail here although this is not so different from the R t.test() output. The point estimate & p-value are of most interest. We can get just the p-value using the pvalue() extractor function passing in the test and tail areas (i.e. one- or two-sided) we want. We can print a rounded p-value using string interpolation with $(thing_we_want_to_print). There doesn’t seem to be a function to extract the point estimate though… that would be handy since p-values don’t contain point estimate information.\n\npv = pvalue(UnequalVarianceTTest(mdata, fdata); tail = :both)\n# print p rounded to 3 dp\nprintln(\"The p-value for the difference in male versus female BMI is $(round(pv, digits = 3)).\")\n\nThe p-value for the difference in male versus female BMI is 0.036.\n\n\nThere’s also a confint() function for confidence intervals.\n\nci = confint(UnequalVarianceTTest(mdata, fdata); tail = :both, level = 0.95)\nprintln(\"The 95% CI for the difference in male versus female BMI is from $(round(ci[1], digits = 3)) to $(round(ci[2], digits = 3)).\")\n\nThe 95% CI for the difference in male versus female BMI is from 0.055 to 1.593.\n\n\nThe 95% CI here ranges from barely different (0.055 units larger) to quite different (1.59 units larger).\n\n\n\nLIke R and python julia has a package for the Stan probabilistic programming language called Stan.jl. So one way to write Bayesian models in julia is to use Stan.jl. However we’ll use a native julia library called Turing.jl (Ge, Xu, and Ghahramani 2018). Turing.jl allows us to write data generating models and then use Markov Chain Monte Carlo (MCMC) sampling with those model definitions to generate posterior distributions. Turing.jl supports a range of MCMC algorithms. In the code below we use the same priors we defined in the posts using R & python.\nFirst we create a dummy variable for sex such that males are coded as 1 and females are coded as 0 and we also extract the BMI values into a separate variable.\n\n# create dummy vars for sex & get value data\nindep_var = Int64.(bmi.sex .== \"M\"); # vector of 1 & 0's for M & F respectively; the . after Int64 means 'broadcast' i.e. apply to every value in the vector of M/F\n# values\ndep_var = bmi.value;\n\nNext we set up the priors and define the likelihood for the data.\n\n#  bayesian model Turing\n#  same priors as R/python\n@model function bayes_bmi(x, y)\n\n    # priors\n    α ~ Normal(25, 10) # julia allows unicode characters; \\alpha + TAB\n    β ~ Normal(0, 5) # \\beta + TAB\n    # +ve only Normal dist for residual var\n    σ ~ truncated(Normal(0, 100), lower = 0) # \\sigma + TAB\n\n    # likelihood for each y\n    for i in 1:length(y)\n        y[i] ~ Normal((α + β * x[i]), σ)\n    end\nend\n\nbayes_bmi (generic function with 2 methods)\n\n\nWe sample from the model we just set up using the NUTS algorithm (the same algorithm used by Stan by default) to create the posterior distribution.\n\n# sample; 1000 is burn in; 0.65 is acceptance rate for samples; 3000 samples; 3 chains; MCMCThreads() required to get &gt; 1 chain\n# note about threads on Turing.jl guide page: \"Be aware that Turing cannot add threads for you – you must have started your Julia instance with multiple threads to experience any kind of parallelism.\"\nbayes_bmi_result = sample(bayes_bmi(indep_var, dep_var), NUTS(1000, 0.65), MCMCThreads(), 3000, 3);\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling (1 threads):  67%|███████████████████▍         |  ETA: 0:00:00Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:00\n\n\nIn the python post we used the arviz library (Kumar et al. 2019) to visualise and summarise the distributions. The same library is available for julia as ArviZ.jl and it works in much the same way. In order to examine summaries of the posterior distributions we first convert the MCMCChains object from the posterior sampling to an InferenceData object.\n\n# convert to InferenceData object using ArviZ\nidata_bayes_bmi_result = from_mcmcchains(bayes_bmi_result)\n\nInferenceData\nposterior\nDataset with dimensions: Dim{:draw}, Dim{:chain}\nand 3 layers:\n  :α Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :σ Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :β Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" =&gt; \"2023-04-09T17:26:46.465\"\n  \"inference_library_version\" =&gt; \"6.0.0\"\n  \"inference_library\" =&gt; \"MCMCChains\"\n\n\nsample_stats\nDataset with dimensions: Dim{:draw}, Dim{:chain}\nand 12 layers:\n  :energy           Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :n_steps          Int64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :diverging        Bool dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :max_energy_error Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :energy_error     Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :is_accept        Bool dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :log_density      Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :tree_depth       Int64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :step_size        Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :acceptance_rate  Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :lp               Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n  :step_size_nom    Float64 dims: Dim{:draw}, Dim{:chain} (3000×3)\n\nwith metadata Dict{String, Any} with 3 entries:\n  \"created_at\" =&gt; \"2023-04-09T17:26:45.376\"\n  \"inference_library_version\" =&gt; \"6.0.0\"\n  \"inference_library\" =&gt; \"MCMCChains\"\n\n\n\n\nFirst we examine the posterior distributions with traceplots of the MCMC sampling process to make sure the MCMC chains converged.\n\nplot_trace(idata_bayes_bmi_result, figsize = (5,6)); # bit annoying that diff plot engines use diff units for fig sizes e.g. px vs inches\n\n\n\n\nThese all look good.\nWe can then examine summary data. ArviZ.jl uses summarystats() rather than summary() which is used by arviz in python.\n\n# show summary stats\nsummarystats(idata_bayes_bmi_result, kind = \"stats\", hdi_prob = 0.9)\n# can also get variables explicitly with var_names = \n# summarystats(idata_bayes_bmi_result, var_names = [\"α\", \"β\", \"σ\"], kind = \"stats\", hdi_prob = 0.9)\n\n3×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nα\n22.834\n0.318\n22.31\n23.344\n\n\n2\nσ\n2.823\n0.143\n2.585\n3.052\n\n\n3\nβ\n0.826\n0.41\n0.177\n1.518\n\n\n\n\n\n\nFinally we can use ArviZ to examine more detailed plots of the posterior distributions.\n\nplot_posterior(idata_bayes_bmi_result, grid=(2,2), hdi_prob = 0.9, round_to = 3, figsize = (8,5));\n\n\n\n\nIn order to assess the full posterior for male BMI we can extract the MCMC chains for the intercept and coefficient for male and add these together. This returns an Array object rather than an MCMCChains object. We convert the Array to an InferenceData object using convert_to_inference_data().\n\n# posterior for male bmi\nmale_bmi = idata_bayes_bmi_result.posterior[:α] + idata_bayes_bmi_result.posterior[:β]\n# convert to InferenceData\nmale_bmi = convert_to_inference_data(male_bmi)\n# plot\nplot_posterior(male_bmi, hdi_prob = 0.9, round_to = 3, figsize=(5,5));\n\n\n\n\nWe can generate a summary table as we did above using ArviZ.jl.\n\nsummarystats(male_bmi, kind = \"stats\", hdi_prob = 0.9)\n\n1×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n\n23.661\n0.262\n23.232\n24.084\n\n\n\n\n\n\nFrom this analysis we’d conclude that the female BMI averages 22.85 and with 90% probability ranges from 22.34 to 23.37. Male BMI is greater with an average of 23.66 (notably greater than the upper limit of the female 90% HDI) and ranging from 23.23 to 24.08 with 90% probability. These values are contingent on the priors we used.\nUsing Turing.jl we have to type the model out explicitly. If you’d prefer a formula type interface then the TuringGLM.jl library can be used to create Bayesian models in a similar manner to brms or rstanarm in R or bambi in python.\n\n\n\n\n\n\nNote\n\n\n\nTuringGLM is a work in progress and at the moment has some limitations. For hierarchical models only single random-intercept hierarchical models are supported (so no random slope models).\nCurrently TuringGLM.jl supports the following likelihoods:\n\nNormal (the default if not specified): linear regression\nTDist: robust linear regression\nBernoulli: logistic regression\nPoisson: count data regression\nNegativeBinomial: robust count data regression where there is overdispersion\n\n\n\nAs before we first have to define the priors we want (although TuringGLM does provide default priors as well).\n\n# create custom priors\n# turingGLM takes predictors first, then intercept, then auxilliary (e.g. sigma)\n# https://beta.turing.ml/TuringGLM.jl/dev/tutorials/custom_priors/ & ?CustomPrior\npriors = CustomPrior(Normal(0, 5), Normal(25, 10), truncated(Normal(0, 100), lower = 0))\n\nCustomPrior(Normal{Float64}(μ=0.0, σ=5.0), Normal{Float64}(μ=25.0, σ=10.0), Truncated(Normal{Float64}(μ=0.0, σ=100.0); lower=0.0))\n\n\nNow we can define the model using a formula interface and TuringGLM will take care of the heavy lifting for us.\n\n# bayesian model TuringGLM\n# add intercept to formula\nfrm = @formula(value ~ 1 + sex)\n# create model (formula, data; priors)\nturing_bmi_bayes = turing_model(frm, bmi; priors) # formula, data; priors... note comma & semi-colon use\n# sample from model as per Turing above\nturing_bmi_bayes_samples = sample(turing_bmi_bayes, NUTS(1000, 0.65), MCMCThreads(), 3000, 3);\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/F9Hbk/src/sample.jl:296\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.003125\n┌ Info: Found initial step size\n└   ϵ = 0.0125\nSampling (1 threads):  67%|███████████████████▍         |  ETA: 0:00:00Sampling (1 threads): 100%|█████████████████████████████| Time: 0:00:00\n\n\nAfter converting the MCMCChains object to an InferenceData object we can use ArviZ to summarise & plot the posterior distributions.\n\n# convert to InferenceData object using ArviZ & shpw summary stats\nidata_turing_bmi_bayes = from_mcmcchains(turing_bmi_bayes_samples)\n# show summary stats; explicit variable selection\nsummarystats(idata_turing_bmi_bayes, var_names = [\"α\", \"β\", \"σ\"] , kind = \"stats\", hdi_prob = 0.9)\n\n3×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\nα\n22.843\n0.309\n22.364\n23.382\n\n\n2\nβ[1]\n0.818\n0.402\n0.151\n1.465\n\n\n3\nσ\n2.814\n0.138\n2.582\n3.034\n\n\n\n\n\n\nWe can plot the posterior distributions.\n\nplot_posterior(idata_turing_bmi_bayes, grid=(2,2), hdi_prob = 0.9, round_to = 3, figsize = (8,10));\n\n\n\n\nWe calculate the posteriors for male BMI as before by extracting the intercept and beta coefficient MCMC samples and adding them together.\n\n# calculate male bmi posterior\nturing_male_bmi = idata_turing_bmi_bayes.posterior[:α] + idata_turing_bmi_bayes.posterior[:β] # returns a 3x3000x1 Array, not an MCMCChains object\n# convert to InferenceData\nidata_turing_male_bmi = convert_to_inference_data(turing_male_bmi) # function here is convert_to_inference_data\n\nInferenceData\nposterior\nDataset with dimensions: Dim{:draw}, Dim{:chain}, Dim{:β_dim_1}\nand 1 layer:\n  : Float64 dims: Dim{:draw}, Dim{:chain}, Dim{:β_dim_1} (3000×3×1)\n\nwith metadata Dict{String, Any} with 1 entry:\n  \"created_at\" =&gt; \"2023-04-09T17:27:12.686\"\n\n\n\n\nWe can summarise and plot the distribtion of male BMI as we did above.\n\n# summarise the posterior\nsummarystats(idata_turing_male_bmi, kind = \"stats\", hdi_prob = 0.9)\n\n1×5 DataFrame\n\n\n\nRow\nvariable\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\nString\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n[1]\n23.66\n0.26\n23.246\n24.1\n\n\n\n\n\n\nNow we can plot the posterior distribution for male BMI.\n\n# plot the posterior\nplot_posterior(idata_turing_male_bmi, hdi_prob = 0.9, round_to = 3, figsize = (5,5));\n\n\n\n\nThese results are essetially the same as we got from Turing.jl & the results from both Bayesian analyses are essentially the same as those we got from the frequentist analysis."
  },
  {
    "objectID": "posts/hello_data_julia/hello_world_data_julia.html#summary",
    "href": "posts/hello_data_julia/hello_world_data_julia.html#summary",
    "title": "Hello Data World 3 (julia)",
    "section": "Summary",
    "text": "Summary\nIn this post we have used the julia language to load, wrangle, filter and plot some data. We’ve also seen how to do some basic frequentist and Bayesian inference."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html",
    "href": "posts/quarto_github_pages/blog_process.html",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Quarto is an “open-source scientific and technical publishing system built on Pandoc”. What does that mean?\nIt means that you can use a simple text file to create documents like blogs, papers, books etc. This blog will be generated using Quarto. Quarto takes simple text files written in markdown and converts those documents into a number of different formats.\nQuarto allows you to embed code written in R, python, julia and Observable JS into your documents and that code will be executable. This makes it easy to share code for analysis or teaching or reminding yourself how things work!.\nBelow I’ll document how I set up this blog on Github pages using Quarto.\n\n\nFirst you’ll need to have the version control sofware git installed on your system. If you’re not familiar with git then there’s some good teaching here. Once you have git installed you have to set your identity.\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"YourEmail@XYZ.com\"\nThe email you use should be the same one you’ll use when you sign up for Github.\nYou can also set a default branch for your code. I use main for this. This is where the final code for the blog will live.\ngit config --global init.defaultBranch main\nYou’ll also need an account on Github. Github provides a free service called Github pages that allows you to host a free website.\nYou’ll also need to install Quarto.\nOnce we have git & Quarto installed and a Github account ready the workflow we’ll follow here is:\n\nCreate a blog repository on Github\nCopy (clone) that repository to our local computer\nSetup the blog structure in the local repository using Quarto\nCreate content for the blog using Quarto\nPush the content to the online repository\n\nOnce we’ve done the first three steps the last two steps can be repeated as we add new blog posts.\nI’ll be using a Unix based operating system either Mac OSX or linux & we’ll be using the command line. If you want to follow the process on Windows you’ll probably need to change a few commands used to create files or change directories from Unix commands to Windows commands. The git and quarto commands will all remain the same.\nOnce you have installed git and set up your git identity go to Github, create a free account and set up the various security options. Install Quarto on your computer.\n\n\n\nOn Github create a new repo named your_github_username.github.io replacing your_github_username with your actual Github username. Do not add a README or license file just now.\nOn your local computer change to the directory you want to use for your blog and clone the Github repo into that directory.\ncd ~/blog_directory\ngit clone git@github.com:THE_REPO_ADDRESS\nif you’re using ssh.\ncd ~/blog_directory\ngit clone https://github.com/THE_REPO_ADDRESS\nif you’re using https.\nThis will download the files on Github into the local directory. You’ll probably get a warning like:\nwarning: You appear to have cloned an empty repository.\nThat’s ok… you have cloned an empty repository!\n\n\n\nThere are full instructions here.\nChange into the local repo you just cloned.\ncd ~/blog_directory/your_github_username.github.io\nCreate a local copy of your blog by typing:\nquarto create-project --type website:blog\nQuarto will create several files and directories required to create the blog:\n\n_quarto.yml\nindex.qmd\nabout.qmd\nposts/\nposts/_metadata.yml\nstyles.css\n\nAdd an empty file named .nojekyll at the top level:\n# change directory\ncd your_github_username.github.io\n# add a file\ntouch .nojekyll\nThis is required so that Github pages will serve our blog properly later. See the Github pages section here.\nWe also need to set the output directory in the _quarto.yml file so the top of the file reads:\nproject:\n  type: website\n  output-dir: docs\nIf you want to add social media details you can make edits to the about.qmd file (see the webpage above).\n\n\n\nMove into the new directory created by Quarto:\ncd your_github_username.github.io\nType git init. This will tell git to start tracking the files in the blog directory. You can make sure this is working by typing git status & git should list all the directories and files in the your_github_username.github.io directory. You should also see that you are on the main branch.\n\n\n\nGit allows us to create different branches for projects we are working on. At the moment we only have one branch in our blog project - the main branch. For adding content etc we do not want to work on the main branch; we want the main branch to be the destination for code/posts we know we want to publish.\nOn your local machine create a new branch in the repo called e.g. adding-content.\ngit checkout -b adding-content\nYou should see a message Switched to a new branch 'adding-content'.\nAs you create content you will create that content on the adding-content branch. Once you’re happy with that content you can merge the adding-content and main branches. That way you’re never going to ‘break’ the main branch (the stuff you will actually blog) with code/content that doesn’t work.\nBefore we go any further we’ll push everything we have done to the adding-content branch on our local machine.\n# make sure we're on the adding-content branch\ngit status\n# add all the files & changes; . means add everything\ngit add .\n# commit the changes to the git repo\ngit commit -m \"started adding-content branch\"\n# check\ngit status\nYou should see a message:\nOn branch adding-content\nnothing to commit, working tree clean\n\n\n\nThe workflow to create content is to write in markdown and then use the tools in Quarto to render the markdown to html. If you made your blog repo as above with Quarto then Quarto will take care of adding blog posts to the index page of your blog.\nCreate a new Quarto file written in markdown. As you’re doing so you can check what the page will look like using the Render button in RStudio orVS Code if you’re using either of these for your writing.\nOnce you’re satisfied with the markdown file create a new folder in the /posts directory of your blog repo. Give the folder an informative name.\n# change to posts dir\ncd your_github.username.github.io/posts\n# create new dir to hold current content\nmkdir quarto_github_pages\n# move .qmd file to posts/new_dir\ncp path/to/content.qmd posts/quarto_github_pages\nNow we can add this file to git.\n# check your on adding content\ngit status\n# add the file\ngit add posts/quarto_github_pages/content.qmd\n# commit the file & add a message\ngit commit -m \"added first post to blog\"\n# check all is well\ngit status\n\n\n\nNow we have some content we can use quarto to render the actual site. Make sure you are in the top level directory of your blog (e.g. your_github_username.github.io) and type quarto preview to preview the site.\nIf it all looks good press ctrl+c to stop the site being served.\nYou can now render the site with quarto render. This will add several new files & directories.\nAdd these to the adding-content branch following the git workflow.\ngit status # make sure you are on adding-content branch\ngit add .\ngit status\ngit commit -m \"some useful commit message\"\ngit status\nNow we’re ready to send the site to Github.\n\n\n\nWe want to push the files we have created to the adding-content branch online.\ngit push origin adding-content\nGo to Github and log in if you have to. You should see a message that the adding-content branch has had content pushed to it.\n\n\n\nPull request\n\n\nPress the green button and follow the prompts to merge your changes to the main branch on Github.\nOn the GitHub page for your repo go to Settings (top right of the page) & scroll down until you see the section for GitHub pages. Click on the Source (it probably says Deploy from a branch) & select /docs.\n\n\n\nSet serve directory\n\n\nPress save. This tells GitHub Pages to look in themain branch of the project and in the docs directory for files to serve.\n\n\n\nSo far we have made all our changes to the local adding-content branch, sent these changes to the adding-content branch online and merged those changes with the main branch online. Our local main branch needs to be synchronised to the online main branch.\nIn your local blog directory checkout your main branch.\ngit checkout main\nNow pull the content from the online main branch down to your local machine.\ngit pull origin main\nYour local main branch and your online main branch will now be the same.\n\n\n\nA lot of what we did above was once off. For subsequent posts we will:\n\nDraft content in markdown\nTrack that content on the local adding-content branch\n\ngit checkout adding-content\ngit status # make sure you're on adding-content\ngit add XYZ\ngit status\ngit commit -m \"some commit message\"\ngit status\n\nPush the adding-content branch to Github when we’re ready to publish the page\n\ngit push origin adding-content\n\nMerge the changes to the main branch online using a Github pull request\n\nGithub should automatically serve the page we created.\n\nPull the finalised main branch from Github back to the local machine\n\ngit checkout main   \ngit pull origin main"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#preliminaries",
    "href": "posts/quarto_github_pages/blog_process.html#preliminaries",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "First you’ll need to have the version control sofware git installed on your system. If you’re not familiar with git then there’s some good teaching here. Once you have git installed you have to set your identity.\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"YourEmail@XYZ.com\"\nThe email you use should be the same one you’ll use when you sign up for Github.\nYou can also set a default branch for your code. I use main for this. This is where the final code for the blog will live.\ngit config --global init.defaultBranch main\nYou’ll also need an account on Github. Github provides a free service called Github pages that allows you to host a free website.\nYou’ll also need to install Quarto.\nOnce we have git & Quarto installed and a Github account ready the workflow we’ll follow here is:\n\nCreate a blog repository on Github\nCopy (clone) that repository to our local computer\nSetup the blog structure in the local repository using Quarto\nCreate content for the blog using Quarto\nPush the content to the online repository\n\nOnce we’ve done the first three steps the last two steps can be repeated as we add new blog posts.\nI’ll be using a Unix based operating system either Mac OSX or linux & we’ll be using the command line. If you want to follow the process on Windows you’ll probably need to change a few commands used to create files or change directories from Unix commands to Windows commands. The git and quarto commands will all remain the same.\nOnce you have installed git and set up your git identity go to Github, create a free account and set up the various security options. Install Quarto on your computer."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#github-set-up",
    "href": "posts/quarto_github_pages/blog_process.html#github-set-up",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "On Github create a new repo named your_github_username.github.io replacing your_github_username with your actual Github username. Do not add a README or license file just now.\nOn your local computer change to the directory you want to use for your blog and clone the Github repo into that directory.\ncd ~/blog_directory\ngit clone git@github.com:THE_REPO_ADDRESS\nif you’re using ssh.\ncd ~/blog_directory\ngit clone https://github.com/THE_REPO_ADDRESS\nif you’re using https.\nThis will download the files on Github into the local directory. You’ll probably get a warning like:\nwarning: You appear to have cloned an empty repository.\nThat’s ok… you have cloned an empty repository!"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#create-the-blog",
    "href": "posts/quarto_github_pages/blog_process.html#create-the-blog",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "There are full instructions here.\nChange into the local repo you just cloned.\ncd ~/blog_directory/your_github_username.github.io\nCreate a local copy of your blog by typing:\nquarto create-project --type website:blog\nQuarto will create several files and directories required to create the blog:\n\n_quarto.yml\nindex.qmd\nabout.qmd\nposts/\nposts/_metadata.yml\nstyles.css\n\nAdd an empty file named .nojekyll at the top level:\n# change directory\ncd your_github_username.github.io\n# add a file\ntouch .nojekyll\nThis is required so that Github pages will serve our blog properly later. See the Github pages section here.\nWe also need to set the output directory in the _quarto.yml file so the top of the file reads:\nproject:\n  type: website\n  output-dir: docs\nIf you want to add social media details you can make edits to the about.qmd file (see the webpage above)."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#setting-git-to-track-the-blog",
    "href": "posts/quarto_github_pages/blog_process.html#setting-git-to-track-the-blog",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Move into the new directory created by Quarto:\ncd your_github_username.github.io\nType git init. This will tell git to start tracking the files in the blog directory. You can make sure this is working by typing git status & git should list all the directories and files in the your_github_username.github.io directory. You should also see that you are on the main branch."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#adding-a-content-branch",
    "href": "posts/quarto_github_pages/blog_process.html#adding-a-content-branch",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Git allows us to create different branches for projects we are working on. At the moment we only have one branch in our blog project - the main branch. For adding content etc we do not want to work on the main branch; we want the main branch to be the destination for code/posts we know we want to publish.\nOn your local machine create a new branch in the repo called e.g. adding-content.\ngit checkout -b adding-content\nYou should see a message Switched to a new branch 'adding-content'.\nAs you create content you will create that content on the adding-content branch. Once you’re happy with that content you can merge the adding-content and main branches. That way you’re never going to ‘break’ the main branch (the stuff you will actually blog) with code/content that doesn’t work.\nBefore we go any further we’ll push everything we have done to the adding-content branch on our local machine.\n# make sure we're on the adding-content branch\ngit status\n# add all the files & changes; . means add everything\ngit add .\n# commit the changes to the git repo\ngit commit -m \"started adding-content branch\"\n# check\ngit status\nYou should see a message:\nOn branch adding-content\nnothing to commit, working tree clean"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#adding-content",
    "href": "posts/quarto_github_pages/blog_process.html#adding-content",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "The workflow to create content is to write in markdown and then use the tools in Quarto to render the markdown to html. If you made your blog repo as above with Quarto then Quarto will take care of adding blog posts to the index page of your blog.\nCreate a new Quarto file written in markdown. As you’re doing so you can check what the page will look like using the Render button in RStudio orVS Code if you’re using either of these for your writing.\nOnce you’re satisfied with the markdown file create a new folder in the /posts directory of your blog repo. Give the folder an informative name.\n# change to posts dir\ncd your_github.username.github.io/posts\n# create new dir to hold current content\nmkdir quarto_github_pages\n# move .qmd file to posts/new_dir\ncp path/to/content.qmd posts/quarto_github_pages\nNow we can add this file to git.\n# check your on adding content\ngit status\n# add the file\ngit add posts/quarto_github_pages/content.qmd\n# commit the file & add a message\ngit commit -m \"added first post to blog\"\n# check all is well\ngit status"
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#render-the-site",
    "href": "posts/quarto_github_pages/blog_process.html#render-the-site",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "Now we have some content we can use quarto to render the actual site. Make sure you are in the top level directory of your blog (e.g. your_github_username.github.io) and type quarto preview to preview the site.\nIf it all looks good press ctrl+c to stop the site being served.\nYou can now render the site with quarto render. This will add several new files & directories.\nAdd these to the adding-content branch following the git workflow.\ngit status # make sure you are on adding-content branch\ngit add .\ngit status\ngit commit -m \"some useful commit message\"\ngit status\nNow we’re ready to send the site to Github."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#push-blog-to-github",
    "href": "posts/quarto_github_pages/blog_process.html#push-blog-to-github",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "We want to push the files we have created to the adding-content branch online.\ngit push origin adding-content\nGo to Github and log in if you have to. You should see a message that the adding-content branch has had content pushed to it.\n\n\n\nPull request\n\n\nPress the green button and follow the prompts to merge your changes to the main branch on Github.\nOn the GitHub page for your repo go to Settings (top right of the page) & scroll down until you see the section for GitHub pages. Click on the Source (it probably says Deploy from a branch) & select /docs.\n\n\n\nSet serve directory\n\n\nPress save. This tells GitHub Pages to look in themain branch of the project and in the docs directory for files to serve."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#syncing-online-and-local",
    "href": "posts/quarto_github_pages/blog_process.html#syncing-online-and-local",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "So far we have made all our changes to the local adding-content branch, sent these changes to the adding-content branch online and merged those changes with the main branch online. Our local main branch needs to be synchronised to the online main branch.\nIn your local blog directory checkout your main branch.\ngit checkout main\nNow pull the content from the online main branch down to your local machine.\ngit pull origin main\nYour local main branch and your online main branch will now be the same."
  },
  {
    "objectID": "posts/quarto_github_pages/blog_process.html#workflow-process",
    "href": "posts/quarto_github_pages/blog_process.html#workflow-process",
    "title": "Quarto with Github Pages",
    "section": "",
    "text": "A lot of what we did above was once off. For subsequent posts we will:\n\nDraft content in markdown\nTrack that content on the local adding-content branch\n\ngit checkout adding-content\ngit status # make sure you're on adding-content\ngit add XYZ\ngit status\ngit commit -m \"some commit message\"\ngit status\n\nPush the adding-content branch to Github when we’re ready to publish the page\n\ngit push origin adding-content\n\nMerge the changes to the main branch online using a Github pull request\n\nGithub should automatically serve the page we created.\n\nPull the finalised main branch from Github back to the local machine\n\ngit checkout main   \ngit pull origin main"
  },
  {
    "objectID": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html",
    "href": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html",
    "title": "Bayes rule & distributions; Inference with grid approximation",
    "section": "",
    "text": "In the first post in this series I introduced some basic probability rules and used them to derive Bayes rule.\n\\[\nP(B|A) = \\frac{P(A|B)P(B)}{P(A)}\n\\]\nBayes rule allows us to ‘invert’ conditional probabilities & this is surprisingly useful!\nIn the first post I used Bayes rule to calculate the probability of a being on a particular degree programme given you earned a first class mark and also the probability of actually having a disease after testing positive for the disease. These examples both used single probability values but Bayes rule really shines when we apply it to entire probability distributions. This can be harder to wrap your head around. In this post we’ll use a technique called grid approximation to illustrate how all the moving parts fit together. We’ll see how Bayes rule allows us to use an entire probability distribution to test multiple hypotheses together.\n\n\n\nA probability distribution is the mapping of disjoint outcomes in an event space to the probability of those outcomes.\nThere are three rules for constructing probability distributions:\n\nThe outcomes listed must be disjoint i.e. they cannot occur at the same time\nEach probability must be between 0 and 1\nThe total probability in the event space and therefore in the distribution must sum up to 1\n\nOver the years statisticians have developed many idealised probability distributions for different kinds of outcome space. These distributions have specific mathematical formulae. They are idealised in that they describe the probability of outcomes well enough to be useful but they are not exact models of the real world. The well known Normal distribution is an example.\nIf probability distributions are unfamiliar to you and you want more detail then Chapter 2 of the OpenIntro Statistics is a good place to start.\n\n\nA key difference between the Bayesian and NHST/frequentist approaches to inference centers on the philosophical stance around probability. For frequentists ‘probability’ of an outcome is defined as the number of times that outcome is seen in a long run of ‘experiments’. The Bayesian stance views probability as something more subjective; we define probability in terms of our expectation based on experience & knowledge about the world; probability can be interpreted as the ‘plausibility’ of an outcome.\nOne of the key differences this makes to inference (which is just educated guessing) is that Bayesians can easily incorporate prior knowledge. Incorporating prior knowledge should be part of principled scientific inference; we should build on what went before. However incorporating prior knowledge is hard for frequentists. For Bayesians the probability of one off events like life on Mars is easy; we just assign this some prior probability then collect data and ‘do’ Bayes! For frequentists the probability of life on Mars is difficult to define. What does it mean to have a frequentist infinity of Mars (what would the plural be? Marses? Marsii? 😕) to examine for the presence of life?\nThe dominant statistical paradigm in use is frequentist null hypothesis significance testing (NHST). For analysis of scientific studies in e.g. the biomedical field where I mostly work, this system relies on an assumption of a long run of study repeats (the frequentist part). But study repeats are rarely done and when they are the results are usually disappointing for statistical and other reasons (Amaral and Neves (2021), Errington et al. (2021), OPEN SCIENCE COLLABORATION (2015)). NHST only examines the probability of the data you have (or more extreme data) under a null hypothesis. The frequentist procedures use a p-value to inform on this null hypothesis. Notably the p-value is a conditional probability; it is the probability of the data you have or data more extreme given the null hypothesis is true. In maths:\n\\[\np\\mbox{-}value = P(data|H_0)\n\\]\nIf we assume the null hypothesis is true then a low p-value counts as ‘some evidence’ against the null… but how much evidence? The p-value tells us the probability of data not the probability of the null hypothesis; in fact we assume the null is true so the probability of the null (or ‘chance alone’ irrespective of the p-value) is 100% under this assumption!\nThis probability - \\(P(data|H_0)\\) - is not usually the probability we want. We usually want the ‘inverse conditional’:\n\\[\nP(H_0|data)\n\\]\nThat is, the probability of the null hypothesis given the data. NHST does not give us this.\nThere are no shortage of other criticisms of the NHST system. For example NHST tells you nothing about any alternative hypotheses. We’ll see below onw way a Bayesian approach can deal with this.\nFinally NHST (as usually used) is a combination of the Fisherian approach (significance testing) and the Neyman-Pearson (hypothesis testing) approaches to inference. These systems are not compatible and whilst each of these alone is coherent the hybrid that is NHST is not coherent.\n\nConfusion surrounding the reporting and interpretation of results of classical statistical tests is widespread among applied researchers, most of whom erroneously [my emphasis] believe that such tests are prescribed by a single coherent theory of statistical inference… In particular, there is a widespread failure to appreciate the incompatibility [my emphasis] of Fisher’s evidential p-value with the Type I error rate, \\(\\alpha\\), of Neyman-Pearson statistical orthodoxy.\n\n(Hubbard and Bayarri 2003)\nAll that is not to say frequentist statistics is not useful - it’s just sensible to know what you’re getting for your money.\nBayes rule is a straightforward combination of basic probability rules and as such inference based on Bayes rule is the result of a coherent system. If you want to read more about the incoherence in NHST then two accessible accounts are given by Cohen (1994) & Hubbard and Bayarri (2003).\nAnyway philosophical rant aside let’s look at how Bayes rule can help us.\n\n\nWe can re-state Bayes rule in terms of data and models - models are just ‘events’ in some probability space.\n\\[\nP(Model|Data) = \\frac{P(Data|Model)P(Model)}{P(Data)}\n\\]\nWe have some model under consideration (e.g. a ‘null’ hypothesis) and we can define the prior probability of that model. This is the \\(P(Model)\\) part. Conceptually this is the plausibility of the model expressed as a probability before we see any data.\nWe combine the prior with the likelihood - \\(P(Data|Model)\\) - of the \\(Data\\) under each possible parameter in the the \\(Model\\). The product in the numerator is then normalized to a probability by taking the probability of the \\(Data\\) over all possible model parameters in the prior. This sum of products - \\(P(Data)\\) - is often called the evidence. The calculation on the right hand side then gives us the probability of our \\(Model\\) given the \\(Data\\) - \\(P(Model|Data)\\) - the posterior.\n\n\n\n\nAll of the above is pretty abstract and will be easier with an example. Suppose I believe that 10% of students in the previous post are capable of scoring 70% or more in an exam (in the UK system this is a first class mark). My belief in 10% is not absolute though… I might consider 5%, 10%, 15% & 20% to be plausible. A Bayesian approach allows me to examine each of these hypotheses (\\(models\\)) given some \\(data\\).\nBefore we go down the Bayesian route though let’s take a look at the usual frequentist NHST approach to this question. If you want to follow along the data for this analysis are here.\n\n\nThe marks for 2015 show that 8 out of 104 students got a mark of 70% or more, a proportion of 7.7%. How can I compare this data to my belief that 10% of students are capable of a mark of 70% or more?\nThe outcome here is binary i.e. each student can score 70% or more (a ‘success’) or less than 70% (a ‘failure’; no judgement - ‘success’& ‘failure’ are just conventional labels for binary outcomes). We have some number of trials (each student represents a trial) and some number of successes. In a situation like this we can use the binomial distribution to calculate a p-value from the data we have. In R the binom.test() function does the heavy lifting. We enter the number of ‘successes’ (8), the total number of trials (104) and the probability of success under the null hypothesis. Since I believe 10% of students are capable of a first class mark the probability of success under the null hypothesis here is 0.1.\n\n# binom.test(successes, total trials, prob success)\nbinom.test(8, 104, 0.1)\n\n\n    Exact binomial test\n\ndata:  8 and 104\nnumber of successes = 8, number of trials = 104, p-value = 0.5154\nalternative hypothesis: true probability of success is not equal to 0.1\n95 percent confidence interval:\n 0.03379462 0.14595163\nsample estimates:\nprobability of success \n            0.07692308 \n\n\nThe interpretation here is that if the true probability of success is 10% then the probability of 8 succsses out of 104 trials (7.7%) or a more extreme proportion is about 51% (the p-value). Conventionally we would fail to reject the null (note this is not the same as accept the null) at conventional 5% significance. I might tentatively conclude that the data somewhat support my guess that 10% of students are capable of a first class mark.\nThis tells me something about an assumed proportion of 10% but what if I want to test other possible proportions like 5%, 15% or 20%? I could do more binomial tests but then I run into multiple comparison problems (Tukey 1991) because I want to control the type 1 error rate at 5%. Note also I have no power calculation here so I can’t actually set a number of observations to control the type 1 error rate. So what does my p-value mean in a strict NHST sense? Note that post-hoc power is not (ever) the right thing to do e.g. (Hoenig and Heisey 2001), (Gelman 2019).\n\n\n\n\nLet’s move on to a Bayesian analysis. I’ll use a simple grid with values representing each of four plausible proportions (5%, 10%, 15% & 20%) and use Bayes rule to update my belief in each proposed proportion in light of the data I have. In order to use Bayes rule I first have to set some prior belief over each of the proposed proportions. I can create probabilities for each proportion by first weighting each proportion arbitrarily for plausibility.\n\n# set 'plausibility' on proportions of 5, 10, 15 & 20%\nprops &lt;- seq(0.05, 0.2, 0.05) # my proportions; start, stop, step\nwgts &lt;- c(4, 8, 3, 0.5) # arbitrary plausibility weight for each proportion; most on 0.1\ncbind(props, wgts)\n\n     props wgts\n[1,]  0.05  4.0\n[2,]  0.10  8.0\n[3,]  0.15  3.0\n[4,]  0.20  0.5\n\n\nNow I can divide each individual weight through by the total weight thus scaling the weights to lie in the interval [0,1] and also making sure they sum to 1. In effect I have created a probability distribution.\n\npriors &lt;- round(wgts/sum(wgts),2) # convert weights to probabilities\ncbind(props, wgts, priors)\n\n     props wgts priors\n[1,]  0.05  4.0   0.26\n[2,]  0.10  8.0   0.52\n[3,]  0.15  3.0   0.19\n[4,]  0.20  0.5   0.03\n\n\nThe priors column in the above represents the plausibility of each proportion. Note that most of my prior probability is on 10%. I don’t place much probability on 20%; first class marks should be hard to get otherwise they’re rather meaningless!\n\n\n\nThe Wisdom of Buddy Pine\n\n\nThese discrete probabilities make a probability distribution.\n\nplot(props, priors, type='h', xlab = 'Proportion', ylab='Probability', main = 'Prior distribution', lwd=4)\npoints(props, priors, pch=16, cex=2)\n\n\n\n\nThe prior probability distribution\n\n\n\n\nAs before the marks for 2015 show that 8 out of 104 students achieved a mark of 70% or more, a proportion of 7.7%. How should this information change my belief about each of the proportions defined above?\nI can use Bayes rule to calculate this.\n\\[\nP(\\theta | D) = \\frac{P(D | \\theta)P(\\theta)}{P(D)}\n\\]\nHere \\(\\theta\\) (the Greek letter theta) is my proportion (5%, 10%, 15% & 20%) and \\(D\\) is my data (0.077). Bayes rule gives me a route to get from the probability of some data given a parameter - \\(P(D | \\theta)\\) - to the probability of the parameter given some data - \\(P(\\theta | D)\\).\nI will set up 4 hypotheses:\n\n\\(H_1: \\theta = 0.05\\). My prior probability = 0.26\n\\(H_2: \\theta = 0.10\\). My prior probability = 0.52\n\\(H_3: \\theta = 0.15\\). My prior probability = 0.19\n\\(H_3: \\theta = 0.20\\). My prior probability = 0.03\n\nTo get the posterior probability for each of these I need to calculate \\(P(D|\\theta)\\) - the likelihood - for each hypothesis i.e. I need to calculate the likelihood of seeing 8/104 people get a mark of 70% or more given each plausible proportion - 5%, 10%, 15% or 20%.\nThe binomial distribution allows me to calculate these likelihoods. The binomial distribution gives me the probability of getting \\(s\\) successes in \\(n\\) trials given some probability, \\(p\\) of success.\nThe formula for the binomial probability distribution is:\n\\[\nP(s) = {n \\choose s}\\theta^s(1-\\theta)^{n-s}\n\\]\nThe \\({n\\choose s}\\) part is called the binomial coefficient and is calculated by \\(\\frac{n!}{s!(n-s)!}\\).\nThe \\(!\\) in \\(n!\\) represents the factorial function:\\(n \\times n-1 \\times n-2 \\times ... \\times 1\\).\nPlugging in the numbers for the first hypothesis (\\(H_1: \\theta = 0.05\\)) we get:\n\\[\nP(8) = {104\\choose 8}0.05^8(1-0.05)^{96} = 0.073\n\\]\nIn R I can use the dbinom() function to calculate the likelihoods:\n\n# binomial prob for 8 successes out of 104 trials with prob s = 0.05\ndbinom(8, 104, 0.05)\n\n[1] 0.07313592\n\n\nSo the likelihood of getting 8 successes out of 104 trials if the ‘true’ probability of success is 5% is 0.073 (or 7.3%).\nIn the code below I use the dbinom() function to quickly calculate the likelihood of the data for each hypothesis.\n\n# calc all likelihoods\nlike_vec &lt;- dbinom(8, 104, props) |&gt; \n  round(4)\n\n# plot likelihoods\nplot(props, like_vec, type='h', xlab = 'Proportion', ylab='Likelihood', lwd=4, main = 'Likelihoods')\npoints(props, like_vec, pch=16, cex=2)\n\n\n\n\nThe likelihoods\n\n\n\n\nLet’s start laying out the different components in a table.\n\n# Bayes box table\nbayes_tab &lt;- data.frame(theta = props, prior = priors, likelihood = round(like_vec,4))\nknitr::kable(bayes_tab, caption = \"Table 1. Priors and likelihoods for different proportions.\")\n\n\nTable 1. Priors and likelihoods for different proportions.\n\n\ntheta\nprior\nlikelihood\n\n\n\n\n0.05\n0.26\n0.0731\n\n\n0.10\n0.52\n0.1043\n\n\n0.15\n0.19\n0.0111\n\n\n0.20\n0.03\n0.0003\n\n\n\n\n\nNow we have to calculate the product of the likelihood and the prior. This is the numerator of Bayes theorem - \\(P(D|\\theta)P(\\theta)\\).\n\n# calc Bayes numerators\nlike_x_prior &lt;- like_vec * priors\nbayes_tab$like_x_prior &lt;- round(like_x_prior, 4)\nknitr::kable(bayes_tab, caption = \"Table 2. Priors, likelihoods and their product for different proportions.\")\n\n\nTable 2. Priors, likelihoods and their product for different proportions.\n\n\ntheta\nprior\nlikelihood\nlike_x_prior\n\n\n\n\n0.05\n0.26\n0.0731\n0.0190\n\n\n0.10\n0.52\n0.1043\n0.0542\n\n\n0.15\n0.19\n0.0111\n0.0021\n\n\n0.20\n0.03\n0.0003\n0.0000\n\n\n\n\n\nFinally we need to calculate the denominator for Bayes rule i.e. \\(P(D)\\). To do this we sum the likelihood and prior products we have calculated i.e.\n\\(P(D) = \\Sigma_1^i {P(D|\\theta)_i P(\\theta)_i}\\)\n\n# calc Bayes denominator (evidence)\ndenom &lt;- sum(bayes_tab$like_x_prior) |&gt; \n  round(3)\n\nWith all these parts in place we can calculate the posterior distribution i.e. the products divided by the evidence.\n\\[P(\\theta | D) = \\frac{P(D | \\theta)P(\\theta)}{P(D)}\\]\n\n# calc posteriors\npost &lt;- bayes_tab$like_x_prior/denom |&gt;\n  round(4)\n# add posterior probs to the table\nbayes_tab$post &lt;- round(post, 4)\nknitr::kable(bayes_tab, caption = \"Table 3. Priors, likelihoods, their product and posterior probabilities for different proportions.\")\n\n\nTable 3. Priors, likelihoods, their product and posterior probabilities for different proportions.\n\n\ntheta\nprior\nlikelihood\nlike_x_prior\npost\n\n\n\n\n0.05\n0.26\n0.0731\n0.0190\n0.2533\n\n\n0.10\n0.52\n0.1043\n0.0542\n0.7227\n\n\n0.15\n0.19\n0.0111\n0.0021\n0.0280\n\n\n0.20\n0.03\n0.0003\n0.0000\n0.0000\n\n\n\n\n\nWe can plot the posterior.\n\n# plot the posteriors\nplot(bayes_tab$theta, bayes_tab$post, type='h', xlab='Proportion', ylab='Posterior Probability', main = 'Posterior distribution', lwd=4)\npoints(bayes_tab$theta, bayes_tab$post, pch=16, cex=2)\n\n\n\n\nThe posterior distribution\n\n\n\n\nThe probability of 10% of students achieving 70% or more on the module has gone from 52% before seeing data to 72% after seeing the data for 2015 (8 out of 104 students). So that’s good - these results strengthen my belief that 10% of students will get 70% or more.\nNote that unlike the NHST approach I was able to examine 4 hypotheses at the same time. The Bayesian approach gives me a richer inferential view of the results of my study. Whilst the data supports my belief in 10% I wouldn’t be surprised to see a proportion of only 5% (25% probability according to this analysis) but I’d be more surprised to see 15% (2.8% probability according to this analysis). This is a more considered conclusion compared to the usual dichotomous (and wrong) decision taken based on a non-significant p-value of ‘no effect’.\n\n\n\nRelationship between prior, likelihood and posterior in a Bayesian analysis.\n\n\nThe figure above shows how the posterior distribution is constructed in a Bayesian analysis. The posterior is the prior weighted by the likelihood (the \\(\\propto\\) symbol means “proportional to” - here it just means we’re ignoring the denominator). The grid approach is useful for seeing how all the moving parts come together but it does not allow us to examine all possible proportions or continuous distributions. In the next post we’ll look at how we can use conjugate priors to address these problems.\n\n\n\nIn this post we moved from Bayes rule to introducing Bayesian inference. We used a grid approximation approach to examine how the data supported several hypotheses at once. This allowed us to see how the moving parts of a basic Bayesian analysis come together. As we move on to examine more complex scenarios keep in mind that all that’s happening under the hood is what was done here."
  },
  {
    "objectID": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#why-would-we-want-to-use-a-bayesian-approach-for-inference",
    "href": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#why-would-we-want-to-use-a-bayesian-approach-for-inference",
    "title": "Bayes rule & distributions; Inference with grid approximation",
    "section": "",
    "text": "A key difference between the Bayesian and NHST/frequentist approaches to inference centers on the philosophical stance around probability. For frequentists ‘probability’ of an outcome is defined as the number of times that outcome is seen in a long run of ‘experiments’. The Bayesian stance views probability as something more subjective; we define probability in terms of our expectation based on experience & knowledge about the world; probability can be interpreted as the ‘plausibility’ of an outcome.\nOne of the key differences this makes to inference (which is just educated guessing) is that Bayesians can easily incorporate prior knowledge. Incorporating prior knowledge should be part of principled scientific inference; we should build on what went before. However incorporating prior knowledge is hard for frequentists. For Bayesians the probability of one off events like life on Mars is easy; we just assign this some prior probability then collect data and ‘do’ Bayes! For frequentists the probability of life on Mars is difficult to define. What does it mean to have a frequentist infinity of Mars (what would the plural be? Marses? Marsii? 😕) to examine for the presence of life?\nThe dominant statistical paradigm in use is frequentist null hypothesis significance testing (NHST). For analysis of scientific studies in e.g. the biomedical field where I mostly work, this system relies on an assumption of a long run of study repeats (the frequentist part). But study repeats are rarely done and when they are the results are usually disappointing for statistical and other reasons (Amaral and Neves (2021), Errington et al. (2021), OPEN SCIENCE COLLABORATION (2015)). NHST only examines the probability of the data you have (or more extreme data) under a null hypothesis. The frequentist procedures use a p-value to inform on this null hypothesis. Notably the p-value is a conditional probability; it is the probability of the data you have or data more extreme given the null hypothesis is true. In maths:\n\\[\np\\mbox{-}value = P(data|H_0)\n\\]\nIf we assume the null hypothesis is true then a low p-value counts as ‘some evidence’ against the null… but how much evidence? The p-value tells us the probability of data not the probability of the null hypothesis; in fact we assume the null is true so the probability of the null (or ‘chance alone’ irrespective of the p-value) is 100% under this assumption!\nThis probability - \\(P(data|H_0)\\) - is not usually the probability we want. We usually want the ‘inverse conditional’:\n\\[\nP(H_0|data)\n\\]\nThat is, the probability of the null hypothesis given the data. NHST does not give us this.\nThere are no shortage of other criticisms of the NHST system. For example NHST tells you nothing about any alternative hypotheses. We’ll see below onw way a Bayesian approach can deal with this.\nFinally NHST (as usually used) is a combination of the Fisherian approach (significance testing) and the Neyman-Pearson (hypothesis testing) approaches to inference. These systems are not compatible and whilst each of these alone is coherent the hybrid that is NHST is not coherent.\n\nConfusion surrounding the reporting and interpretation of results of classical statistical tests is widespread among applied researchers, most of whom erroneously [my emphasis] believe that such tests are prescribed by a single coherent theory of statistical inference… In particular, there is a widespread failure to appreciate the incompatibility [my emphasis] of Fisher’s evidential p-value with the Type I error rate, \\(\\alpha\\), of Neyman-Pearson statistical orthodoxy.\n\n(Hubbard and Bayarri 2003)\nAll that is not to say frequentist statistics is not useful - it’s just sensible to know what you’re getting for your money.\nBayes rule is a straightforward combination of basic probability rules and as such inference based on Bayes rule is the result of a coherent system. If you want to read more about the incoherence in NHST then two accessible accounts are given by Cohen (1994) & Hubbard and Bayarri (2003).\nAnyway philosophical rant aside let’s look at how Bayes rule can help us.\n\n\nWe can re-state Bayes rule in terms of data and models - models are just ‘events’ in some probability space.\n\\[\nP(Model|Data) = \\frac{P(Data|Model)P(Model)}{P(Data)}\n\\]\nWe have some model under consideration (e.g. a ‘null’ hypothesis) and we can define the prior probability of that model. This is the \\(P(Model)\\) part. Conceptually this is the plausibility of the model expressed as a probability before we see any data.\nWe combine the prior with the likelihood - \\(P(Data|Model)\\) - of the \\(Data\\) under each possible parameter in the the \\(Model\\). The product in the numerator is then normalized to a probability by taking the probability of the \\(Data\\) over all possible model parameters in the prior. This sum of products - \\(P(Data)\\) - is often called the evidence. The calculation on the right hand side then gives us the probability of our \\(Model\\) given the \\(Data\\) - \\(P(Model|Data)\\) - the posterior."
  },
  {
    "objectID": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#estimating-a-proportion",
    "href": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#estimating-a-proportion",
    "title": "Bayes rule & distributions; Inference with grid approximation",
    "section": "",
    "text": "All of the above is pretty abstract and will be easier with an example. Suppose I believe that 10% of students in the previous post are capable of scoring 70% or more in an exam (in the UK system this is a first class mark). My belief in 10% is not absolute though… I might consider 5%, 10%, 15% & 20% to be plausible. A Bayesian approach allows me to examine each of these hypotheses (\\(models\\)) given some \\(data\\).\nBefore we go down the Bayesian route though let’s take a look at the usual frequentist NHST approach to this question. If you want to follow along the data for this analysis are here.\n\n\nThe marks for 2015 show that 8 out of 104 students got a mark of 70% or more, a proportion of 7.7%. How can I compare this data to my belief that 10% of students are capable of a mark of 70% or more?\nThe outcome here is binary i.e. each student can score 70% or more (a ‘success’) or less than 70% (a ‘failure’; no judgement - ‘success’& ‘failure’ are just conventional labels for binary outcomes). We have some number of trials (each student represents a trial) and some number of successes. In a situation like this we can use the binomial distribution to calculate a p-value from the data we have. In R the binom.test() function does the heavy lifting. We enter the number of ‘successes’ (8), the total number of trials (104) and the probability of success under the null hypothesis. Since I believe 10% of students are capable of a first class mark the probability of success under the null hypothesis here is 0.1.\n\n# binom.test(successes, total trials, prob success)\nbinom.test(8, 104, 0.1)\n\n\n    Exact binomial test\n\ndata:  8 and 104\nnumber of successes = 8, number of trials = 104, p-value = 0.5154\nalternative hypothesis: true probability of success is not equal to 0.1\n95 percent confidence interval:\n 0.03379462 0.14595163\nsample estimates:\nprobability of success \n            0.07692308 \n\n\nThe interpretation here is that if the true probability of success is 10% then the probability of 8 succsses out of 104 trials (7.7%) or a more extreme proportion is about 51% (the p-value). Conventionally we would fail to reject the null (note this is not the same as accept the null) at conventional 5% significance. I might tentatively conclude that the data somewhat support my guess that 10% of students are capable of a first class mark.\nThis tells me something about an assumed proportion of 10% but what if I want to test other possible proportions like 5%, 15% or 20%? I could do more binomial tests but then I run into multiple comparison problems (Tukey 1991) because I want to control the type 1 error rate at 5%. Note also I have no power calculation here so I can’t actually set a number of observations to control the type 1 error rate. So what does my p-value mean in a strict NHST sense? Note that post-hoc power is not (ever) the right thing to do e.g. (Hoenig and Heisey 2001), (Gelman 2019)."
  },
  {
    "objectID": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#bayesian-inference-with-grid-approximation",
    "href": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#bayesian-inference-with-grid-approximation",
    "title": "Bayes rule & distributions; Inference with grid approximation",
    "section": "",
    "text": "Let’s move on to a Bayesian analysis. I’ll use a simple grid with values representing each of four plausible proportions (5%, 10%, 15% & 20%) and use Bayes rule to update my belief in each proposed proportion in light of the data I have. In order to use Bayes rule I first have to set some prior belief over each of the proposed proportions. I can create probabilities for each proportion by first weighting each proportion arbitrarily for plausibility.\n\n# set 'plausibility' on proportions of 5, 10, 15 & 20%\nprops &lt;- seq(0.05, 0.2, 0.05) # my proportions; start, stop, step\nwgts &lt;- c(4, 8, 3, 0.5) # arbitrary plausibility weight for each proportion; most on 0.1\ncbind(props, wgts)\n\n     props wgts\n[1,]  0.05  4.0\n[2,]  0.10  8.0\n[3,]  0.15  3.0\n[4,]  0.20  0.5\n\n\nNow I can divide each individual weight through by the total weight thus scaling the weights to lie in the interval [0,1] and also making sure they sum to 1. In effect I have created a probability distribution.\n\npriors &lt;- round(wgts/sum(wgts),2) # convert weights to probabilities\ncbind(props, wgts, priors)\n\n     props wgts priors\n[1,]  0.05  4.0   0.26\n[2,]  0.10  8.0   0.52\n[3,]  0.15  3.0   0.19\n[4,]  0.20  0.5   0.03\n\n\nThe priors column in the above represents the plausibility of each proportion. Note that most of my prior probability is on 10%. I don’t place much probability on 20%; first class marks should be hard to get otherwise they’re rather meaningless!\n\n\n\nThe Wisdom of Buddy Pine\n\n\nThese discrete probabilities make a probability distribution.\n\nplot(props, priors, type='h', xlab = 'Proportion', ylab='Probability', main = 'Prior distribution', lwd=4)\npoints(props, priors, pch=16, cex=2)\n\n\n\n\nThe prior probability distribution\n\n\n\n\nAs before the marks for 2015 show that 8 out of 104 students achieved a mark of 70% or more, a proportion of 7.7%. How should this information change my belief about each of the proportions defined above?\nI can use Bayes rule to calculate this.\n\\[\nP(\\theta | D) = \\frac{P(D | \\theta)P(\\theta)}{P(D)}\n\\]\nHere \\(\\theta\\) (the Greek letter theta) is my proportion (5%, 10%, 15% & 20%) and \\(D\\) is my data (0.077). Bayes rule gives me a route to get from the probability of some data given a parameter - \\(P(D | \\theta)\\) - to the probability of the parameter given some data - \\(P(\\theta | D)\\).\nI will set up 4 hypotheses:\n\n\\(H_1: \\theta = 0.05\\). My prior probability = 0.26\n\\(H_2: \\theta = 0.10\\). My prior probability = 0.52\n\\(H_3: \\theta = 0.15\\). My prior probability = 0.19\n\\(H_3: \\theta = 0.20\\). My prior probability = 0.03\n\nTo get the posterior probability for each of these I need to calculate \\(P(D|\\theta)\\) - the likelihood - for each hypothesis i.e. I need to calculate the likelihood of seeing 8/104 people get a mark of 70% or more given each plausible proportion - 5%, 10%, 15% or 20%.\nThe binomial distribution allows me to calculate these likelihoods. The binomial distribution gives me the probability of getting \\(s\\) successes in \\(n\\) trials given some probability, \\(p\\) of success.\nThe formula for the binomial probability distribution is:\n\\[\nP(s) = {n \\choose s}\\theta^s(1-\\theta)^{n-s}\n\\]\nThe \\({n\\choose s}\\) part is called the binomial coefficient and is calculated by \\(\\frac{n!}{s!(n-s)!}\\).\nThe \\(!\\) in \\(n!\\) represents the factorial function:\\(n \\times n-1 \\times n-2 \\times ... \\times 1\\).\nPlugging in the numbers for the first hypothesis (\\(H_1: \\theta = 0.05\\)) we get:\n\\[\nP(8) = {104\\choose 8}0.05^8(1-0.05)^{96} = 0.073\n\\]\nIn R I can use the dbinom() function to calculate the likelihoods:\n\n# binomial prob for 8 successes out of 104 trials with prob s = 0.05\ndbinom(8, 104, 0.05)\n\n[1] 0.07313592\n\n\nSo the likelihood of getting 8 successes out of 104 trials if the ‘true’ probability of success is 5% is 0.073 (or 7.3%).\nIn the code below I use the dbinom() function to quickly calculate the likelihood of the data for each hypothesis.\n\n# calc all likelihoods\nlike_vec &lt;- dbinom(8, 104, props) |&gt; \n  round(4)\n\n# plot likelihoods\nplot(props, like_vec, type='h', xlab = 'Proportion', ylab='Likelihood', lwd=4, main = 'Likelihoods')\npoints(props, like_vec, pch=16, cex=2)\n\n\n\n\nThe likelihoods\n\n\n\n\nLet’s start laying out the different components in a table.\n\n# Bayes box table\nbayes_tab &lt;- data.frame(theta = props, prior = priors, likelihood = round(like_vec,4))\nknitr::kable(bayes_tab, caption = \"Table 1. Priors and likelihoods for different proportions.\")\n\n\nTable 1. Priors and likelihoods for different proportions.\n\n\ntheta\nprior\nlikelihood\n\n\n\n\n0.05\n0.26\n0.0731\n\n\n0.10\n0.52\n0.1043\n\n\n0.15\n0.19\n0.0111\n\n\n0.20\n0.03\n0.0003\n\n\n\n\n\nNow we have to calculate the product of the likelihood and the prior. This is the numerator of Bayes theorem - \\(P(D|\\theta)P(\\theta)\\).\n\n# calc Bayes numerators\nlike_x_prior &lt;- like_vec * priors\nbayes_tab$like_x_prior &lt;- round(like_x_prior, 4)\nknitr::kable(bayes_tab, caption = \"Table 2. Priors, likelihoods and their product for different proportions.\")\n\n\nTable 2. Priors, likelihoods and their product for different proportions.\n\n\ntheta\nprior\nlikelihood\nlike_x_prior\n\n\n\n\n0.05\n0.26\n0.0731\n0.0190\n\n\n0.10\n0.52\n0.1043\n0.0542\n\n\n0.15\n0.19\n0.0111\n0.0021\n\n\n0.20\n0.03\n0.0003\n0.0000\n\n\n\n\n\nFinally we need to calculate the denominator for Bayes rule i.e. \\(P(D)\\). To do this we sum the likelihood and prior products we have calculated i.e.\n\\(P(D) = \\Sigma_1^i {P(D|\\theta)_i P(\\theta)_i}\\)\n\n# calc Bayes denominator (evidence)\ndenom &lt;- sum(bayes_tab$like_x_prior) |&gt; \n  round(3)\n\nWith all these parts in place we can calculate the posterior distribution i.e. the products divided by the evidence.\n\\[P(\\theta | D) = \\frac{P(D | \\theta)P(\\theta)}{P(D)}\\]\n\n# calc posteriors\npost &lt;- bayes_tab$like_x_prior/denom |&gt;\n  round(4)\n# add posterior probs to the table\nbayes_tab$post &lt;- round(post, 4)\nknitr::kable(bayes_tab, caption = \"Table 3. Priors, likelihoods, their product and posterior probabilities for different proportions.\")\n\n\nTable 3. Priors, likelihoods, their product and posterior probabilities for different proportions.\n\n\ntheta\nprior\nlikelihood\nlike_x_prior\npost\n\n\n\n\n0.05\n0.26\n0.0731\n0.0190\n0.2533\n\n\n0.10\n0.52\n0.1043\n0.0542\n0.7227\n\n\n0.15\n0.19\n0.0111\n0.0021\n0.0280\n\n\n0.20\n0.03\n0.0003\n0.0000\n0.0000\n\n\n\n\n\nWe can plot the posterior.\n\n# plot the posteriors\nplot(bayes_tab$theta, bayes_tab$post, type='h', xlab='Proportion', ylab='Posterior Probability', main = 'Posterior distribution', lwd=4)\npoints(bayes_tab$theta, bayes_tab$post, pch=16, cex=2)\n\n\n\n\nThe posterior distribution\n\n\n\n\nThe probability of 10% of students achieving 70% or more on the module has gone from 52% before seeing data to 72% after seeing the data for 2015 (8 out of 104 students). So that’s good - these results strengthen my belief that 10% of students will get 70% or more.\nNote that unlike the NHST approach I was able to examine 4 hypotheses at the same time. The Bayesian approach gives me a richer inferential view of the results of my study. Whilst the data supports my belief in 10% I wouldn’t be surprised to see a proportion of only 5% (25% probability according to this analysis) but I’d be more surprised to see 15% (2.8% probability according to this analysis). This is a more considered conclusion compared to the usual dichotomous (and wrong) decision taken based on a non-significant p-value of ‘no effect’.\n\n\n\nRelationship between prior, likelihood and posterior in a Bayesian analysis.\n\n\nThe figure above shows how the posterior distribution is constructed in a Bayesian analysis. The posterior is the prior weighted by the likelihood (the \\(\\propto\\) symbol means “proportional to” - here it just means we’re ignoring the denominator). The grid approach is useful for seeing how all the moving parts come together but it does not allow us to examine all possible proportions or continuous distributions. In the next post we’ll look at how we can use conjugate priors to address these problems."
  },
  {
    "objectID": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#summary",
    "href": "posts/bayes_two_grid_approx/Bayes_part_2_grid_estimation.html#summary",
    "title": "Bayes rule & distributions; Inference with grid approximation",
    "section": "",
    "text": "In this post we moved from Bayes rule to introducing Bayesian inference. We used a grid approximation approach to examine how the data supported several hypotheses at once. This allowed us to see how the moving parts of a basic Bayesian analysis come together. As we move on to examine more complex scenarios keep in mind that all that’s happening under the hood is what was done here."
  },
  {
    "objectID": "posts/bayes_one_bayes_rule/bayes_rule.html",
    "href": "posts/bayes_one_bayes_rule/bayes_rule.html",
    "title": "Probability and Bayes Rule",
    "section": "",
    "text": "This is the first of a series of posts going over the basics of Bayesian inference. Bayesian inference uses Bayes rule which comes from simple algebra of basic probability rules. In this post we’ll look at the basic rules of probability and derive Bayes rule. We’ll look at a couple of simple examples to see how Bayes rule works.\n\n\n\n\n\n\nNote\n\n\n\nThere’s a little bit of R code in this post and I’m using the new native R pipe |&gt;. If you’re used to tidyverse semantics you might think that |&gt; behaves like the magrittr pipe, %&gt;% but it doesn’t! But it can (mostly) be used in the same way. See here for more details."
  },
  {
    "objectID": "posts/bayes_one_bayes_rule/bayes_rule.html#marginal-probabilities",
    "href": "posts/bayes_one_bayes_rule/bayes_rule.html#marginal-probabilities",
    "title": "Probability and Bayes Rule",
    "section": "Marginal probabilities",
    "text": "Marginal probabilities\nThe row and column totals for each variable in the table are called the marginal totals.\n\nTable 2. Grade classifications with marginal totals\n\n\n\nProgramme\n\n\n\n\n\n\nGrade\nSES\nSS\nMarginal Total\n\n\n1\n10\n3\n13\n\n\n2.1\n20\n6\n26\n\n\n2.2\n11\n15\n26\n\n\n3\n4\n20\n24\n\n\nMarginal Total\n45\n44\n89\n\n\n\nWe can get R to add marginal totals to contingency tables for us.\n\ngrade_table_marginals &lt;- grade_table |&gt; \n  addmargins()\ngrade_table_marginals\n\n    SES SS Sum\n1    10  3  13\n2.1  20  6  26\n2.2  11 15  26\n3     4 20  24\nSum  45 44  89\n\n\nThe marginal totals can be used to calculate marginal probabilities by dividing the marginal total for a row or column by the grand total. We denote marginal probabilities as \\(P(A)\\) where \\(A\\) is some outcome.\nFor example the marginal probability of students getting a 2.1 is the marginal number of students who got a 2.1 (26) divided by the total number of students in the course (89).\n\\[\nP(2.1) = \\textcolor{#00BFFF}{26}/\\textcolor{#00008B}{89} = 0.29\n\\]\n\nTable 3. Numbers involved in marginal probabilities are coloured.\n\n\n\nProgramme\n\n\n\n\n\n\nGrade\nSES\nSS\nMarginal Total\n\n\n1\n10\n3\n13\n\n\n2.1\n20\n6\n26\n\n\n2.2\n11\n15\n26\n\n\n3\n4\n20\n24\n\n\nMarginal Total\n45\n44\n89\n\n\n\nWe see that \\(P(2.1)\\) is 29%."
  },
  {
    "objectID": "posts/bayes_one_bayes_rule/bayes_rule.html#joint-probabilties",
    "href": "posts/bayes_one_bayes_rule/bayes_rule.html#joint-probabilties",
    "title": "Probability and Bayes Rule",
    "section": "Joint probabilties",
    "text": "Joint probabilties\nThe probability of two (or more) outcomes considered together is called a joint probability.\nFor events \\(A\\) and \\(B\\) joint probabilities are often written as \\(P(A \\text{ } and \\text{ } B)\\) or \\(P(A, \\text{ } B)\\).\nTo calculate joint probability we divide the number of outcomes that fulfill a specific criteria by the grand total.\nHere an example might be:\n\\[\nP(2.1 \\text{ and } SES)\n\\]\nWe can calculate this probability from the total number of SES students who got a 2.1 classification (20) divided by the total number of students (89).\n\\[\nP(2.1 \\text{ and } SES) = \\textcolor{#00BFFF}{20}/\\textcolor{#00008B}{89} = 0.225\n\\]\n\nTable 4. Numbers involved in joint probabilities are coloured.\n\n\n\n\n\n\n\n\n\nProgramme\n\n\n\n\n\n\nGrade\nSES\nSS\nMarginal Total\n\n\n1\n10\n3\n13\n\n\n2.1\n20\n6\n26\n\n\n2.2\n11\n15\n26\n\n\n3\n4\n20\n24\n\n\nMarginal Total\n45\n44\n89\n\n\n\nWe see that the joint probability of 2.1 and being on the SES programme is 22.5%.\nFor joint probabilities the order of the outcomes doesn’t matter.\n\\(P(A \\text{ and } B)\\) = \\(P(B \\text{ and } A)\\)\nIn R the prop.table() function can convert a contingency table of raw numbers to a table of joint probabilities.\n\ngrade_probs &lt;- grade_table |&gt; \n  prop.table() |&gt;\n  addmargins() |&gt; # add marginal totals\n  round(3) # round to 3dp\n\ngrade_probs\n\n      SES    SS   Sum\n1   0.112 0.034 0.146\n2.1 0.225 0.067 0.292\n2.2 0.124 0.169 0.292\n3   0.045 0.225 0.270\nSum 0.506 0.494 1.000\n\n\n\n\n\n\n\n\nNote\n\n\n\nKey to both marginal and joint probability is that we use the grand total as the denominator to calculate these probabilities."
  },
  {
    "objectID": "posts/bayes_one_bayes_rule/bayes_rule.html#conditional-probabilities",
    "href": "posts/bayes_one_bayes_rule/bayes_rule.html#conditional-probabilities",
    "title": "Probability and Bayes Rule",
    "section": "Conditional probabilities",
    "text": "Conditional probabilities\nConditional probability is the probability of an event or outcome occurring given some other event or outcome has already occurred or is in place.\nWe denote conditional probabilities as \\(P(outcome | condition)\\).\nWe can read this as “The probability that outcome occurs given condition is in place”.\nWe might ask “What’s the probability of a 2.1 mark given the student is in the SES programme?” We’d denote that as:\n\\[\nP(2.1|SES)\n\\]\n\nTable 5. When we calculate conditional probabilities we restrict ourselves to one row or column (here coloured) of a contingency table.\n\n\n\n\n\n\n\n\n\nProgramme\n\n\n\n\n\n\nGrade\nSES\nSS\nMarginal Total\n\n\n1\n10\n3\n13\n\n\n2.1\n20\n6\n26\n\n\n2.2\n11\n15\n26\n\n\n3\n4\n20\n24\n\n\nMarginal Total\n45\n44\n89\n\n\n\nIf we look in the SES column we see 20 students got a 2.1 classification. There were a total of 45 SES students so \\(P(2.1|SES)\\) is 20/45 or 0.444 or 44.4%.\nThe general formula for conditional probability is:\n\\[\nP(A|B) = \\frac{P(A\\text{ and }B)}{P(B)}\n\\]\nLet’s see this at work for \\(P(2.1 | SES)\\).\nThe joint probability - \\(P(2.1 \\text{ and } SES)\\) - is 20/89 = 0.225.\nThe marginal probability of SES - \\(P(SES)\\) - is 45/89 = 0.506.\nThe conditional probability of 2.1 given SES (\\(P(2.1 | SES)\\)) is therefore 0.225/0.506 = 0.444; exactly the same as before.\nImportantly \\(P(A|B) \\neq P(B|A)\\).\nExamining our contingency table we saw that \\(P(2.1|SES)\\) was 0.444.\nHowever this is not the same as \\(P(SES|2.1) = \\frac{P(2.1 \\text{ and } SES)}{P(2.1)}\\) = [(20/89) / (26/89)] = 0.77.\n\n\n\n\n\n\nNote\n\n\n\nKey to conditional probability calculations using a contingency table is that we are restricting ourselves to one row or column of the table."
  },
  {
    "objectID": "posts/bayes_one_bayes_rule/bayes_rule.html#the-general-multiplication-rule",
    "href": "posts/bayes_one_bayes_rule/bayes_rule.html#the-general-multiplication-rule",
    "title": "Probability and Bayes Rule",
    "section": "The General Multiplication rule",
    "text": "The General Multiplication rule\nWe can rearrange the equation for conditional probability to get the General Multiplication Rule for calculating the joint probability of A and B.\n\\[\nP(A \\text{ and } B) = P(A|B)P(B)\n\\]\nThis is useful if we are not given the joint probability for some outcome.\nMaybe we only know that 50.6% of students on the module are on the SES programme and that if a student was on the SES programme the probability of a 2.1. was 44.4%.\nWe can use this information to work back to the joint probability, \\(P(2.1 \\text{ and } SES\\)) using the formula above.\n\\(P(2.1 \\text{ and } SES) = P(2.1|SES)P(SES))\\) = 0.444 * 0.506 = 0.225 - i.e. 22.5% as before."
  },
  {
    "objectID": "posts/bayes_one_bayes_rule/bayes_rule.html#components-of-bayes-rule",
    "href": "posts/bayes_one_bayes_rule/bayes_rule.html#components-of-bayes-rule",
    "title": "Probability and Bayes Rule",
    "section": "Components of Bayes rule",
    "text": "Components of Bayes rule\nEach part of Bayes rule has a name.\nIn the numerator \\(P(A|B)\\) is the likelihood and \\(P(B)\\) is the prior. The denominator, \\(P(A)\\) is the marginal probability of A and is also called the evidence. The left hand probability, \\(P(B|A)\\) is called the posterior.\n\n\n\nFigure 2. The components of Bayes rule.\n\n\nBeing able to go from \\(P(A|B)\\) to \\(P(B|A\\)) might seem trivial but it turns out to be really useful."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html",
    "href": "posts/hello_data_r/hello_world_data_R.html",
    "title": "Hello Data World 1 (R)",
    "section": "",
    "text": "In programming it’s traditional that the first thing you learn to do in a new language is to print ‘Hello, World!’ to the screen. This is the first of three ‘Hello World’ posts that will walk through some data handling & analysis tasks. These will be a bit more complex than printing ‘Hello, World!’ but will provide a look at how to approach data loading, exploration, filtering, plotting and statistical testing. Each post will use a different language & in this first post we will use R - because it’s the language I know best (i.e. least worst). The next two posts will carry out the same tasks using python and julia. R and python are popular in data science and julia is a promising newcomer.\nIn each post we will load a dataset from a csv file, carry out some summarisation and exploratory plotting, some data filtering and finally carry out statistical testing on two groups using frequentist and Bayesian techniques. These are not exactly beginners posts but the aim is to give a flavour of how basic data exploration & analysis can be done in each language.\nIf you want to follow along the data are here."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html#introduction",
    "href": "posts/hello_data_r/hello_world_data_R.html#introduction",
    "title": "Hello Data World 1 (R)",
    "section": "",
    "text": "In programming it’s traditional that the first thing you learn to do in a new language is to print ‘Hello, World!’ to the screen. This is the first of three ‘Hello World’ posts that will walk through some data handling & analysis tasks. These will be a bit more complex than printing ‘Hello, World!’ but will provide a look at how to approach data loading, exploration, filtering, plotting and statistical testing. Each post will use a different language & in this first post we will use R - because it’s the language I know best (i.e. least worst). The next two posts will carry out the same tasks using python and julia. R and python are popular in data science and julia is a promising newcomer.\nIn each post we will load a dataset from a csv file, carry out some summarisation and exploratory plotting, some data filtering and finally carry out statistical testing on two groups using frequentist and Bayesian techniques. These are not exactly beginners posts but the aim is to give a flavour of how basic data exploration & analysis can be done in each language.\nIf you want to follow along the data are here."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html#preliminaries",
    "href": "posts/hello_data_r/hello_world_data_R.html#preliminaries",
    "title": "Hello Data World 1 (R)",
    "section": "Preliminaries",
    "text": "Preliminaries\nR has a lot of base functionality for data handling, exploration & statistical analysis; it’s what R was designed for. However we are going to make use of the ‘tidyverse’ (Wickham et al. 2019) because it has become a very popular approach to data handling & analysis in R.\n\nThe tidyverse encompasses the repeated tasks at the heart of every data science project: data import, tidying, manipulation, visualisation, and programming.\n\nAs well as data handling & visualisation we will also be carrying out some statistical testing. R is well served for basic frequentist statistics and there’s nothing extra we need. For Bayesian analysis we will use the Stan probabilistic programming language (Carpenter et al. 2017). We will code a model by hand and use the cmdstanr package to pass that model to Stan. We will also use the brms package (Bürkner 2017) which makes writing Stan models easier. Details on how to install the cmdstanr package and Stan are here (see the section on Installing CmdStan for how to install Stan). Note that brms also needs Stan to be installed. We load the packages we need in the code below.\n\n# data loading & plotting\nlibrary(tidyverse) # meta-package; loads several packages\n# set theme for ggplot2 plotting\ntheme_set(theme_bw())\n# bayesian modeling\nlibrary(cmdstanr)\n# easier bayesian modeling\nlibrary(brms)\n# plot bayesian models\nlibrary(bayesplot)\n\n\nLoading the data\nThese data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a number of years by the students who carried out various measures on themselves.\n\n# load the data\ndata_in &lt;- read_csv('data/BODY_COMPOSITION_DATA.csv')\n\nRows: 203 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): sex\ndbl (9): girths, bia, DW, jackson, HW, skinfolds, BMI, WHR, Waist\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nExploration & tidying\nFirst we make sure the data looks as we expect it to.\n\n# examine the data\nglimpse(data_in)\n\nRows: 203\nColumns: 10\n$ sex       &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ girths    &lt;dbl&gt; 10.85, 14.12, 12.30, 8.50, 11.66, 15.65, 13.22, 14.62, 17.21…\n$ bia       &lt;dbl&gt; 5.7, 6.2, 6.3, 6.4, 6.6, 6.8, 6.9, 7.4, 7.6, 7.7, 7.8, 7.9, …\n$ DW        &lt;dbl&gt; 9.220, 11.800, 12.000, 10.850, 15.600, 21.420, 14.400, 9.820…\n$ jackson   &lt;dbl&gt; 4.75, 5.50, 5.50, 5.00, 12.00, 3.00, 7.80, 4.50, 9.00, 6.80,…\n$ HW        &lt;dbl&gt; 17.00, 16.90, 14.80, 10.20, 11.86, 33.10, 13.40, 14.35, 21.4…\n$ skinfolds &lt;dbl&gt; 50.75, 46.30, 45.80, 43.55, 93.50, 49.75, 56.70, 39.70, 73.5…\n$ BMI       &lt;dbl&gt; 20.70, 21.90, 21.39, 19.26, 22.30, 20.23, 23.54, 21.18, 20.5…\n$ WHR       &lt;dbl&gt; 0.8000, 0.8100, 0.7300, 0.7400, 0.7800, 0.8500, 0.8700, 0.77…\n$ Waist     &lt;dbl&gt; 76.5, 75.0, 70.0, 68.5, 74.0, 73.0, 80.0, 76.0, 75.0, 76.7, …\n\nsummary(data_in) # tells us about NA values\n\n     sex                girths           bia              DW       \n Length:203         Min.   : 7.15   Min.   : 5.70   Min.   : 4.10  \n Class :character   1st Qu.:15.04   1st Qu.:11.90   1st Qu.:16.34  \n Mode  :character   Median :20.12   Median :16.20   Median :21.40  \n                    Mean   :20.70   Mean   :16.98   Mean   :21.66  \n                    3rd Qu.:24.60   3rd Qu.:21.18   3rd Qu.:28.00  \n                    Max.   :87.90   Max.   :39.30   Max.   :45.90  \n                                    NA's   :1                      \n    jackson            HW          skinfolds           BMI       \n Min.   : 3.00   Min.   : 4.10   Min.   : 27.75   Min.   : 2.90  \n 1st Qu.: 8.00   1st Qu.:15.04   1st Qu.: 59.27   1st Qu.:21.18  \n Median :12.80   Median :21.00   Median : 76.23   Median :23.00  \n Mean   :14.23   Mean   :21.42   Mean   : 82.88   Mean   :23.25  \n 3rd Qu.:19.00   3rd Qu.:27.00   3rd Qu.:100.67   3rd Qu.:24.80  \n Max.   :35.00   Max.   :43.00   Max.   :181.00   Max.   :33.03  \n                 NA's   :1                                       \n      WHR             Waist       \n Min.   :0.6700   Min.   : 61.00  \n 1st Qu.:0.7400   1st Qu.: 72.25  \n Median :0.7800   Median : 76.00  \n Mean   :0.7821   Mean   : 76.84  \n 3rd Qu.:0.8170   3rd Qu.: 81.00  \n Max.   :0.9900   Max.   :100.80  \n                                  \n\n\nWe should deal with the missing values before we do any further analysis. There are many ways to deal with missing values but here we will just drop rows with missing values from the data using the complete.cases() function.\n\n# drop rows with NA values\ndata_in &lt;- data_in[complete.cases(data_in), ]\nsummary(data_in)\n\n     sex                girths           bia              DW       \n Length:201         Min.   : 7.15   Min.   : 5.70   Min.   : 4.10  \n Class :character   1st Qu.:15.08   1st Qu.:11.90   1st Qu.:16.30  \n Mode  :character   Median :20.12   Median :15.90   Median :21.40  \n                    Mean   :20.73   Mean   :16.98   Mean   :21.61  \n                    3rd Qu.:24.40   3rd Qu.:21.20   3rd Qu.:28.00  \n                    Max.   :87.90   Max.   :39.30   Max.   :45.90  \n    jackson            HW          skinfolds           BMI       \n Min.   : 3.00   Min.   : 4.10   Min.   : 27.75   Min.   : 2.90  \n 1st Qu.: 8.00   1st Qu.:15.00   1st Qu.: 59.25   1st Qu.:21.17  \n Median :12.60   Median :21.00   Median : 76.23   Median :23.00  \n Mean   :14.21   Mean   :21.43   Mean   : 82.68   Mean   :23.22  \n 3rd Qu.:19.00   3rd Qu.:27.00   3rd Qu.:100.35   3rd Qu.:24.80  \n Max.   :35.00   Max.   :43.00   Max.   :181.00   Max.   :33.03  \n      WHR             Waist       \n Min.   :0.6700   Min.   : 61.00  \n 1st Qu.:0.7400   1st Qu.: 72.00  \n Median :0.7800   Median : 76.00  \n Mean   :0.7815   Mean   : 76.76  \n 3rd Qu.:0.8150   3rd Qu.: 81.00  \n Max.   :0.9900   Max.   :100.80  \n\n\nAccording to the ‘tidy data’ philosophy (Wickham 2014) we want our data in long format rather than wide format. This also makes it easier to carry out later data wrangling, plotting and testing.\n\n# wide to long data\ndata_inL &lt;- pivot_longer(data_in, cols = `girths`:`Waist`, names_to = 'measure', values_to = 'value')\nhead(data_inL)\n\n# A tibble: 6 × 3\n  sex   measure   value\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n1 M     girths    10.8 \n2 M     bia        5.7 \n3 M     DW         9.22\n4 M     jackson    4.75\n5 M     HW        17   \n6 M     skinfolds 50.8 \n\n\nNow the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations. Exploration with plots is an essential step for checking values and the distribution of data. The tidyverse provides the ggplot2 package for this.\n\n# custom colors for male & female\nplot_cols &lt;- c('firebrick', 'cornflowerblue')\n# make the plot\nggplot(data_inL, aes(sex, value, colour = sex)) + geom_jitter(width = 0.1) + \n  scale_colour_manual(values = plot_cols) + \n  theme(legend.position = \"none\") +\n  facet_wrap(~measure, scales = \"free_y\")\n\n\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. Removing outliers is a contentious subject but here a BMI of 2 is incompatible with life! So we’ll remove this unreasonably low value.\n\n# get just bmi data\nbmi_data &lt;- data_inL %&gt;% filter(measure == \"BMI\")\n# remove low value\nbmi_data &lt;- bmi_data %&gt;% filter(value &gt; 15)\n# check with a new plot\nbmi_data %&gt;% ggplot(aes(sex, value, colour = sex)) + geom_jitter(width = 0.1, size = 3) +\n  scale_colour_manual(values = plot_cols) + \n  theme(legend.position = \"none\") \n\n\n\n\nMuch better!\n\n\nFrequentist testing\nNow let’s use a t-test to examine whether male and female BMI is different. In R basic statistical tests are easy; there are no extraneous packages to load and there’s a pretty simple ‘formula’ interface using the tilde (~). Note that by default R uses Welch’s t-test which does not assume equal variances in each group (see ?t.test).\n\n# t-test\nt.test(value ~ sex, data = bmi_data)\n\n\n    Welch Two Sample t-test\n\ndata:  value by sex\nt = -2.1134, df = 192.05, p-value = 0.03586\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -1.59338708 -0.05499779\nsample estimates:\nmean in group F mean in group M \n       22.83834        23.66253 \n\n\nThe difference between male & female BMI is significant. This means that in a hypothetical long series of repeats of this study with different samples from the same population we would expect to see a difference as big or bigger between the sexes in more than 95% of those study repeats.\n\n\nBayesian testing\nThere are several packages for Bayesian statistics in R. We’ll use the cmdstanr package to write a Bayesian model in the Stan probabilistic programming language for assessing the difference between male and female BMI. Stan will do the heavy lifting for us (Markov Chain Monte Carlo (MCMC sampling)) and return a data object we can use in R.\n\n# create data list\nsex &lt;- bmi_data %&gt;% select(sex) %&gt;% pull() # labels for participant sex\n# convert to dummy coding; females are coded as 0\nsex_dummy &lt;- ifelse(sex == 'F', 0, 1)\n# bmi values\nbmi &lt;- bmi_data %&gt;% select(value) %&gt;% pull() \n# get num subjects\nN &lt;- nrow(bmi_data) # length of dataset\n# make a list of data to pass to Stan\ndata_list &lt;- list(N = N, sex = sex_dummy, bmi = bmi)\n\n# define the model in Stan as a text string; can also pass in a separate .stan file\n# stan code is written in blocks (data, parameters, model etc) defined by {}\nmodel_string &lt;- \"\n\n// data we want to model\ndata{\n  int&lt;lower=1&gt; N; // length of the data\n  vector[N] bmi; // bmi data of length N\n  vector[N] sex; // sex data of length N\n}\n\n// parameters we want to estimate\nparameters{\n  real beta0; // intercept\n  real beta1; // slope\n  real&lt;lower=0&gt; sigma; // residual sd, must be positive\n}\n\n// priors for model\nmodel{\n  // priors\n  beta0 ~ normal(25, 10); // intercept\n  beta1 ~ normal(0, 5); // slope\n  sigma ~ normal(0,100); // defined as positive only in parameters block\n  \n  //likelihood\n  bmi ~ normal(beta0 + beta1*sex, sigma);\n}\"\n\n# write file to temp dir\nstan_mod_temp &lt;- write_stan_file(model_string, dir = tempdir())\n# create Stan model\nstan_mod &lt;- cmdstan_model(stan_mod_temp)\n# fit the model using Stan\nfit &lt;- stan_mod$sample(data = data_list, seed = 123, chains = 4, parallel_chains = 2, refresh = 500 )\n\nRunning MCMC with 4 chains, at most 2 in parallel...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.3 seconds.\n\n# summary plus diagnostics\nfit$summary()\n\n# A tibble: 4 × 10\n  variable     mean   median    sd   mad       q5     q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;num&gt;    &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n1 lp__     -306.    -306.    1.19  0.980 -308.    -305.    1.00    1899.\n2 beta0      22.8     22.8   0.313 0.322   22.3     23.4   1.00    2021.\n3 beta1       0.813    0.812 0.406 0.405    0.136    1.47  1.00    1965.\n4 sigma       2.82     2.82  0.140 0.140    2.60     3.07  1.00    2856.\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n# just the params\n# fit$summary(c(\"beta0\", \"beta1\", \"sigma\"), \"mean\", \"sd\")\n\nThe output tells us that the estimated means for female BMI is 22.8 (females were dummy coded as 0). Given the priors we used we can say that there is a 90% probability that the value for female BMI lies between 22.3 and 23.4. The estimated male BMI is 0.81 (with 90% probability of being between 0.13 & 1.48) units greater than female BMI i.e. ~23.6. The mean values are the same as estimated by the frequentist \\(t\\)-test procedure.\nTo plot the posterior distributions we can extract the posterior draws and use the bayesplot package.\n\n# get the draws; uses posterior package\ndraws &lt;- fit$draws(variables = c('beta0', 'beta1', 'sigma'))\n# plot the draws; bayesplot package\nmcmc_dens(draws)\n\n\n\n\nPlotting the posterior distribution for the male BMI is as simple as adding together the draws for beta0 and beta1.\n\n# draws to dataframe\ndraws_df &lt;- as_draws_df(draws)\n# posterior for male bmi included\nbmi_posteriors &lt;- draws_df %&gt;% mutate(male_bmi_post = beta0 + beta1)\nmcmc_dens(bmi_posteriors, pars = c('beta0', 'male_bmi_post', 'sigma'))\n\n\n\n\nThere are easier ways to create basic (and more complex) Bayesian models than writing out the Stan code by hand. The brms package allows us to write Bayesian models using R modeling syntax. The model is translated to Stan and then compiled & run.\n\n# brms bayesian modelling; same priors as above\nbrms_mod &lt;- brm(value ~ sex, data = bmi_data,\n                prior = c(prior(normal(25, 10), class = \"Intercept\"), # prior on intercept\n                          prior(normal(0, 5), class = \"b\", coef = 'sexM'), # prior on slope\n                          prior(normal(0, 100), class = \"sigma\")), # prior on resid var\n                iter = 3000, warmup = 500, chains = 4, seed = 1234)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 1: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 1: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 1: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 1: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 1: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 1: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 1: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 1: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 1: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 1: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 1: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.005772 seconds (Warm-up)\nChain 1:                0.020392 seconds (Sampling)\nChain 1:                0.026164 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 2: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 2: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 2: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 2: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 2: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 2: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 2: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 2: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 2: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 2: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 2: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.005593 seconds (Warm-up)\nChain 2:                0.019947 seconds (Sampling)\nChain 2:                0.02554 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 3: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 3: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 3: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 3: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 3: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 3: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 3: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 3: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 3: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 3: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 3: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.005579 seconds (Warm-up)\nChain 3:                0.018799 seconds (Sampling)\nChain 3:                0.024378 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL '38af6dc35b245825a290a54ed93cd989' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 3e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 3000 [  0%]  (Warmup)\nChain 4: Iteration:  300 / 3000 [ 10%]  (Warmup)\nChain 4: Iteration:  501 / 3000 [ 16%]  (Sampling)\nChain 4: Iteration:  800 / 3000 [ 26%]  (Sampling)\nChain 4: Iteration: 1100 / 3000 [ 36%]  (Sampling)\nChain 4: Iteration: 1400 / 3000 [ 46%]  (Sampling)\nChain 4: Iteration: 1700 / 3000 [ 56%]  (Sampling)\nChain 4: Iteration: 2000 / 3000 [ 66%]  (Sampling)\nChain 4: Iteration: 2300 / 3000 [ 76%]  (Sampling)\nChain 4: Iteration: 2600 / 3000 [ 86%]  (Sampling)\nChain 4: Iteration: 2900 / 3000 [ 96%]  (Sampling)\nChain 4: Iteration: 3000 / 3000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.005447 seconds (Warm-up)\nChain 4:                0.02118 seconds (Sampling)\nChain 4:                0.026627 seconds (Total)\nChain 4: \n\n# model summary\nsummary(brms_mod)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: value ~ sex \n   Data: bmi_data (Number of observations: 200) \n  Draws: 4 chains, each with iter = 3000; warmup = 500; thin = 1;\n         total post-warmup draws = 10000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    22.84      0.31    22.22    23.46 1.00     9382     7692\nsexM          0.82      0.41     0.01     1.63 1.00    10240     7126\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.82      0.14     2.56     3.12 1.00     9313     7484\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe values for each coefficient are the same as both the frequentist model and the handcoded Stan model (as we’d expect).\nPlotting the model can be done with the mcmc_plot() function in brms.\n\n# plot the draws using built-in brms functions (that calls bayesplot)\n# regex  = TRUE for regular expression (^b) to pull out beta coefficients\nmcmc_plot(brms_mod, variable = c('^b', 'sigma'), type = 'dens', regex = TRUE)\n\n\n\n\nAn even easier (but less flexible) package is rstanarm."
  },
  {
    "objectID": "posts/hello_data_r/hello_world_data_R.html#summary",
    "href": "posts/hello_data_r/hello_world_data_R.html#summary",
    "title": "Hello Data World 1 (R)",
    "section": "Summary",
    "text": "Summary\nThis post has been a quick skip through some data loading, exploration, filtering and both frequentist & Bayesian modelling in R."
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html",
    "href": "posts/hello_data_python/hello_world_data_python.html",
    "title": "Hello Data World 2 (python)",
    "section": "",
    "text": "This is the second of three posts that will carry out data loading, exploration, filtering and statistical testing (frequentist & Bayesian). In the first post of the series we used R. In this post we’ll use python. Like the previous post there won’t be much exposition - we’ll just move through the process.\nIf you want to follow along the data are here.\n\n\nPython, like R, has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. The first step is to load the packages we will need. I use the Anaconda python distribution and packages that are not installed by default can be installed with the conda tool. In this post we use the pymc package for Bayesian modeling. The installation notes for pymc recommend installing it into its own python conda environment so this is what I did! To run the code in VSCode I set the relevant python interpreter by using Ctrl+Shift+P to bring up the Command Palette and selecting the relevant python environment. The other packages had to be installed into the same enviroment using conda install.\n\n\n\nSetting the python environment\n\n\nOk, let’s get on with loading the packages we’ll need!\n\nimport pandas as pd # dataframes for python\nimport plotnine as pn # ggplot2 clone\npn.options.figure_size = (5, 5) # set a default figure size for plotnine plots\npn.options.current_theme = pn.theme_bw() # set simple theme\nimport seaborn as sns # statistical plotting in python land\nsns.set_theme(style=\"whitegrid\") # plot theme\n# frequentist modeling\nimport scipy.stats as stats # classic freq stats for python\nimport pingouin as pg # alt to scipy.stats\n# bayesian modeling\nimport pymc as pm # write your models explicitly\nimport bambi as bmb # formula like interface\nimport arviz as az # plots for MCMC objects\n\n\n\n\nWe can use the read_csv() function of the pandas (Reback et al. 2020) package to read in the data. These data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a numbers of years by the students who carried out various measures on themselves.\n\n# get the data\ndata_in = pd.read_csv('data/BODY_COMPOSITION_DATA.csv', sep=',', na_values = \"NA\")\n\n\n\n\nThe pandas package also provides some tools for exploring the data.\n\ndata_in.head()\n# examine summary of types etc\ndata_in.info() # there are missing values in bia & HW\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 203 entries, 0 to 202\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        203 non-null    object \n 1   girths     203 non-null    float64\n 2   bia        202 non-null    float64\n 3   DW         203 non-null    float64\n 4   jackson    203 non-null    float64\n 5   HW         202 non-null    float64\n 6   skinfolds  203 non-null    float64\n 7   BMI        203 non-null    float64\n 8   WHR        203 non-null    float64\n 9   Waist      203 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 16.0+ KB\n\n\nWe can see that there are some missing values in the BIA and HW variables (these variables have 202 non-null values). There are many ways to deal with missing values but here we will just drop rows with missing values. The dropna() method for pandas dataframes allows us to drop rows (axis 0) or columns (axis 1) with missing values. We also specify the inplace = True argument so that the data we are working on is altered.\n\n# drop the rows (index 0) with missing values; alter dataframe (inplace = True)\ndata_in.dropna(axis = 0, inplace = True)\ndata_in.info() # all non-null values\n\n# summary stats\ndata_in.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 0 to 201\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        201 non-null    object \n 1   girths     201 non-null    float64\n 2   bia        201 non-null    float64\n 3   DW         201 non-null    float64\n 4   jackson    201 non-null    float64\n 5   HW         201 non-null    float64\n 6   skinfolds  201 non-null    float64\n 7   BMI        201 non-null    float64\n 8   WHR        201 non-null    float64\n 9   Waist      201 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 17.3+ KB\n\n\n\n\n\n\n\n\n\ngirths\nbia\nDW\njackson\nHW\nskinfolds\nBMI\nWHR\nWaist\n\n\n\n\ncount\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n\n\nmean\n20.734552\n16.980100\n21.614647\n14.212189\n21.426408\n82.684751\n23.223000\n0.781529\n76.756716\n\n\nstd\n8.382091\n6.792665\n7.307617\n7.479344\n8.006785\n33.108192\n3.168286\n0.057142\n7.352106\n\n\nmin\n7.150000\n5.700000\n4.100000\n3.000000\n4.100000\n27.750000\n2.900000\n0.670000\n61.000000\n\n\n25%\n15.080000\n11.900000\n16.300000\n8.000000\n15.000000\n59.250000\n21.170000\n0.740000\n72.000000\n\n\n50%\n20.120000\n15.900000\n21.400000\n12.600000\n21.000000\n76.230000\n23.000000\n0.780000\n76.000000\n\n\n75%\n24.400000\n21.200000\n28.000000\n19.000000\n27.000000\n100.350000\n24.800000\n0.815000\n81.000000\n\n\nmax\n87.900000\n39.300000\n45.900000\n35.000000\n43.000000\n181.000000\n33.030000\n0.990000\n100.800000\n\n\n\n\n\n\n\nNext we will convert our data from wide format to long format (Wickham 2014) with the pandas.melt() function. Long data makes plotting and statistical analyses easier. In long format data the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations.\n\n# long data\ndataL = pd.melt(data_in, id_vars = \"sex\", var_name = \"method\", value_name = \"value\")\ndataL.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n0\nM\ngirths\n10.85\n\n\n1\nM\ngirths\n14.12\n\n\n2\nM\ngirths\n12.30\n\n\n3\nM\ngirths\n8.50\n\n\n4\nM\ngirths\n11.66\n\n\n\n\n\n\n\nExploration with plots is an essential step for checking values and the distribution of data. There is an extensive plotting ecosystem in python.\n\n\n\nPython visualisation landscape (source)\n\n\nThe seaborn (Waskom 2021) package provides a high level interface for plotting data & statistical summaries. If you’re used to e.g. ggplot2 in R then the plotnine package provides very similar functionality.The tabs below demonstrate the same plot using each of these packages.\n\nseabornplotnine\n\n\n\nfg = sns.FacetGrid(dataL, col = 'method', hue = 'sex', col_wrap = 3, sharey = False); # create grid\n\nfg.map(sns.stripplot, 'sex', 'value', jitter = 0.05, size = 10, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.5, order = [\"F\", \"M\"]); # map stripplot onto grid\n\n\n\n\n\n\n\npt = pn.ggplot(dataL, pn.aes('sex', 'value', colour = 'sex')) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.facet_wrap(\"method\", scales = \"free_y\") + pn.scale_colour_manual(values=['firebrick', 'cornflowerblue'])\npt\n\n/home/iain/anaconda3/envs/pymc_env/lib/python3.11/site-packages/plotnine/facets/facet.py:440: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. First we’ll filter the data to just BMI.\n\n# filter to just bmi data\nbmi_data = dataL[dataL.method == \"BMI\"]\nbmi_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 1206 to 1406\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   sex     201 non-null    object \n 1   method  201 non-null    object \n 2   value   201 non-null    float64\ndtypes: float64(1), object(2)\nmemory usage: 6.3+ KB\n\n\n\n# first few values\nbmi_data.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n1206\nM\nBMI\n20.70\n\n\n1207\nM\nBMI\n21.90\n\n\n1208\nM\nBMI\n21.39\n\n\n1209\nM\nBMI\n19.26\n\n\n1210\nM\nBMI\n22.30\n\n\n\n\n\n\n\nWe’ll re-plot these data.\n\nseabornplotnine\n\n\n\nbmi_pt1 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt1\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\nbmi_pt2 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt2\n\n\n\n\n\n\n\nWe can clearly see the outlier in the male data. Removing outliers is a contentious subject but a BMI of 2 is unrealistic so we’ll remove this value.\n\n# note very low bmi point in M; let's drop that\nbmi_data = bmi_data[bmi_data.value &gt; 15]\n# summary\nbmi_data.describe()\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\ncount\n200.000000\n\n\nmean\n23.324615\n\n\nstd\n2.828887\n\n\nmin\n18.080000\n\n\n25%\n21.177500\n\n\n50%\n23.000000\n\n\n75%\n24.802500\n\n\nmax\n33.030000\n\n\n\n\n\n\n\n\nseabornplotnine\n\n\n\n# seaborn plot\nbmi_pt3 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt3\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\n# plotnine plot\nbmi_pt4 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt4\n\n\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#introduction",
    "href": "posts/hello_data_python/hello_world_data_python.html#introduction",
    "title": "Hello Data World 2 (python)",
    "section": "",
    "text": "This is the second of three posts that will carry out data loading, exploration, filtering and statistical testing (frequentist & Bayesian). In the first post of the series we used R. In this post we’ll use python. Like the previous post there won’t be much exposition - we’ll just move through the process.\nIf you want to follow along the data are here.\n\n\nPython, like R, has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. The first step is to load the packages we will need. I use the Anaconda python distribution and packages that are not installed by default can be installed with the conda tool. In this post we use the pymc package for Bayesian modeling. The installation notes for pymc recommend installing it into its own python conda environment so this is what I did! To run the code in VSCode I set the relevant python interpreter by using Ctrl+Shift+P to bring up the Command Palette and selecting the relevant python environment. The other packages had to be installed into the same enviroment using conda install.\n\n\n\nSetting the python environment\n\n\nOk, let’s get on with loading the packages we’ll need!\n\nimport pandas as pd # dataframes for python\nimport plotnine as pn # ggplot2 clone\npn.options.figure_size = (5, 5) # set a default figure size for plotnine plots\npn.options.current_theme = pn.theme_bw() # set simple theme\nimport seaborn as sns # statistical plotting in python land\nsns.set_theme(style=\"whitegrid\") # plot theme\n# frequentist modeling\nimport scipy.stats as stats # classic freq stats for python\nimport pingouin as pg # alt to scipy.stats\n# bayesian modeling\nimport pymc as pm # write your models explicitly\nimport bambi as bmb # formula like interface\nimport arviz as az # plots for MCMC objects\n\n\n\n\nWe can use the read_csv() function of the pandas (Reback et al. 2020) package to read in the data. These data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a numbers of years by the students who carried out various measures on themselves.\n\n# get the data\ndata_in = pd.read_csv('data/BODY_COMPOSITION_DATA.csv', sep=',', na_values = \"NA\")\n\n\n\n\nThe pandas package also provides some tools for exploring the data.\n\ndata_in.head()\n# examine summary of types etc\ndata_in.info() # there are missing values in bia & HW\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 203 entries, 0 to 202\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        203 non-null    object \n 1   girths     203 non-null    float64\n 2   bia        202 non-null    float64\n 3   DW         203 non-null    float64\n 4   jackson    203 non-null    float64\n 5   HW         202 non-null    float64\n 6   skinfolds  203 non-null    float64\n 7   BMI        203 non-null    float64\n 8   WHR        203 non-null    float64\n 9   Waist      203 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 16.0+ KB\n\n\nWe can see that there are some missing values in the BIA and HW variables (these variables have 202 non-null values). There are many ways to deal with missing values but here we will just drop rows with missing values. The dropna() method for pandas dataframes allows us to drop rows (axis 0) or columns (axis 1) with missing values. We also specify the inplace = True argument so that the data we are working on is altered.\n\n# drop the rows (index 0) with missing values; alter dataframe (inplace = True)\ndata_in.dropna(axis = 0, inplace = True)\ndata_in.info() # all non-null values\n\n# summary stats\ndata_in.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 0 to 201\nData columns (total 10 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   sex        201 non-null    object \n 1   girths     201 non-null    float64\n 2   bia        201 non-null    float64\n 3   DW         201 non-null    float64\n 4   jackson    201 non-null    float64\n 5   HW         201 non-null    float64\n 6   skinfolds  201 non-null    float64\n 7   BMI        201 non-null    float64\n 8   WHR        201 non-null    float64\n 9   Waist      201 non-null    float64\ndtypes: float64(9), object(1)\nmemory usage: 17.3+ KB\n\n\n\n\n\n\n\n\n\ngirths\nbia\nDW\njackson\nHW\nskinfolds\nBMI\nWHR\nWaist\n\n\n\n\ncount\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n201.000000\n\n\nmean\n20.734552\n16.980100\n21.614647\n14.212189\n21.426408\n82.684751\n23.223000\n0.781529\n76.756716\n\n\nstd\n8.382091\n6.792665\n7.307617\n7.479344\n8.006785\n33.108192\n3.168286\n0.057142\n7.352106\n\n\nmin\n7.150000\n5.700000\n4.100000\n3.000000\n4.100000\n27.750000\n2.900000\n0.670000\n61.000000\n\n\n25%\n15.080000\n11.900000\n16.300000\n8.000000\n15.000000\n59.250000\n21.170000\n0.740000\n72.000000\n\n\n50%\n20.120000\n15.900000\n21.400000\n12.600000\n21.000000\n76.230000\n23.000000\n0.780000\n76.000000\n\n\n75%\n24.400000\n21.200000\n28.000000\n19.000000\n27.000000\n100.350000\n24.800000\n0.815000\n81.000000\n\n\nmax\n87.900000\n39.300000\n45.900000\n35.000000\n43.000000\n181.000000\n33.030000\n0.990000\n100.800000\n\n\n\n\n\n\n\nNext we will convert our data from wide format to long format (Wickham 2014) with the pandas.melt() function. Long data makes plotting and statistical analyses easier. In long format data the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations.\n\n# long data\ndataL = pd.melt(data_in, id_vars = \"sex\", var_name = \"method\", value_name = \"value\")\ndataL.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n0\nM\ngirths\n10.85\n\n\n1\nM\ngirths\n14.12\n\n\n2\nM\ngirths\n12.30\n\n\n3\nM\ngirths\n8.50\n\n\n4\nM\ngirths\n11.66\n\n\n\n\n\n\n\nExploration with plots is an essential step for checking values and the distribution of data. There is an extensive plotting ecosystem in python.\n\n\n\nPython visualisation landscape (source)\n\n\nThe seaborn (Waskom 2021) package provides a high level interface for plotting data & statistical summaries. If you’re used to e.g. ggplot2 in R then the plotnine package provides very similar functionality.The tabs below demonstrate the same plot using each of these packages.\n\nseabornplotnine\n\n\n\nfg = sns.FacetGrid(dataL, col = 'method', hue = 'sex', col_wrap = 3, sharey = False); # create grid\n\nfg.map(sns.stripplot, 'sex', 'value', jitter = 0.05, size = 10, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.5, order = [\"F\", \"M\"]); # map stripplot onto grid\n\n\n\n\n\n\n\npt = pn.ggplot(dataL, pn.aes('sex', 'value', colour = 'sex')) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.facet_wrap(\"method\", scales = \"free_y\") + pn.scale_colour_manual(values=['firebrick', 'cornflowerblue'])\npt\n\n/home/iain/anaconda3/envs/pymc_env/lib/python3.11/site-packages/plotnine/facets/facet.py:440: PlotnineWarning: If you need more space for the x-axis tick text use ... + theme(subplots_adjust={'wspace': 0.25}). Choose an appropriate value for 'wspace'.\n\n\n\n\n\n\n\n\nThere are a couple of mad values in the BMI and girths variables. For the rest of the analysis we’ll concentrate on the BMI variable. First we’ll filter the data to just BMI.\n\n# filter to just bmi data\nbmi_data = dataL[dataL.method == \"BMI\"]\nbmi_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 201 entries, 1206 to 1406\nData columns (total 3 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   sex     201 non-null    object \n 1   method  201 non-null    object \n 2   value   201 non-null    float64\ndtypes: float64(1), object(2)\nmemory usage: 6.3+ KB\n\n\n\n# first few values\nbmi_data.head()\n\n\n\n\n\n\n\n\nsex\nmethod\nvalue\n\n\n\n\n1206\nM\nBMI\n20.70\n\n\n1207\nM\nBMI\n21.90\n\n\n1208\nM\nBMI\n21.39\n\n\n1209\nM\nBMI\n19.26\n\n\n1210\nM\nBMI\n22.30\n\n\n\n\n\n\n\nWe’ll re-plot these data.\n\nseabornplotnine\n\n\n\nbmi_pt1 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt1\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\nbmi_pt2 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt2\n\n\n\n\n\n\n\nWe can clearly see the outlier in the male data. Removing outliers is a contentious subject but a BMI of 2 is unrealistic so we’ll remove this value.\n\n# note very low bmi point in M; let's drop that\nbmi_data = bmi_data[bmi_data.value &gt; 15]\n# summary\nbmi_data.describe()\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\ncount\n200.000000\n\n\nmean\n23.324615\n\n\nstd\n2.828887\n\n\nmin\n18.080000\n\n\n25%\n21.177500\n\n\n50%\n23.000000\n\n\n75%\n24.802500\n\n\nmax\n33.030000\n\n\n\n\n\n\n\n\nseabornplotnine\n\n\n\n# seaborn plot\nbmi_pt3 = sns.stripplot(x = \"sex\", y = \"value\", data = bmi_data, jitter = 0.05, palette=[\"firebrick\", \"cornflowerblue\"], alpha = 0.8, order = [\"F\", \"M\"]);\nbmi_pt3\n\n&lt;Axes: xlabel='sex', ylabel='value'&gt;\n\n\n\n\n\n\n\n\n# plotnine plot\nbmi_pt4 = pn.ggplot(bmi_data, pn.aes(\"sex\", \"value\", colour = \"sex\")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = [\"firebrick\", \"cornflowerblue\"])\nbmi_pt4\n\n\n\n\n\n\n\nMuch better!"
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#frequentist-testing",
    "href": "posts/hello_data_python/hello_world_data_python.html#frequentist-testing",
    "title": "Hello Data World 2 (python)",
    "section": "Frequentist testing",
    "text": "Frequentist testing\nWe’re now in a position to undertake some statistical analysis. We’ll start with a simple t-test to examine the mean difference in BMI between males and females. The scipy.stats (Virtanen et al. 2020) library provides functions for one sample, paired & independent t-tests (and other tests). We first extract the data we want to test into separate series and then pass these series to the appropriate function. The stats.ttest_ind() function returns a tuple containing the t-statistic and the p-value for the test and we can extract these and print those. The equal_var = False argument means we get Welch’s t-test which doesn’t assume equal variances in each group.\n\n# test diff between men & women; get data\nmale_data = bmi_data[bmi_data.sex == \"M\"]\nfemale_data = bmi_data[bmi_data.sex == \"F\"]\n\n# do the test\nt_res = stats.ttest_ind(male_data.value, female_data.value, equal_var = False) # tuple out, t-stat and p-value\nt_res\n# print informative result\nprint(\"The t-statistic is %.2f with a p-value of %.3f.\" % (t_res[0], t_res[1]))\n\nThe t-statistic is 2.11 with a p-value of 0.036.\n\n\nThe pingouin (Vallat 2018) package also provides functions for statistical testing.\nUsing the ttest() function with correction = 'auto' means pingouin automatically uses Welch’s T-test when the sample sizes are unequal as they are here.\n\n# pingouin example; correction =‘auto’\npg.ttest(male_data.value, female_data.value, paired = False, correction = 'auto')\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nT-test\n2.11342\n192.047574\ntwo-sided\n0.035856\n[0.05, 1.59]\n0.293662\n1.242\n0.529014\n\n\n\n\n\n\n\nThe pingouin package provides us with much more information - which may or may not be useful to you. The difference between male & female BMI is significant. This means that in a hypothetical long series of repeats of this study with different samples from the same population we would expect to see a difference as big or bigger between the sexes in more than 95% of those repeats. The pingouin package also reports the power of the test here. This is post-hoc power though & post-hoc power is witchcraft e.g. (Gelman 2019)."
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#bayesian-testing",
    "href": "posts/hello_data_python/hello_world_data_python.html#bayesian-testing",
    "title": "Hello Data World 2 (python)",
    "section": "Bayesian testing",
    "text": "Bayesian testing\nIn the previous post with R we used the Stan probabilistic programming language to create a Bayesian model for the BMI data. We could also use Stan here via the pystan interface but instead we’ll use a native python library called [pymc] (Salvatier, Wiecki, and Fonnesbeck 2016). The pymc package allows us to write data generating models and then use Markov Chain Monte Carlo (MCMC) sampling with those model definitions to generate posterior distributions. pymc supports a range of MCMC algorithms. In the code below we use the same priors we defined in the post using R.\n\n# bayesian test with pymc\n# create dummy variables; F = 0, M = 1\nbmi_data_dummy = pd.get_dummies(bmi_data, columns = [\"sex\"], drop_first = True)\n\n# set up priors & likelihood\n# https://docs.pymc.io/en/latest/api/generated/pymc.sample.html\nwith pm.Model() as model:  # model specifications in PyMC3 are wrapped in a `with` statement\n\n    # Define priors\n    sigma = pm.HalfNormal(\"sigma\", sigma = 100)\n    intercept = pm.Normal(\"Intercept\", mu = 25, sigma=10)\n    x_coeff = pm.Normal(\"male_diff\", mu = 0, sigma = 5)\n\n    # Define likelihood\n    likelihood = pm.Normal(\"value\", mu = intercept + x_coeff * bmi_data_dummy.sex_M, sigma=sigma, observed=bmi_data_dummy.value)\n\nNext we run the MCMC sampling on the model we defined above; by default the NUTS algorithm is used. This is the same MCMC algorithm as the Stan probabilistic progamming language uses by default. Using return_inferencedata = True means we can easily plot the results (see below).\n\n# MCMC sampling\n# 3 MCMC chains\n# draw 3000 posterior samples using NUTS sampling; 1000 iter burn-in\nwith model:\n    bayes_bmi = pm.sample(3000, tune = 1000, return_inferencedata = True, chains = 3)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (3 chains in 4 jobs)\nNUTS: [sigma, Intercept, male_diff]\nSampling 3 chains for 1_000 tune and 3_000 draw iterations (3_000 + 9_000 draws total) took 2 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:01&lt;00:00 Sampling 3 chains, 0 divergences]\n    \n    \n\n\nThe arviz library (Kumar et al. 2019) provides tools for summarising & plotting data from MCMC chains & posterior distributions.\n\naz.plot_trace(bayes_bmi);\n\n\n\n\nWe want the traceplots (on the right) to look like ‘hairy caterpillars’ & they all look fine here. The posterior distributions for each parameter also look healthy. We can plot the posteriors using arviz as well.\n\naz.plot_posterior(bayes_bmi, grid = (2,2), hdi_prob = 0.95);\n\n\n\n\nThe posterior distributions all look good. We can extract the intercept posterior and the posterior for the effect of ‘male’ and add these together to get the posterior for male BMI.\n\n# add Intercept & male diff posteriors; keep this new posterior in existing InferenceData object\nbayes_bmi.posterior[\"male_bmi\"] = bayes_bmi.posterior[\"Intercept\"] + bayes_bmi.posterior[\"male_diff\"]\n\n# replot with only intercept (female BMI), male BMI and sigma\naz.plot_posterior(bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"sigma\"]  , grid = (2,2), hdi_prob = 0.95);\n\n\n\n\n\n# summary\naz.summary(bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"sigma\"] , kind = \"stats\", hdi_prob = 0.9)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\n\nIntercept\n22.835\n0.316\n22.292\n23.317\n\n\nmale_bmi\n23.661\n0.258\n23.247\n24.099\n\n\nsigma\n2.826\n0.143\n2.588\n3.051\n\n\n\n\n\n\n\nThe output tells us that the estimated mean for female BMI is 22.8 (females were dummy coded as 0). Given the priors we used we can say that there is a 90% probability that the value for female BMI lies between 22.3 and 23.4. The estimated male BMI is 23.6 with 90% probability of being between 23.2 & 24. Note that the actual values might vary in the decimal point because the MCMC chains are random.\nThe bambi library (Capretto et al. 2022) can be used to create Bayesian models with a more intuitive formula interface like brms or rstanarm in R.\n\n# model with bambi\n# define priors\nprior_spec = {\n    \"Intercept\": bmb.Prior(\"Normal\", mu = 25, sigma = 10),\n    \"sex_M\": bmb.Prior(\"Normal\", mu = 0, sigma = 5),\n    \"value_sigma\": bmb.Prior(\"HalfNormal\", sigma = 100)\n}\n\n# define the model; formula syntax\nbmb_bayes_model = bmb.Model(\"value ~ sex\", priors = prior_spec, data = bmi_data)\n# MCMC sampling; returns InferenceData obj\nbmb_bayes_bmi = bmb_bayes_model.fit(draws = 3000, tune = 1000, chains = 3)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (3 chains in 4 jobs)\nNUTS: [value_sigma, Intercept, sex]\nSampling 3 chains for 1_000 tune and 3_000 draw iterations (3_000 + 9_000 draws total) took 2 seconds.\n\n\n\n\n\n\n\n    \n      \n      100.00% [12000/12000 00:01&lt;00:00 Sampling 3 chains, 0 divergences]\n    \n    \n\n\nThe bmb_bayes_bmi object is of type InferenceData like that returned from pymc (bambi uses pymc under the hood). We can use the bambi result in the same way we used the pymc result with arviz.\nFirst we’ll plot the posterior distributions and plots for each MCMC chain.\n\n# plots and dists\naz.plot_trace(bmb_bayes_bmi);\n\n\n\n\nNext we’ll plot the posterior distributions and get summaries of those posteriors.\n\n# plot posteriors\naz.plot_posterior(bmb_bayes_bmi, grid = (2,2), hdi_prob = 0.95);\n\n# Key summary and diagnostic info on the model parameters\naz.summary(bmb_bayes_bmi)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n22.840\n0.309\n22.255\n23.417\n0.003\n0.002\n15248.0\n7273.0\n1.0\n\n\nsex[M]\n0.828\n0.406\n0.027\n1.549\n0.003\n0.003\n15087.0\n7029.0\n1.0\n\n\nvalue_sigma\n2.816\n0.144\n2.545\n3.086\n0.001\n0.001\n12954.0\n6515.0\n1.0\n\n\n\n\n\n\n\n\n\n\nWe get some extra information using the bambi summary.\nAs we did in the pymc3 example we can add the Intercept and sex chains together to get a posterior distribution for the male BMI and add this data to our existing `` object.\n\nbmb_bayes_bmi.posterior[\"male_bmi\"] = bmb_bayes_bmi.posterior[\"Intercept\"] + bmb_bayes_bmi.posterior[\"sex\"]\n\nWe can easily summarise & plot the parameters we are interested in.\n\n# plot selected posteriors\naz.plot_posterior(bmb_bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"value_sigma\"], grid = (2,2), hdi_prob = 0.95);\n\n# posterior summary\naz.summary(bmb_bayes_bmi, var_names = [\"Intercept\", \"male_bmi\", \"value_sigma\"], kind = \"stats\", hdi_prob = 0.9)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_5%\nhdi_95%\n\n\n\n\nIntercept\n22.840\n0.309\n22.322\n23.335\n\n\nmale_bmi[M]\n23.668\n0.259\n23.249\n24.091\n\n\nvalue_sigma\n2.816\n0.144\n2.570\n3.043"
  },
  {
    "objectID": "posts/hello_data_python/hello_world_data_python.html#summary",
    "href": "posts/hello_data_python/hello_world_data_python.html#summary",
    "title": "Hello Data World 2 (python)",
    "section": "Summary",
    "text": "Summary\nThis post has been a quick skip through some data loading, exploration, filtering and both frequentist & Bayesian modelling with python."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]