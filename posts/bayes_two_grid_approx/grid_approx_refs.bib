@article{cohen1994,
  title = {The Earth Is Round (p\hspace{0.6em}{$<$}\hspace{0.6em}.05)},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  issn = {1935-990X 0003-066X},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H{$_0$} is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H{$_0$} one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication.},
  copyright = {(c) 2016 APA, all rights reserved},
  langid = {english},
  keywords = {Null Hypothesis Testing},
  file = {/home/iain/Documents/Work/Zotero/storage/MWITP846/Cohen.pdf}
}

@article{hubbard2003,
  title = {Confusion {{Over Measures}} of {{Evidence}} (p's) {{Versus Errors}} ({$\alpha$}'s) in {{Classical Statistical Testing}}},
  author = {Hubbard, Raymond and Bayarri, M. J},
  year = {2003},
  month = aug,
  journal = {The American Statistician},
  volume = {57},
  number = {3},
  pages = {171--178},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/0003130031856},
  file = {/home/iain/Documents/Work/Zotero/storage/S4G9Q4BX/0003130031856.pdf;/home/iain/Documents/Work/Zotero/storage/TBN6AGTI/0003130031856.pdf;/home/iain/Documents/Work/Zotero/storage/KAU6HCZU/0003130031856.html}
}

@article{hoenig2001,
  title = {The {{Abuse}} of {{Power}}},
  author = {Hoenig, John M. and Heisey, Dennis M.},
  year = {2001},
  month = feb,
  journal = {The American Statistician},
  volume = {55},
  number = {1},
  pages = {19--24},
  issn = {0003-1305},
  doi = {10.1198/000313001300339897},
  abstract = {It is well known that statistical power calculations can be valuable in planning an experiment. There is also a large literature advocating that power calculations be made whenever one performs a statistical test of a hypothesis and one obtains a statistically nonsignificant result. Advocates of such post-experiment power calculations claim the calculations should be used to aid in the interpretation of the experimental results. This approach, which appears in various forms, is fundamentally flawed. We document that the problem is extensive and present arguments to demonstrate the flaw in the logic.},
  keywords = {Bioequivalence testing,Burden of proof,Observed power,Retrospective power analysis,Statistical power,Type II error},
  file = {/home/iain/Documents/Work/Zotero/storage/KS9G2SHD/hoenig-heisey-the-abuse-of-power2001.pdf;/home/iain/Documents/Work/Zotero/storage/38CU9FN3/000313001300339897.html}
}

@article{gelman2019,
  title = {Don't {{Calculate Post-hoc Power Using Observed Estimate}} of {{Effect Size}}},
  author = {Gelman, Andrew},
  year = {2019},
  month = jan,
  journal = {Annals of Surgery},
  volume = {269},
  number = {1},
  pages = {e9-e10},
  issn = {1528-1140},
  doi = {10.1097/SLA.0000000000002908},
  langid = {english},
  pmid = {29994928},
  keywords = {Diabetes Mellitus; Type 2,Humans,Military Personnel},
  file = {/home/iain/Documents/Work/Zotero/storage/FE7RMD8E/Gelman_2019_Don't Calculate Post-hoc Power Using Observed Estimate of Effect Size.pdf}
}

@article{tukey1991,
  title = {The {{Philosophy}} of {{Multiple Comparisons}}},
  author = {Tukey, John W.},
  year = {1991},
  journal = {Statistical Science},
  volume = {6},
  number = {1},
  pages = {100--116},
  issn = {0883-4237},
  abstract = {This paper is based on the 1989 Miller Memorial Lecture at Stanford University. The topic was chosen because of Rupert Miller's long involvement and significant contributions to multiple comparison procedures and theory. Our emphasis will be on the major questions that have received relatively little attention--on what one wants multiple comparisons to do, on why one wants to do that, and on how one can communicate the results. Very little attention will be given to how the results can be calculated--after all, there are books about that (e.g., Miller, 1966, 1981; Hochberg and Tamhane, 1987).},
  file = {/home/iain/Documents/Work/Zotero/storage/IKCM2CN5/euclid.ss.1177011945.pdf}
}

@article{amaralReproducibilityExpectLess2021,
  title = {Reproducibility: Expect Less of the Scientific Paper},
  shorttitle = {Reproducibility},
  author = {Amaral, Olavo B. and Neves, Kleber},
  year = {2021},
  month = sep,
  journal = {Nature},
  volume = {597},
  number = {7876},
  pages = {329--331},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-021-02486-7},
  abstract = {Make science more reliable by placing the burden of replicability on the community, not on individual laboratories.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {Publishing,read,Research data,Research management},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Publishing, Research data, Research management},
  file = {/home/iain/Documents/Work/Zotero/storage/UDLHEEPH/Amaral_Neves_2021_Reproducibility.pdf;/home/iain/Documents/Work/Zotero/storage/WKQF6H4W/d41586-021-02486-7.html}
}

@article{erringtonChallengesAssessingReplicability2021,
  title = {Challenges for Assessing Replicability in Preclinical Cancer Biology},
  author = {Errington, Timothy M and Denis, Alexandria and Perfito, Nicole and Iorns, Elizabeth and Nosek, Brian A},
  editor = {Rodgers, Peter and Franco, Eduardo},
  year = {2021},
  month = dec,
  journal = {eLife},
  volume = {10},
  pages = {e67995},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.67995},
  abstract = {We conducted the Reproducibility Project: Cancer Biology to investigate the replicability of preclinical research in cancer biology. The initial aim of the project was to repeat 193 experiments from 53 high-impact papers, using an approach in which the experimental protocols and plans for data analysis had to be peer reviewed and accepted for publication before experimental work could begin. However, the various barriers and challenges we encountered while designing and conducting the experiments meant that we were only able to repeat 50 experiments from 23 papers. Here we report these barriers and challenges. First, many original papers failed to report key descriptive and inferential statistics: the data needed to compute effect sizes and conduct power analyses was publicly accessible for just 4 of 193 experiments. Moreover, despite contacting the authors of the original papers, we were unable to obtain these data for 68\% of the experiments. Second, none of the 193 experiments were described in sufficient detail in the original paper to enable us to design protocols to repeat the experiments, so we had to seek clarifications from the original authors. While authors were extremely or very helpful for 41\% of experiments, they were minimally helpful for 9\% of experiments, and not at all helpful (or did not respond to us) for 32\% of experiments. Third, once experimental work started, 67\% of the peer-reviewed protocols required modifications to complete the research and just 41\% of those modifications could be implemented. Cumulatively, these three factors limited the number of experiments that could be repeated. This experience draws attention to a basic and fundamental concern about replication \textendash{} it is hard to assess whether reported findings are credible.},
  keywords = {open data,open science,preregistration,replication,reproducibility,Reproducibility Project: Cancer Biology},
  file = {/home/iain/Documents/Work/Zotero/storage/L9SME9HA/Errington et al_2021_Challenges for assessing replicability in preclinical cancer biology.pdf}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{OPEN SCIENCE COLLABORATION}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aac4716},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  file = {/home/iain/Documents/Work/Zotero/storage/AR4BI8YZ/OPEN SCIENCE COLLABORATION_2015_Estimating the reproducibility of psychological science.pdf}
}
