---
title: Hello Data World 3 (julia)
date: 09/23/2022
bibliography: julia_references.bib
categories:
  - julia
editor_options:
  chunk_output_type: console
format:
  html:
    code-overflow: wrap
image: julia_circles.png
jupyter: julia-1.8
---

## Introduction

This is the third of three posts that will carry out data loading, exploration, filtering and statistical testing using different 'data science' programming languages. In the [first post](https://iaingallagher.github.io/posts/hello_data_r/hello_world_data_R.html) of the series we used [R](https://www.r-project.org); in the [second post](https://iaingallagher.github.io/posts/hello_data_python/hello_world_data_python.html) we used [python](https://www.python.org/). In this post we'll use [julia](https://julialang.org/). I'll add some extra commentary in this post about using julia because it's new and not so familiar (to me anyway). If you want to follow along then the data are [here](https://github.com/iaingallagher/iaingallagher.github.io/tree/main/data).

:::{.callout-tip}
## 'Time to first plot' problem
Julia has been designed to be fast as well as having a bunch of other advantages from modern computer science. The speed comes from the use of software called [LLVM](https://llvm.org/) for just-in-time compilation. The developers hope it helps solve the 'two-language problem' where machine learning applications/data science are written in a slow high-level language like R or python and then translated to a fast lower-level language like C++ or Rust for actual use. You can read more about this [here](https://julialang.org/blog/2012/02/why-we-created-julia/). 

However one consequence of just-in-time compilation is increased latency the first time any function is called because new machine code has to be compiled. In the julia community this is described as the 'time to first plot problem' because it can take a while to generate a plot the first time you call a plotting function (as we'll see later). The time-to-first plot problem makes `julia` like the F1 car in [this video](https://www.youtube.com/watch?v=3RuUp5MT3Uc) (start at ~5.30 if you don't want the whole thing). It starts later but once it gets going it flies along. The julia version used to write this post was version 1.8.5. Latency improvments are [expected](https://twitter.com/vchuravy/status/1607866217259479041) in julia 1.9.

If all this is gobbledygook then the TLDR is that the first time you do anything in julia in a fresh session it can take a while (especially plotting). Once it's going though it goes very fast.
::: 

### Preliminaries

Like R and python, julia has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. Julia is a young language in the data science/ numerical computing space. The version 1.0 release was only in 2018. This means that the infrastructure for analysis, data wrangling, plotting etc is not quite as stable as either R or python (although the version 1.0 release helped a lot with this). Julia packages may come and go and may or may not be maintained over the coming years. Everything I've used here (written in 2022) has a good level of support though and these packages should still be in existence in years to come although the exact syntax for usage might change. You can read about how to install julia packages [here](https://docs.julialang.org/en/v1/stdlib/Pkg/).

In any case the first step is to load the packages we will need. This will also take a while because some code is compiled at load time!

```{julia}
# loading & wrangling data
using CSV, DataFrames 

# plotting; algebraofgraphics is built on the Makie plotting package
using CairoMakie, AlgebraOfGraphics
CairoMakie.activate!(type = "svg") # high quality plots; note use of !

# frequentist stats
using HypothesisTests 

# bayesian stats
using Turing, TuringGLM, ArviZ 
```

### Loading the data

The `read()` function from the [`CSV`](https://csv.juliadata.org/stable/) package reads in the csv formatted data. The last argument to the function (`DataFrame`) provides a 'sink' for the loaded data i.e. turns the loaded data into a [`DataFrame`](https://dataframes.juliadata.org/stable/) object.

```{julia}
# get data; note defintion of missing values in function call
df = CSV.read("data/BODY_COMPOSITION_DATA.csv", header = 1, missingstring = "NA", DataFrame)
first(df, 5) # first 5 rows
```

### Exploration & tidying

The `DataFrames` package provides tools for exploring the data. 

```{julia}
#summarise data
describe(df)
```

We can see from the `nmissing` column that there are missing data in the HW and bia columns. The last column of this output (`eltype`) tells us the type of data we have & where we see `Union{Missing, Float64}` the column of data contains both `Float64` and `Missing` data. 

We can drop the rows containing missing values with the `dropmissing()` function. The `dropmissing!()` variant (i.e. with `!`) means we change the data 'in place'; the actual data we loaded is changed. The use of `!` like this is a common motif in julia to make in-place changes to objects (data, plots, variables etc).

```{julia}
# ! means in-place change
DataFrames.dropmissing!(df)
describe(df)
```

The missing value rows have been removed from the data. Next we will convert our data from wide format to long format [@wickham2014] using the `stack()` function. In long format the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations. The long data format will make later plotting and statistical analyses easier.

```{julia}
# reshape data to long
dfl = DataFrames.stack(df, 2:10)
# DataFrames.stack() here because TuringGLM also has a stack function; we need to be explicit about the version of stack() we want to use
first(dfl, 5)
```

```{julia}
describe(dfl)
```

As well as data summaries, exploration with plots is an essential step for checking values and the distribution of data. There are quite a few [plotting packages for julia](https://juliapackages.com/c/graphical-plotting) with various general or more specific uses. In this post we'll use the [`Makie`](https://makie.juliaplots.org/stable/) [@danischMakieJlFlexible2021] package which seems to have good mindshare in the julia community, is being actively developed and can cover many different graphical presentation styles. To use `Makie` for faceted plots we can call on the [`AlgebraOfGraphics`](http://juliaplots.org/AlgebraOfGraphics.jl/stable/) package which is built on `Makie`. If you've used `ggplot2` in R then `AlgebraOfGraphics` aims to provide very similar functionality. I tried several other packages (`Gadfly`, `StatsPlots`, `VegaLite`) here as well but I couldn't get any of them to produce a plot I liked. It seems as though production of facet/trellis plots with jittered data points is an area for development in julia!

Unfortunately `AlgebraOfGraphics` doesn't support jittering points (or a beeswarm plot) yet (I think jittering is useful in a plot like this so all the data can be seen) so in the code below we create boxplots rather than jittered points. 

```{julia}
# faceted boxplot of all variables coloured by Sex
bxpl = data(dfl) * visual(BoxPlot) * mapping(:sex, :value, color =:sex, layout = :variable) # faceting is defined by layout argument
# http://juliaplots.org/AlgebraOfGraphics.jl/dev/gallery/gallery/layout/faceting/
cols = ["F" => :firebrick, "M" => :cornflowerblue]
# draw() to show plot
draw(bxpl, facet = (;linkyaxes = :none), palettes = (color = cols,), figure=(;resolution=(800,800))) # note trailing comma needed in palettes argument (defines a tuple)
```

:::{.callout-note}
On my systems (i7 macbook pro (2015) & i7 linux machine; both 16Gb RAM) this plot takes about 55s... time to first plot problem!
:::

There are a couple of mad values in the `BMI` and `girths` variables. For the rest of the analysis we'll concentrate on the `BMI` variable. First we'll filter the data to just BMI and then plot that data.

```{julia}
# filter using anonymous function x -> x == "BMI"; x where x = BMI
# https://juliadatascience.io/filter_subset
bmi = subset(dfl, :variable => ByRow(x -> x == "BMI"))

# plot just BMI
bmipl = data(bmi) * visual(BoxPlot) * mapping(:sex, :value, color = :sex)
draw(bmipl, palettes = (color = cols,), figure = (; resolution = (400,400)))
```

The unrealistically low value in the male BMI is obvious.

We'll filter the BMI variable to sensible values for sample (i.e. students) by only including values here BMI is > 18.

```{julia}
bmi = filter(:value => x -> x > 18, bmi)
# redo plot to check
bmipl = data(bmi) * visual(BoxPlot) * mapping(:sex, :value, color = :sex)
draw(bmipl, palettes = (color = cols,), figure = (; resolution = (400,400)))
```

The data look much better.

### Frequentist testing

We're now in a position to undertake some statistical analysis. We'll start with a simple t-test to examine the mean difference in BMI between males and females. The [`HypothesisTesting.jl`](https://juliastats.org/HypothesisTests.jl/stable/) package provides functions for frequentist testing including t-tests. We first extract the data we want to test into separate series and then pass these series to the appropriate function. Here we are using the unequal variance t-test (i.e. Welch's test).

```{julia}
# create data vectors
mdata = filter(:sex => x -> x == "M", bmi).value
fdata  = filter(:sex => x -> x == "F", bmi).value
# carry out test
res = UnequalVarianceTTest(mdata, fdata)
res
```

There is quite a lot of detail here although this is not so different from the R `t.test()` output. The point estimate & p-value are of most interest. We can get just the p-value using the `pvalue()` extractor function passing in the test and tail areas (i.e. one- or two-sided) we want. We can print a rounded p-value using string interpolation with `$(thing_we_want_to_print)`. There doesn't seem to be a function to extract the point estimate though... that would be handy since p-values don't contain point estimate information.

```{julia}
pv = pvalue(UnequalVarianceTTest(mdata, fdata); tail = :both)
# print p rounded to 3 dp
println("The p-value for the difference in male versus female BMI is $(round(pv, digits = 3)).")
```

There's also a `confint()` function for confidence intervals.

```{julia}
ci = confint(UnequalVarianceTTest(mdata, fdata); tail = :both, level = 0.95)
println("The 95% CI for the difference in male versus female BMI is from $(round(ci[1], digits = 3)) to $(round(ci[2], digits = 3)).")
```

The 95% CI here ranges from barely different (0.055 units larger) to quite different (1.59 units larger).

### Bayesian testing

LIke R and python julia has a package for the `Stan` probabilistic programming language called [`Stan.jl`](https://github.com/StanJulia/Stan.jl). So one way to write Bayesian models in julia is to use `Stan.jl`. However we'll use a native julia library called [`Turing.jl`](https://turing.ml/stable/) [@geTuringLanguageFlexible2018]. `Turing.jl` allows us to write data generating models and then use Markov Chain Monte Carlo (MCMC) sampling with those model definitions to generate posterior distributions. `Turing.jl` supports a range of MCMC algorithms. In the code below we use the same priors we defined in the posts using R & python.

First we create a dummy variable for sex such that males are coded as 1 and females are coded as 0 and we also extract the BMI values into a separate variable.

```{julia}
# create dummy vars for sex & get value data
indep_var = Int64.(bmi.sex .== "M"); # vector of 1 & 0's for M & F respectively; the . after Int64 means 'broadcast' i.e. apply to every value in the vector of M/F
# values
dep_var = bmi.value;
```

Next we set up the priors and define the likelihood for the data.

```{julia}
#  bayesian model Turing
#  same priors as R/python
@model function bayes_bmi(x, y)

	# priors
	α ~ Normal(25, 10) # julia allows unicode characters; \alpha + TAB
	β ~ Normal(0, 5) # \beta + TAB
	# +ve only Normal dist for residual var
	σ ~ truncated(Normal(0, 100), lower = 0) # \sigma + TAB

	# likelihood for each y
	for i in 1:length(y)
		y[i] ~ Normal((α + β * x[i]), σ)
	end
end
```

We sample from the model we just set up using the `NUTS` algorithm (the same algorithm used by `Stan` by default) to create the posterior distribution.

```{julia}
# sample; 1000 is burn in; 0.65 is acceptance rate for samples; 3000 samples; 3 chains; MCMCThreads() required to get > 1 chain
# note about threads on Turing.jl guide page: "Be aware that Turing cannot add threads for you – you must have started your Julia instance with multiple threads to experience any kind of parallelism."
bayes_bmi_result = sample(bayes_bmi(indep_var, dep_var), NUTS(1000, 0.65), MCMCThreads(), 3000, 3);
```

In the [python post]() we used the `arviz` library [@kumarArviZUnifiedLibrary2019a] to visualise and summarise the distributions. The same library is available for `julia` as [`ArviZ.jl`](https://julia.arviz.org/stable/) and it works in much the same way. In order to examine summaries of the posterior distributions we first convert the `MCMCChains` object from the posterior sampling to an `InferenceData` object.

```{julia}
# convert to InferenceData object using ArviZ
idata_bayes_bmi_result = from_mcmcchains(bayes_bmi_result)
```

First we examine the posterior distributions with traceplots of the MCMC sampling process to make sure the MCMC chains converged.

```{julia}
plot_trace(idata_bayes_bmi_result, figsize = (5,6)); # bit annoying that diff plot engines use diff units for fig sizes e.g. px vs inches
```

These all look good.

We can then examine summary data. `ArviZ.jl` uses `summarystats()` rather than `summary()` which is used by `arviz` in `python`.

```{julia}
# show summary stats
summarystats(idata_bayes_bmi_result, kind = "stats", hdi_prob = 0.9)
# can also get variables explicitly with var_names = 
# summarystats(idata_bayes_bmi_result, var_names = ["α", "β", "σ"], kind = "stats", hdi_prob = 0.9)
```

Finally we can use `ArviZ` to examine more detailed plots of the posterior distributions.

```{julia}
plot_posterior(idata_bayes_bmi_result, grid=(2,2), hdi_prob = 0.9, round_to = 3, figsize = (8,5));
```

In order to assess the full posterior for male BMI we can extract the MCMC chains for the intercept and coefficient for `male` and add these together. This returns an `Array` object rather than an `MCMCChains` object. We convert the `Array` to an `InferenceData` object using `convert_to_inference_data()`.

```{julia}
# posterior for male bmi
male_bmi = idata_bayes_bmi_result.posterior[:α] + idata_bayes_bmi_result.posterior[:β]
# convert to InferenceData
male_bmi = convert_to_inference_data(male_bmi)
# plot
plot_posterior(male_bmi, hdi_prob = 0.9, round_to = 3, figsize=(5,5));
```

We can generate a summary table as we did above using `ArviZ.jl`.

```{julia}
summarystats(male_bmi, kind = "stats", hdi_prob = 0.9)
```

From this analysis we'd conclude that the female BMI averages 22.85 and with 90% probability ranges from 22.34 to 23.37. Male BMI is greater with an average of 23.66 (notably greater than the upper limit of the female 90% HDI) and ranging from 23.23 to 24.08 with 90% probability. These values are contingent on the priors we used.

Using `Turing.jl` we have to type the model out explicitly. If you'd prefer a formula type interface then the [`TuringGLM.jl`](https://beta.turing.ml/TuringGLM.jl/dev/) library can be used to create Bayesian models in a similar manner to `brms` or `rstanarm` in R or `bambi` in python. 

:::{.callout-note}
`TuringGLM` is a work in progress and at the moment has some limitations. For hierarchical models only single random-intercept hierarchical models are supported (so no random slope models).

Currently `TuringGLM.jl` supports the following likelihoods:

* Normal (the default if not specified): linear regression
* TDist: robust linear regression
* Bernoulli: logistic regression
* Poisson: count data regression
* NegativeBinomial: robust count data regression where there is overdispersion
:::

As before we first have to define the priors we want (although `TuringGLM` does provide default priors as well).

```{julia}
# create custom priors
# turingGLM takes predictors first, then intercept, then auxilliary (e.g. sigma)
# https://beta.turing.ml/TuringGLM.jl/dev/tutorials/custom_priors/ & ?CustomPrior
priors = CustomPrior(Normal(0, 5), Normal(25, 10), truncated(Normal(0, 100), lower = 0))
```

Now we can define the model using a formula interface and `TuringGLM` will take care of the heavy lifting for us.

```{julia}
# bayesian model TuringGLM
# add intercept to formula
frm = @formula(value ~ 1 + sex)
# create model (formula, data; priors)
turing_bmi_bayes = turing_model(frm, bmi; priors) # formula, data; priors... note comma & semi-colon use
# sample from model as per Turing above
turing_bmi_bayes_samples = sample(turing_bmi_bayes, NUTS(1000, 0.65), MCMCThreads(), 3000, 3);
```

After converting the `MCMCChains` object to an `InferenceData` object we can use `ArviZ` to summarise & plot the posterior distributions.

```{julia}
# convert to InferenceData object using ArviZ & shpw summary stats
idata_turing_bmi_bayes = from_mcmcchains(turing_bmi_bayes_samples)
# show summary stats; explicit variable selection
summarystats(idata_turing_bmi_bayes, var_names = ["α", "β", "σ"] , kind = "stats", hdi_prob = 0.9)
```

We can plot the posterior distributions.

```{julia}
plot_posterior(idata_turing_bmi_bayes, grid=(2,2), hdi_prob = 0.9, round_to = 3, figsize = (8,10));
```

We calculate the posteriors for male BMI as before by extracting the intercept and beta coefficient MCMC samples and adding them together. 

```{julia}
# calculate male bmi posterior
turing_male_bmi = idata_turing_bmi_bayes.posterior[:α] + idata_turing_bmi_bayes.posterior[:β] # returns a 3x3000x1 Array, not an MCMCChains object
# convert to InferenceData
idata_turing_male_bmi = convert_to_inference_data(turing_male_bmi) # function here is convert_to_inference_data
```

We can summarise and plot the distribtion of male BMI as we did above.

```{julia}
# summarise the posterior
summarystats(idata_turing_male_bmi, kind = "stats", hdi_prob = 0.9)
```

Now we can plot the posterior distribution for male BMI.

```{julia}
# plot the posterior
plot_posterior(idata_turing_male_bmi, hdi_prob = 0.9, round_to = 3, figsize = (5,5));
```

These results are essetially the same as we got from `Turing.jl` & the results from both Bayesian analyses are essentially the same as those we got from the frequentist analysis. 

## Summary

In this post we have used the `julia` language to load, wrangle, filter and plot some data. We've also seen how to do some basic frequentist and Bayesian inference.

