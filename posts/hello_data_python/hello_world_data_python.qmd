---
title: "Hello Data World 2 (python)"
date: 08/03/2022
bibliography: python_references.bib
categories: [python]
editor_options: 
  chunk_output_type: console
jupyter: python3
format:
  html:
    code-overflow: wrap
image: python_logo.png    
---

## Introduction

This is the second of three posts that will carry out data loading, exploration, filtering and statistical testing (frequentist & Bayesian). In the [first post](https://iaingallagher.github.io/posts/hello_data_r/hello_world_data_R.html) of the series we used [R](https://www.r-project.org). In this post we'll use [python](https://www.python.org). Like the previous post there won't be much exposition - we'll just move through the process. 

If you want to follow along the data are [here](https://github.com/iaingallagher/iaingallagher.github.io/tree/main/data).

### Preliminaries

Python, like R, has a host of extra packages to help with data import, wrangling, plotting & building various kinds of models. The first step is to load the packages we will need. I use the [Anaconda python distribution](https://www.anaconda.com/products/distribution) and packages that are not installed by default can be installed with the [`conda`](https://conda.io/projects/conda/en/latest/user-guide/getting-started.html) tool. In this post we use the `pymc` package for Bayesian modeling. The [installation notes]() for `pymc` recommend installing it into its own python `conda` environment so this is what I did! To run the code in VSCode I set the relevant `python` interpreter by using Ctrl+Shift+P to bring up the Command Palette and selecting the relevant python environment. The other packages had to be installed into the same enviroment using `conda install`.

![Setting the python environment](python_pymc_env_vscode.png){width=50%}

Ok, let's get on with loading the packages we'll need!

```{python}
import pandas as pd # dataframes for python
import plotnine as pn # ggplot2 clone
pn.options.figure_size = (5, 5) # set a default figure size for plotnine plots
pn.options.current_theme = pn.theme_bw() # set simple theme
import seaborn as sns # statistical plotting in python land
sns.set_theme(style="whitegrid") # plot theme
# frequentist modeling
import scipy.stats as stats # classic freq stats for python
import pingouin as pg # alt to scipy.stats
# bayesian modeling
import pymc as pm # write your models explicitly
import bambi as bmb # formula like interface
import arviz as az # plots for MCMC objects
```

### Loading the data

We can use the `read_csv()` function of the `pandas` [@rebackPandasdevPandasPandas2020] package to read in the data. These data are from body composition practicals run as part of the Sport & Exercise Science degree at the University of Stirling. They were collected over a numbers of years by the students who carried out various measures on themselves.

```{python}
# get the data
data_in = pd.read_csv('data/BODY_COMPOSITION_DATA.csv', sep=',', na_values = "NA")
```

### Exploration & tidying

The `pandas` package also provides some tools for exploring the data. 

```{python}
data_in.head()
# examine summary of types etc
data_in.info() # there are missing values in bia & HW
```

We can see that there are some missing values in the BIA and HW variables (these variables have 202 non-null values). There are many ways to deal with missing values but here we will just drop rows with missing values. The `dropna()` method for `pandas` dataframes allows us to drop rows (axis 0) or columns (axis 1) with missing values. We also specify the `inplace = True` argument so that the data we are working on is altered. 

```{python}
# drop the rows (index 0) with missing values; alter dataframe (inplace = True)
data_in.dropna(axis = 0, inplace = True)
data_in.info() # all non-null values

# summary stats
data_in.describe()
```

Next we will convert our data from wide format to long format [@wickham2014] with the  `pandas.melt()` function. Long data makes plotting and statistical analyses easier. In long format data the values for each individual and each measurement technique are identified by rows rather than spread across row & column combinations.

```{python}
# long data
dataL = pd.melt(data_in, id_vars = "sex", var_name = "method", value_name = "value")
dataL.head()
```

Exploration with plots is an essential step for checking values and the distribution of data. There is an extensive plotting ecosystem in python. 

![Python visualisation landscape ([source](https://geo-python.github.io/site/lessons/L7/python-plotting.html))](python_viz_landscape.png)

The `seaborn` [@waskomSeabornStatisticalData2021] package provides a high level interface for plotting data & statistical summaries. If you're used to e.g. `ggplot2` in R then the [`plotnine`](https://plotnine.readthedocs.io/en/stable/) package provides very similar functionality.The tabs below demonstrate the same plot using each of these packages. 

::: {.panel-tabset}

## seaborn
```{python}
#| warning: false
fg = sns.FacetGrid(dataL, col = 'method', hue = 'sex', col_wrap = 3, sharey = False); # create grid

fg.map(sns.stripplot, 'sex', 'value', jitter = 0.05, size = 10, palette=["firebrick", "cornflowerblue"], alpha = 0.5, order = ["F", "M"]); # map stripplot onto grid
```

## plotnine
```{python}
pt = pn.ggplot(dataL, pn.aes('sex', 'value', colour = 'sex')) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.facet_wrap("method", scales = "free_y") + pn.scale_colour_manual(values=['firebrick', 'cornflowerblue'])
pt
```
:::

There are a couple of mad values in the `BMI` and `girths` variables. For the rest of the analysis we'll concentrate on the `BMI` variable. First we'll filter the data to just BMI.

```{python}
# filter to just bmi data
bmi_data = dataL[dataL.method == "BMI"]
bmi_data.info()
```

```{python}
# first few values
bmi_data.head()
```

We'll re-plot these data.

::: {.panel-tabset}
## seaborn
```{python}
#| warning: false
bmi_pt1 = sns.stripplot(x = "sex", y = "value", data = bmi_data, jitter = 0.05, palette=["firebrick", "cornflowerblue"], alpha = 0.8, order = ["F", "M"]);
bmi_pt1
```

## plotnine
```{python}
bmi_pt2 = pn.ggplot(bmi_data, pn.aes("sex", "value", colour = "sex")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = ["firebrick", "cornflowerblue"])
bmi_pt2
```
:::

We can clearly see the outlier in the male data. Removing outliers is a contentious subject but a BMI of 2 is unrealistic so we'll remove this value.

```{python}
# note very low bmi point in M; let's drop that
bmi_data = bmi_data[bmi_data.value > 15]
# summary
bmi_data.describe()
```

::: {.panel-tabset}
## seaborn
```{python}
#| warning: false

# seaborn plot
bmi_pt3 = sns.stripplot(x = "sex", y = "value", data = bmi_data, jitter = 0.05, palette=["firebrick", "cornflowerblue"], alpha = 0.8, order = ["F", "M"]);
bmi_pt3
```

## plotnine
```{python}
# plotnine plot
bmi_pt4 = pn.ggplot(bmi_data, pn.aes("sex", "value", colour = "sex")) + pn.geom_jitter(width = 0.1, alpha = 0.5) + pn.scale_colour_manual(values = ["firebrick", "cornflowerblue"])
bmi_pt4
```
:::

Much better!

## Frequentist testing

We're now in a position to undertake some statistical analysis. We'll start with a simple t-test to examine the mean difference in BMI between males and females. The `scipy.stats` [@virtanenSciPyFundamentalAlgorithms2020] library provides functions for one sample, paired & independent t-tests (and other tests). We first extract the data we want to test into separate series and then pass these series to the appropriate function. The `stats.ttest_ind()` function returns a tuple containing the t-statistic and the p-value for the test and we can extract these and print those. The `equal_var = False` argument means we get Welch's t-test which *doesn't* assume equal variances in each group.

```{python}
# test diff between men & women; get data
male_data = bmi_data[bmi_data.sex == "M"]
female_data = bmi_data[bmi_data.sex == "F"]

# do the test
t_res = stats.ttest_ind(male_data.value, female_data.value, equal_var = False) # tuple out, t-stat and p-value
t_res
# print informative result
print("The t-statistic is %.2f with a p-value of %.3f." % (t_res[0], t_res[1]))
``` 

The `pingouin` [@vallatPingouinStatisticsPython2018] package also provides functions for statistical testing.

Using the `ttest()` function with `correction = 'auto'` means `pingouin` automatically uses Welch's T-test when the sample sizes are unequal as they are here.

```{python}
# pingouin example; correction =‘auto’
pg.ttest(male_data.value, female_data.value, paired = False, correction = 'auto')
```

The `pingouin` package provides us with much more information - which may or may not be useful to you. The difference between male & female BMI is significant. This means that in a hypothetical long series of repeats of this study with different samples from the same population we would expect to see a difference as big or bigger between the sexes in more than 95% of those repeats. The `pingouin` package also reports the power of the test here. This is post-hoc power though & post-hoc power is witchcraft e.g. [@gelmanDonCalculatePosthoc2019].

## Bayesian testing

In the previous post with R we used the Stan probabilistic programming language to create a Bayesian model for the BMI data. We could also use Stan here via the [pystan](https://pystan.readthedocs.io/en/latest/index.html) interface but instead we'll use a native python library called [`pymc`] [@salvatierProbabilisticProgrammingPython2016]. The `pymc` package allows us to write data generating models and then use Markov Chain Monte Carlo (MCMC) sampling with those model definitions to generate posterior distributions. `pymc` supports a range of MCMC algorithms. In the code below we use the same priors we defined in the post using R.

```{python}
# bayesian test with pymc
# create dummy variables; F = 0, M = 1
bmi_data_dummy = pd.get_dummies(bmi_data, columns = ["sex"], drop_first = True)

# set up priors & likelihood
# https://docs.pymc.io/en/latest/api/generated/pymc.sample.html
with pm.Model() as model:  # model specifications in PyMC3 are wrapped in a `with` statement

    # Define priors
    sigma = pm.HalfNormal("sigma", sigma = 100)
    intercept = pm.Normal("Intercept", mu = 25, sigma=10)
    x_coeff = pm.Normal("male_diff", mu = 0, sigma = 5)

    # Define likelihood
    likelihood = pm.Normal("value", mu = intercept + x_coeff * bmi_data_dummy.sex_M, sigma=sigma, observed=bmi_data_dummy.value)
```

Next we run the MCMC sampling on the model we defined above; by default the NUTS algorithm is used. This is the same MCMC algorithm as the `Stan` probabilistic progamming language uses by default. Using `return_inferencedata = True` means we can easily plot the results (see below).

```{python}
# MCMC sampling
# 3 MCMC chains
# draw 3000 posterior samples using NUTS sampling; 1000 iter burn-in
with model:
    bayes_bmi = pm.sample(3000, tune = 1000, return_inferencedata = True, chains = 3)
```


The `arviz` library [@kumarArviZUnifiedLibrary2019a] provides tools for summarising & plotting data from MCMC chains & posterior distributions.

```{python}
az.plot_trace(bayes_bmi);
```

We want the traceplots (on the right) to look like 'hairy caterpillars' & they all look fine here. The posterior distributions for each parameter also look healthy. We can plot the posteriors using `arviz` as well.

```{python}
az.plot_posterior(bayes_bmi, grid = (2,2), hdi_prob = 0.95);
```

The posterior distributions all look good. We can extract the intercept posterior and the posterior for the effect of 'male' and add these together to get the posterior for male BMI.

```{python}
# add Intercept & male diff posteriors; keep this new posterior in existing InferenceData object
bayes_bmi.posterior["male_bmi"] = bayes_bmi.posterior["Intercept"] + bayes_bmi.posterior["male_diff"]

# replot with only intercept (female BMI), male BMI and sigma
az.plot_posterior(bayes_bmi, var_names = ["Intercept", "male_bmi", "sigma"]  , grid = (2,2), hdi_prob = 0.95);
```

```{python}
# summary
az.summary(bayes_bmi, var_names = ["Intercept", "male_bmi", "sigma"] , kind = "stats", hdi_prob = 0.9)
```

The output tells us that the estimated mean for female BMI is 22.8 (females were dummy coded as 0). Given the priors we used we can say that there is a 90% probability that the value for female BMI lies between 22.3 and 23.4. The estimated male BMI is 23.6 with 90% probability of being between 23.2 & 24. Note that the actual values might vary in the decimal point because the MCMC chains are random.

The `bambi` library [@caprettoBambiSimpleInterface2022] can be used to create Bayesian models with a more intuitive formula interface like `brms` or `rstanarm` in R.

```{python}
# model with bambi
# define priors
prior_spec = {
    "Intercept": bmb.Prior("Normal", mu = 25, sigma = 10),
    "sex_M": bmb.Prior("Normal", mu = 0, sigma = 5),
    "value_sigma": bmb.Prior("HalfNormal", sigma = 100)
}

# define the model; formula syntax
bmb_bayes_model = bmb.Model("value ~ sex", priors = prior_spec, data = bmi_data)
# MCMC sampling; returns InferenceData obj
bmb_bayes_bmi = bmb_bayes_model.fit(draws = 3000, tune = 1000, chains = 3)
```

The `bmb_bayes_bmi` object is of type `InferenceData` like that returned from `pymc` (`bambi` uses `pymc` under the hood). We can use the `bambi` result in the same way we used the `pymc` result with `arviz`.

First we'll plot the posterior distributions and plots for each MCMC chain.

```{python}
# plots and dists
az.plot_trace(bmb_bayes_bmi);
```

Next we'll plot the posterior distributions and get summaries of those posteriors.

```{python}
# plot posteriors
az.plot_posterior(bmb_bayes_bmi, grid = (2,2), hdi_prob = 0.95);

# Key summary and diagnostic info on the model parameters
az.summary(bmb_bayes_bmi)
```

We get some extra information using the `bambi` summary.

As we did in the `pymc3` example we can add the `Intercept` and `sex` chains together to get a posterior distribution for the male BMI and add this data to our existing `` object.

```{python}
bmb_bayes_bmi.posterior["male_bmi"] = bmb_bayes_bmi.posterior["Intercept"] + bmb_bayes_bmi.posterior["sex"]
```

We can easily summarise & plot the parameters we are interested in.

```{python}
# plot selected posteriors
az.plot_posterior(bmb_bayes_bmi, var_names = ["Intercept", "male_bmi", "value_sigma"], grid = (2,2), hdi_prob = 0.95);

# posterior summary
az.summary(bmb_bayes_bmi, var_names = ["Intercept", "male_bmi", "value_sigma"], kind = "stats", hdi_prob = 0.9)
```

## Summary

This post has been a quick skip through some data loading, exploration, filtering and both frequentist & Bayesian modelling with python.  
