{
  "hash": "b578c8ca78a61d7434ebdec2ba7f4f47",
  "result": {
    "markdown": "---\ntitle: Probability and Bayes Rule\ndate: 12/29/2022\nbibliography: bayes_rule_refs.bib\ncategories:\n  - R\n  - Bayes\neditor_options:\n  chunk_output_type: console\nformat:\n  html:\n    code-overflow: wrap\nimage: bayes_leds.jpg\n---\n\n\n# Introduction\n\nThis is the first of a series of posts going over the basics of Bayesian inference. Bayesian inference uses Bayes rule which comes from simple algebra of basic probability rules. In this post we'll look at the basic rules of probability and derive Bayes rule. We'll look at a couple of simple examples to see how Bayes rule works.\n\n::: callout-note\nThere's a little bit of `R` code in this post and I'm using the new native `R` pipe `|>`. If you're used to `tidyverse` semantics you might think that `|>` behaves like the `magrittr` pipe, `%>%` but it doesn't! But it can (mostly) be used in the same way. See [here](https://ivelasq.rbind.io/blog/understanding-the-r-pipe/) for more details.\n:::\n\n# On the way to Bayes rule\n\nThe grade classifications for a course I taught in my previous job are shown in the table below. The course was taken by students across two programmes.\n\n|           | Programme |     |\n|-----------|-----------|-----|\n| **Grade** | SES       | SS  |\n| 1         | 10        | 3   |\n| 2.1       | 20        | 6   |\n| 2.2       | 11        | 15  |\n| 3         | 4         | 20  |\n\n: Table 1: Grade classification by programme.\n\nTables like this are called *contingency* tables and they are used to summarise the relationship between two (or more) categorical variables. We can create contingency tables in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# dimnames is a list; rownames then colnames; |> is native R pipe; pipe matrix to as.table func\ngrade_table <- matrix(c(10,20,11,4,3,6,15,20), ncol = 2, byrow = FALSE, \n              dimnames = list(c(\"1\", \"2.1\", \"2.2\", \"3\"), c(\"SES\", \"SS\"))) |> \n  as.table()\n              \ngrade_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    SES SS\n1    10  3\n2.1  20  6\n2.2  11 15\n3     4 20\n```\n:::\n:::\n\n\nWe can use contingency tables to calculate different types of probabilities.\n\n## Marginal probabilities\n\nThe row and column totals for each variable in the table are called the *marginal* totals.\n\n|                    | Programme |     |                    |\n|--------------------|-----------|-----|--------------------|\n| **Grade**          | SES       | SS  | **Marginal Total** |\n| 1                  | 10        | 3   | 13                 |\n| 2.1                | 20        | 6   | 26                 |\n| 2.2                | 11        | 15  | 26                 |\n| 3                  | 4         | 20  | 24                 |\n| **Marginal Total** | 45        | 44  | 89                 |\n\n: Table 2. Grade classifications with marginal totals\n\nWe can get R to add marginal totals to contingency tables for us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrade_table_marginals <- grade_table |> \n  addmargins()\ngrade_table_marginals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    SES SS Sum\n1    10  3  13\n2.1  20  6  26\n2.2  11 15  26\n3     4 20  24\nSum  45 44  89\n```\n:::\n:::\n\n\nThe marginal totals can be used to calculate *marginal* probabilities by dividing the [marginal]{style=\"color: #00BFFF;\"} total for a row or column by the [grand]{style=\"color: #00008B;\"} total. We denote marginal probabilities as $P(A)$ where $A$ is some outcome.\n\nFor example the marginal probability of students getting a 2.1 is the marginal number of students who got a 2.1 (26) divided by the total number of students in the course (89).\n\n$$\nP(2.1) = \\textcolor{#00BFFF}{26}/\\textcolor{#00008B}{89} = 0.29\n$$\n\n|                    | Programme |     |                               |\n|--------------------|-----------|-----|-------------------------------|\n| **Grade**          | SES       | SS  | **Marginal Total**            |\n| 1                  | 10        | 3   | 13                            |\n| 2.1                | 20        | 6   | [26]{style=\"color: #00BFFF;\"} |\n| 2.2                | 11        | 15  | 26                            |\n| 3                  | 4         | 20  | 24                            |\n| **Marginal Total** | 45        | 44  | [89]{style=\"color: #00008B;\"} |\n\n: Table 3. Numbers involved in marginal probabilities are coloured.\n\nWe see that $P(2.1)$ is 29%.\n\n## Joint probabilties\n\nThe probability of two (or more) outcomes considered together is called a *joint* probability.\n\nFor events $A$ and $B$ joint probabilities are often written as $P(A \\text{ } and \\text{ } B)$ or $P(A, \\text{ } B)$.\n\nTo calculate joint probability we divide [the number of outcomes that fulfill a specific criteria]{style=\"color: #00BFFF;\"} by the [grand total]{style=\"color: #00008B;\"}.\n\nHere an example might be:\n\n$$\nP(2.1 \\text{ and } SES)\n$$\n\nWe can calculate this probability from the total number of SES students who got a 2.1 classification (20) divided by the total number of students (89).\n\n$$\nP(2.1 \\text{ and } SES) = \\textcolor{#00BFFF}{20}/\\textcolor{#00008B}{89} = 0.225\n$$\n\n|                    | Programme                     |     |                               |\n|--------------------|-------------------------------|-----|-------------------------------|\n| **Grade**          | SES                           | SS  | **Marginal Total**            |\n| 1                  | 10                            | 3   | 13                            |\n| 2.1                | [20]{style=\"color: #00BFFF;\"} | 6   | 26                            |\n| 2.2                | 11                            | 15  | 26                            |\n| 3                  | 4                             | 20  | 24                            |\n| **Marginal Total** | 45                            | 44  | [89]{style=\"color: #00008B;\"} |\n\n: Table 4. Numbers involved in joint probabilities are coloured.\n\nWe see that the joint probability of 2.1 and being on the SES programme is 22.5%.\n\nFor joint probabilities the order of the outcomes doesn't matter.\n\n$P(A \\text{ and } B)$ = $P(B \\text{ and } A)$\n\nIn R the `prop.table()` function can convert a contingency table of raw numbers to a table of joint probabilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrade_probs <- grade_table |> \n  prop.table() |>\n  addmargins() |> # add marginal totals\n  round(3) # round to 3dp\n\ngrade_probs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      SES    SS   Sum\n1   0.112 0.034 0.146\n2.1 0.225 0.067 0.292\n2.2 0.124 0.169 0.292\n3   0.045 0.225 0.270\nSum 0.506 0.494 1.000\n```\n:::\n:::\n\n\n::: callout-note\nKey to both marginal and joint probability is that we use the **grand total** as the denominator to calculate these probabilities.\n:::\n\n## Conditional probabilities\n\nConditional probability is the probability of [an event or outcome occurring]{style=\"color: #00BFFF;\"} given some [other event or outcome]{style=\"color: #00008B;\"} has already occurred or is in place.\n\nWe denote conditional probabilities as $P(outcome | condition)$.\n\nWe can read this as \"The probability that outcome occurs *given* condition is in place\".\n\nWe might ask \"What's the probability of a 2.1 mark *given* the student is in the SES programme?\" We'd denote that as:\n\n$$\nP(2.1|SES)\n$$\n\n|                    | Programme                      |                               |                               |\n|--------------------|--------------------------------|-------------------------------|-------------------------------|\n| **Grade**          | [SES]{style=\"color: #5F9EA0;\"} | SS                            | **Marginal Total**            |\n| 1                  | [10]{style=\"color: #5F9EA0;\"}  | [3]{style=\"color: #D3D3D3;\"}  | [13]{style=\"color: #D3D3D3;\"} |\n| 2.1                | [20]{style=\"color: #00BFFF;\"}  | [6]{style=\"color: #D3D3D3;\"}  | [26]{style=\"color: #D3D3D3;\"} |\n| 2.2                | [11]{style=\"color: #5F9EA0;\"}  | [15]{style=\"color: #D3D3D3;\"} | [26]{style=\"color: #D3D3D3;\"} |\n| 3                  | [4]{style=\"color: #5F9EA0;;\"}  | [20]{style=\"color: #D3D3D3;\"} | [24]{style=\"color: #D3D3D3;\"} |\n| **Marginal Total** | [45]{style=\"color: #00008B;\"}  | [44]{style=\"color: #D3D3D3;\"} | [89]{style=\"color: #D3D3D3;\"} |\n\n: Table 5. When we calculate conditional probabilities we restrict ourselves to one row or column (here coloured) of a contingency table.\n\nIf we look in the SES column we see 20 students got a 2.1 classification. There were a total of 45 SES students so $P(2.1|SES)$ is [20]{style=\"color: #5F9EA0;\"}/[45]{style=\"color: #00008B;\"} or 0.444 or 44.4%.\n\nThe general formula for conditional probability is:\n\n$$\nP(A|B) = \\frac{P(A\\text{ and }B)}{P(B)}\n$$\n\nLet's see this at work for $P(2.1 | SES)$.\n\nThe joint probability - $P(2.1 \\text{ and } SES)$ - is 20/89 = 0.225.\n\nThe marginal probability of SES - $P(SES)$ - is 45/89 = 0.506.\n\nThe conditional probability of 2.1 given SES ($P(2.1 | SES)$) is therefore 0.225/0.506 = 0.444; exactly the same as before.\n\n**Importantly** $P(A|B) \\neq P(B|A)$**.**\n\nExamining our contingency table we saw that $P(2.1|SES)$ was 0.444.\n\nHowever this is not the same as $P(SES|2.1) = \\frac{P(2.1 \\text{ and } SES)}{P(2.1)}$ = \\[(20/89) / (26/89)\\] = 0.77.\n\n::: callout-note\nKey to conditional probability calculations using a contingency table is that we are **restricting ourselves to one row or column** of the table.\n:::\n\n## The General Multiplication rule\n\nWe can rearrange the equation for conditional probability to get the General Multiplication Rule for calculating the joint probability of A *and* B.\n\n$$\nP(A \\text{ and } B) = P(A|B)P(B)\n$$\n\nThis is useful if we are not given the joint probability for some outcome.\n\nMaybe we only know that 50.6% of students on the module are on the SES programme and that if a student was on the SES programme the probability of a 2.1. was 44.4%.\n\nWe can use this information to work back to the joint probability, $P(2.1 \\text{ and } SES$) using the formula above.\n\n$P(2.1 \\text{ and } SES) = P(2.1|SES)P(SES))$ = 0.444 \\* 0.506 = 0.225 - i.e. 22.5% as before.\n\n# Bayes rule\n\n::: column-margin\n[![Figure 1. (Probably not) Thomas Bayes 1701-1761](pics/Thomas_Bayes.gif)](https://en.wikipedia.org/wiki/Thomas_Bayes)\n\nThomas Bayes was a Presbyterian minister & philosopher. Although this portrait is universally used to portray Bayes it is probably [not](https://www.york.ac.uk/depts/maths/histstat/bayespic.htm) his portrait. The probability rule we call Bayes rule was published after Bayes' death by his friend Richard Price in 'An Essay towards solving a Problem in the Doctrine of Chances' in 1763.\n:::\n\nWe are often given conditional probabilities like $P(2.1 | SES)$ and we actually want the inverse conditional probability, $P(SES|2.1)$. As we noted above these are **not** the same probabilities!\n\nBayes rule allows us to invert conditional probabilities.\n\nWe know from above that:\n\n-   $P(A \\text{ and } B) = P(A|B)P(B)$\n\n-   $P(B \\text{ and } A) = P(B|A)P(A)$\n\nWe also know that the order of $A$ and $B$ in joint probability doesn't matter:\n\n$P(A \\text{ and } B) = P(B \\text{ and } A)$\n\nThis means that:\n\n$$\nP(B|A)P(A) = P(A|B)P(B)\n$$\n\nIf we divide through by $P(A)$ we get:\n\n$$\nP(B|A) = \\frac{P(A|B)P(B)}{P(A)}\n$$\n\nThis is exactly Bayes rule!\n\nIf we know $P(A|B)$ and the marginal probabilities, $P(A)$ and $P(B)$ we can get to $P(B|A)$ by applying Bayes rule.\n\nSuppose we know that $P(2.1|SES)$ is 0.444 and we know the marginal probabilities of $P(2.1)$ (0.29) and $P(SES)$ (0.506).\n\nUsing Bayes rule we can easily calculate the inverse conditional probability $P(SES|2.1)$:\n\n$$\nP(SES|2.1) = \\frac{0.444 \\times 0.506}{0.29} = 0.77\n$$\n\nThis is the same result we got above.\n\n## Components of Bayes rule\n\nEach part of Bayes rule has a name.\n\nIn the numerator $P(A|B)$ is the *likelihood* and $P(B)$ is the *prior*. The denominator, $P(A)$ is the *marginal probability of A* and is also called the *evidence*. The left hand probability, $P(B|A)$ is called the *posterior*.\n\n![Figure 2. The components of Bayes rule.](pics/Bayes_rule.png)\n\nBeing able to go from $P(A|B)$ to $P(B|A$) might seem trivial but it turns out to be really useful.\n\n# Simple Application of Bayes rule\n\nA classic application of Bayes rule is calculating the probability of actually having a disease after a positive test for that disease knowing only:\n\n-   The test result\n-   The disease prevalence rate i.e. the probability of getting the disease\n-   The true positive rate of the test (sensitivity) i.e. the probability the test is positive *if you have the disease*\n-   The true negative rate of the test (specificity) i.e. the probability the test is negative *if you do not have the disease*\n\nWhile I was writing this the COVID-19 prevalence (by positive test) for my age group where I live was 8.8% ($P(COVID)$ = 0.088). It was also the beginning of winter so the flu season was starting up.\n\nSuppose I start feeling unwell and decide to take a lateral flow test to see if I have COVID-19 as opposed to flu (or hopefully just a cold). Lateral flow device true positive rate ($P(+ve|COVID)$) seems to be pretty good but the true negative rate ($P(-ve|no \\text{ } COVID)$) seems to be much more variable e.g. @mistry2021. Let's say the test I use has a true positive rate of 99% and a true negative rate of 85%.\n\nIf I take the test and get a positive result there is a 99% probability the test is correct **if I have COVID-19** i.e. $P(+ve|COVID) = 0.99$... but this is not what I want to know. If I *already knew* I had COVID why bother with a test 🤡\n\nWhat I actually want to know is $P(COVID|+ve)$ i.e. the probability I actually *have* COVID *given* the test is positive.\n\nBayes rule can get us there.\n\n$$\nP(COVID|+ve) = \\frac{P(+ve|COVID)P(COVID)}{P(+ve)}\n$$\n\nThe numerator is easy from the information we have.\n\n-   Likelihood = $P(+ve|COVID) = 0.99$ i.e. true positive rate\n-   Prior = $P(COVID) = 0.088$ i.e. COVID prevalence\n\nThe denominator is the sum of the joint probabilities for getting a positive test and there are two ways I can get a positive test.\n\n-   Either I have COVID *and* the test is positive:\n\n$P(+ve \\text{ and } COVID)$ = $P(+ve|COVID)P(COVID)$ = 0.99 x 0.088 = 0.08712.\n\nNote that this is the same as the numerator; we just applied the general multiplication rule to get a joint probability.\n\n-   Alternatively I can get a false positive result i.e. I do not have COVID *but the test is positive anyway*.\n\nGiven the COVID-19 prevalence rate the probability I *do not* have COVID is 1-0.088 = 0.912.\n\nThe test has a false positive rate of 1-0.85 = 0.15.\n\nThe joint probability of a positive test and no COVID, $P(+ve \\text{ and } not \\text{ } COVID)$ is therefore 0.912 x 0.15 = 0.1368.\n\nThe denominator for Bayes rule (here $P(+ve)$) is the sum of these two joint probabilities = $0.08712 + 0.1368$.\n\nPutting all this together in Bayes rule gives us:\n\n$$\nP(COVID|+ve) = \\frac{P(+ve|COVID)P(COVID)}{P(+ve)}\n$$ $$\nP(COVID|+ve) = \\frac{0.99 \\times 0.088}{0.08712+0.1368}\n$$ $$\nP(COVID|+ve) = \\frac{0.08712}{0.2239}\n$$ $$\nP(COVID|+ve) = 0.3891\n$$\n\nSo given current prevalence and information about the accuracy of the test there is an approximately 39% chance I actually have COVID after a positive lateral flow test.\n\nWe could write a little function to calculate this in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# function takes sensitivity, specificity & prevalence\nbayes_calc <- function(sens, spec, prevalance){\n  # calc numerator\n  numerator = sens*prevalance\n  # calc denom\n  denominator = numerator + ((1-spec) * (1-prevalance))\n  # calc prob of disease\n  probDisease = numerator / denominator\n  return(probDisease)\n}\n```\n:::\n\n\nUsing this with the figures above for COVID testing gives us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_pos <- bayes_calc(0.99, 0.85, 0.088)\nprob_pos\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3890675\n```\n:::\n:::\n\n\nExactly the same result we got from our manual calculation.\n\n# Conclusion\n\nBayes rule is a conditional probability and is derived from simple algebra of established probability rules. Bayes rule allows us to invert conditional probabilities. This in turn can help us address questions like \"What's the probability I have the disease if the test is positive?\" rather than simply knowing the probability the test result is right if I have the disease. Questions like the former are often what we really need answers to!\n\nBayes rule may seem pretty trivial but it is in fact extremely useful. In the examples above the probabilities we used were point probabilities i.e. single numbers but Bayes rule can be used with full probability distributions as well. In the next few posts we'll explore how using Bayes rule with probability distributions can help us calculate the probability of hypotheses and models as well as estimate effect sizes in statistical analysis. This is where Bayes rule is really useful!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}